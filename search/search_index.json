{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<code>marginaleffects</code> for <code>R</code> and <code>Python</code> <p><code>marginaleffects</code> allows users to compute and plot predictions, slopes, marginal means, and comparisons (contrasts, risk ratios, odds, etc.) for over 100 classes of statistical models. It can conduct linear and non-linear hypothesis tests, or equivalence tests, and calculate uncertainty estimates using the delta method, bootstrapping, or simulation-based inference.</p> <p>The Marginal Effects Zoo is a free online book with over 30 chapters full of tutorials, case studies, and technical notes.</p>"},{"location":"CITATION/","title":"Citation","text":"<pre><code>To cite package \u2018marginaleffects\u2019 in publications use:\n\n  Arel-Bundock V (????). _marginaleffects: Predictions, Comparisons,\n  Slopes, Marginal Means, and Hypothesis Tests_. R package version\n  0.16.0.9018, &lt;https://marginaleffects.com/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests},\n    author = {Vincent Arel-Bundock},\n    note = {R package version 0.16.0.9018},\n    url = {https://marginaleffects.com/},\n  }\n</code></pre>"},{"location":"LICENSE/","title":"License","text":"<p>Copyright (C) 2023 Vincent Arel-Bundock vincent.arel-bundock@umontreal.ca</p> <p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program.  If not, see http://www.gnu.org/licenses/.</p>"},{"location":"LICENSE/#gnu-general-public-license","title":"GNU General Public License","text":"<p>Version 3, 29 June 2007 Copyright \u00a9 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt;</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions","text":"<p>\u201cThis License\u201d refers to version 3 of the GNU General Public License.</p> <p>\u201cCopyright\u201d also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\u201cThe Program\u201d refers to any copyrightable work licensed under this License. Each licensee is addressed as \u201cyou\u201d. \u201cLicensees\u201d and \u201crecipients\u201d may be individuals or organizations.</p> <p>To \u201cmodify\u201d a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \u201cmodified version\u201d of the earlier work or a work \u201cbased on\u201d the earlier work.</p> <p>A \u201ccovered work\u201d means either the unmodified Program or a work based on the Program.</p> <p>To \u201cpropagate\u201d a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \u201cconvey\u201d a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \u201cAppropriate Legal Notices\u201d to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code","text":"<p>The \u201csource code\u201d for a work means the preferred form of the work for making modifications to it. \u201cObject code\u201d means any non-source form of a work.</p> <p>A \u201cStandard Interface\u201d means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \u201cSystem Libraries\u201d of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \u201cMajor Component\u201d, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \u201cCorresponding Source\u201d for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \u201ckeep intact all notices\u201d.</li> <li>c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \u201caggregate\u201d if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \u201cUser Product\u201d is either (1) a \u201cconsumer product\u201d, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \u201cnormally used\u201d refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\u201cInstallation Information\u201d for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms","text":"<p>\u201cAdditional permissions\u201d are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \u201cfurther restrictions\u201d within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \u201centity transaction\u201d is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents","text":"<p>A \u201ccontributor\u201d is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \u201ccontributor version\u201d.</p> <p>A contributor's \u201cessential patent claims\u201d are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \u201ccontrol\u201d includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \u201cpatent license\u201d is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \u201cgrant\u201d such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \u201cKnowingly relying\u201d means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \u201cdiscriminatory\u201d if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-use-with-the-gnu-affero-general-public-license","title":"13. Use with the GNU Affero General Public License","text":"<p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \u201cor any later version\u201d applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p>"},{"location":"NEWS/","title":"News","text":""},{"location":"NEWS/#dev","title":"dev","text":"<p>Breaking changes:</p> <ul> <li>The <code>comparisons()</code> now uses \"forward contrasts\" by default for numeric predictors, instead of \"centered  contrasts\". This can lead to small numerical differences in non-linear models.</li> <li>The <code>variables</code> argument of the <code>comparisons()</code> function no longer accepts numeric vectors unless they are of length 2, specifying the low and high contrast values. This is to avoid ambiguity between the two vector version. Users should supply a data frame or a function instead. This is nearly as easy, and removes ambiguity.</li> </ul> <p>New supported packages:</p> <ul> <li><code>dbarts</code>: https://cran.r-project.org/package=dbarts</li> <li><code>mvgam</code>: https://nicholasjclark.github.io/mvgam/ Not available on CRAN yet, but this package maintains its own <code>marginaleffects</code> support function.</li> <li><code>rms::Gls</code>: https://cran.r-project.org/package=rms</li> </ul> <p>Misc:</p> <ul> <li><code>comparisons()</code>: The <code>variables</code> argument now accepts functions and data frames for factor, character, and logical variables.</li> <li>Deprecation warning for: <code>plot_cap()</code>, <code>plot_cme()</code>, and <code>plot_cco()</code>. These function names will be removed in version 1.0.0.</li> <li><code>options(modelsummary_factory_default=...)</code> is respected in Quarto and Rmarkdown documents.</li> </ul> <p>Bugs:</p> <ul> <li><code>wts</code> argument now respected in <code>avg_slopes()</code> for binary variables. Thanks to @trose64 for report #961</li> <li>Custom functions in the <code>comparison</code> argument of <code>comparisons()</code> did not supply the correct <code>x</code> vector length for bayesian models when the <code>by</code> argument is used. Thanks to @Sandhu-SS for report #931.</li> <li>Add support for two facet variables (through <code>facet_grid</code>) when plotting using <code>condition</code></li> <li><code>comparisons()</code>: When <code>variables</code> is a vector of length two and <code>newdata</code> has exactly two columns, there was ambiguity between custom vectors and length two vector of contrasts. But reported by C. Rainey on Twitter.</li> </ul>"},{"location":"NEWS/#0160","title":"0.16.0","text":"<p>Machine learning support:</p> <ul> <li><code>tidymodels</code> package</li> <li><code>mlr3</code> package</li> </ul> <p>Misc:</p> <ul> <li>New vignettes:</li> <li>Inverse Probability Weighting </li> <li>Machine Learning</li> <li>Matching</li> <li>Add support for <code>hypotheses()</code> to <code>inferences()</code>. Thanks to @Tristan-Siegfried for code contribution #908.</li> <li>Support <code>survival::survreg()</code>. Thanks to Carlisle Rainey for Report #911.</li> <li><code>column_names</code> argument in <code>print.marginaleffects()</code> to suppress the printed column names at the bottom of the printout.</li> <li>The function supplied to the <code>comparison</code> argument of the <code>comparisons()</code> function can now operate on <code>x</code> and on <code>newdata</code> directly (e.g., to check the number of observations).</li> <li>More informative errors from <code>predict()</code>.</li> </ul> <p>Bugs:</p> <ul> <li>Some <code>gamlss</code> models generated an error related to the <code>what</code> argument. Thanks to @DHLocke for Issue #933</li> </ul>"},{"location":"NEWS/#0151","title":"0.15.1","text":"<ul> <li><code>hypotheses()</code>: The <code>FUN</code> argument handles <code>group</code> columns gracefully.</li> <li>Native support for <code>Amelia</code> for multiple imputation.</li> </ul> <p>Documentation:</p> <ul> <li>New section on \"Complex aggregations\" in the Hypothesis testing vignette.</li> </ul> <p>Bug fix:</p> <ul> <li>Results of the <code>predictions()</code> function could be inaccurate when (a) running version 0.15.0, (b) <code>type</code> is <code>NULL</code> or <code>invlink(link)</code>, (c) model is <code>glm()</code>, and (d) the <code>hypothesis</code> argument is non-numeric. Thanks to @strengejacke for report #903</li> </ul>"},{"location":"NEWS/#0150","title":"0.15.0","text":"<p>New:</p> <ul> <li>Conformal prediction via <code>inferences()</code></li> <li><code>hypothesis</code> argument now accepts multiple string formulas.</li> <li>The <code>type</code> argument now accepts an explicit <code>invlink(link)</code> value instead of silently back-transforming. Users are no longer pointed to <code>type_dictionary</code>. Instead, they should call their function with a bad <code>type</code> value, and they will obtain a list of valid types. The default <code>type</code> value is printed in the output. This is useful because the default <code>type</code> value is <code>NULL</code>, so the user often does not explicitly decide.</li> <li>Allow install with Rcpp 1.0.0 and greater.</li> </ul> <p>Support new models:</p> <ul> <li><code>survey::svyolr()</code></li> </ul> <p>Misc:</p> <ul> <li><code>inferences(method=\"simulation\")</code> uses the original point estimate rather than the mean of the simulation distribution. Issue #851.</li> <li>Better documentation and error messages for <code>newdata=NULL</code></li> <li>Some performance improvements for <code>predictions()</code> and <code>marginalmeans()</code> (#880, #882, @etiennebacher).</li> </ul> <p>Bug fix:</p> <ul> <li><code>newdata=\"median\"</code> returned mean of binary variables. Thanks to @jkhanson1970 for report #896.</li> </ul>"},{"location":"NEWS/#0140","title":"0.14.0","text":"<p>Breaking changes:</p> <ul> <li>Row order of the output changes for some objects. Rows are not sorted alphabetically by <code>term</code>, <code>by</code>, and variables explicitly supplied to <code>datagrid</code>. This can affect hypothesis tests computed using the b1, b2, b3, and other indices.</li> <li>New procedure <code>numderiv</code> argument use a different procedure to select the step size used in the finite difference numeric derivative used to compute standard errors: abs(x) * sqrt(.Machine$double.eps). The numerical results may not be exactly identical to previous versions of <code>marginaleffects</code>, but the step size should be adequate in a broader variety of cases. Note that users can use the <code>numderiv</code> argument for more control on numeric differentiation, as documented.</li> <li><code>bife</code> models are no longer supported pending investigation in weird results in the tests. Looking for volunteers write more thorough tests.</li> </ul> <p>New:</p> <ul> <li>Support: <code>logistf</code> package.</li> <li>Support: <code>DCchoice</code> package.</li> <li>Support: <code>stats::nls</code></li> <li><code>hypotheses()</code> can now accept raw data frame, which gives a lot of flexibility for custom contrasts and functions. See the Hypothesis vignette for an example.</li> <li><code>numderiv</code> argument allows users to use finite difference (center or forward) or Richardson's method to compute the numerical derivatives used in the calculation of standard errors.</li> </ul> <p>Bug fixes:</p> <ul> <li><code>inferences()</code> supports the <code>cross</code> argument for <code>comparisons()</code> objects. Thanks to Kirill Solovev for report #856.</li> <li><code>splines::bs()</code> in formulas could produce incorrect results due to weirdness in <code>stats::model.matrix()</code>. Thanks to @chiungming for report #831.</li> <li><code>mgcv</code> with <code>ocat</code> are now supported. Thanks to Lorenzo Fabbri for Issue #844.</li> <li><code>quantreg</code> problem with <code>rowid</code> merge did not affect estimates but did not return the full original data. Issue #829.</li> <li><code>get_modeldata()</code> extracts weights variable when available.</li> <li><code>predictions()</code> is no longer broken in some <code>inferences()</code> calls. Issue #853</li> <li>Inaccurate averaging with <code>comparison=differenceavg</code> some models where all predictors are categorical. Thanks to Karl Ove Hufthammer for report #865.</li> </ul> <p>Misc:</p> <ul> <li>Major refactor to simplify the code base and make maintenance easier.</li> </ul>"},{"location":"NEWS/#0130","title":"0.13.0","text":"<p>Breaking change:</p> <ul> <li><code>glmmTMB</code>: Standard errors are no longer supported because they may have been erroneous. Follow Issue #810 on Github for developments: https://github.com/vincentarelbundock/marginaleffects/issues/810</li> </ul> <p>New:</p> <ul> <li><code>hypothesis</code> argument accepts wildcards: <code>hypothesis = \"b*=b1\"</code></li> <li><code>s.value</code> column in all output: Shannon transforms for p values. See Greenland (2019).</li> <li><code>marginal_means</code> supports <code>mira</code> (<code>mice</code> objects).</li> <li><code>comparisons()</code>: The <code>variables</code> arguments now accepts arbitrary numeric vectors of length equal to the number of rows in <code>newdata</code>. This allows users to specify fully custom treatment sizes. In the documentation examples, we show how to estimate the difference for a 1 standard deviation shift in a regressor, where the standard deviation is calculated on a group-wise basis.</li> <li><code>comparisons()</code>: the <code>variables</code> argument now accepts \"revpairwise\", \"revsequential\", \"revreference\" for factor and character variables.</li> <li><code>comparisons()</code>: the <code>comparison</code> argument now accept \"lift\" and \"liftavg\".</li> </ul> <p>Performance:</p> <ul> <li>Computing elasticities for linear models is now up to 30% faster (#787, @etiennebacher).</li> </ul> <p>Bug fixes:</p> <ul> <li>Better handling of environments when <code>newdata</code> is a function call. Thanks to @jcccf for report #814 and to @capnrefsmmat for the proposed fix using the <code>rlang</code> package.</li> <li>Degrees of freedom mismatch for joint hypothesis tests. Thanks to @snhansen for report #789.</li> </ul>"},{"location":"NEWS/#0120","title":"0.12.0","text":"<p>Breaking change:</p> <ul> <li>Row order of output has changed for many calls, especially those using the <code>by</code> argument. This may break hypothesis tests conducted by indexing <code>b1</code>, <code>b2</code>, etc. This was necessary to fix Issue #776. Thanks to @marcora for the report.</li> </ul> <p>New:</p> <ul> <li><code>hypotheses()</code>: Joint hypothesis tests (F and Chi-square) with the <code>joint</code> and <code>joint_test</code> arguments.</li> <li><code>vcov.hypotheses</code> method.</li> <li><code>wts</code> is now available in <code>plot_predictions()</code>, <code>plot_comparisons()</code>, and <code>plot_slopes()</code>.</li> </ul> <p>Bug:</p> <ul> <li>Wrong order of rows in bayesian models with <code>by</code> argument. Thanks to @shirdekel for report #782.</li> </ul>"},{"location":"NEWS/#0112","title":"0.11.2","text":"<ul> <li><code>vcov()</code> and <code>coef()</code> methods for <code>marginaleffects</code> objects.</li> <li>Strings in <code>wts</code> are accepted with the <code>by</code> argument.</li> <li><code>predictions()</code> and <code>avg_predictions()</code> no longer use an automatic backtransformation for GLM models unless <code>hypothesis</code> is <code>NULL</code>.</li> <li><code>vcov()</code> can be used to retrieve a full variance-covariance matrix from objects produced by <code>comparisons()</code>, <code>slopes()</code>, <code>predictions()</code>, or <code>marginal_means()</code> objects.</li> <li>When processing objects obtained using <code>mice</code> multiple imputation, the pooled model using <code>mice::pool</code> is attached to the <code>model</code> attribute of the output. This means that functions like <code>modelsummary::modelsummary()</code> will not erroneously report goodness-of-fit statistics from just a single model and will instead appropriately report the statistics for the pooled model. Thanks to @Tristan-Siegfried for PR #740.</li> <li>More informative error messages on some prediction problems. Thanks to @andymilne for Report #751.</li> </ul> <p>Performance:</p> <ul> <li><code>inferences()</code> is now up to 17x faster and much more memory-efficient when <code>method</code> is <code>\"boot\"</code> or <code>\"rsample\"</code> (#770, #771, @etiennebacher).</li> </ul> <p>Bugs:</p> <ul> <li><code>brms</code> models with <code>nl=TRUE</code> and a single predictor generated an error. Thanks to @Tristan-Siegried for Report #759.</li> <li><code>avg_predictions()</code>: Incorrect group-wise averaging when all predictors are categorical, the <code>variables</code> variable is used, and  we are averaging with <code>avg_</code> or the <code>by</code> argument. Thanks to BorgeJorge for report #766.</li> <li>Bug when <code>datagrid()</code> when called inside a user-written function. Thanks to @NickCH-K for report #769 and to @capnrefsmmat for the diagnostics.</li> </ul>"},{"location":"NEWS/#0111","title":"0.11.1","text":"<p>Breaking change:</p> <ul> <li>Row orders are now more consistent, but may have changed from previous version. This could affect results from <code>hypothesis</code> with <code>b1</code>, <code>b2</code>, ... indexing.</li> </ul> <p>Support new models:</p> <ul> <li><code>nlme::lme()</code></li> <li><code>phylolm::phylolm()</code></li> <li><code>phylolm::phyloglm()</code></li> </ul> <p>New:</p> <ul> <li>Vignette on 2x2 experimental designs. Thanks to Demetri Pananos.</li> <li><code>comparisons()</code> accepts data frames with two numeric columns (\"low\" and \"high\") to specify fully customizable contrasts.</li> <li><code>datagrid()</code> gets a new <code>by</code> argument to create apply grid-making functions within groups.</li> <li><code>plot_*()</code> gain a <code>newdata</code> argument for use with <code>by</code>.</li> </ul> <p>Bug:</p> <ul> <li><code>comparisons(comparison = \"lnratioavg\")</code> ignored <code>wts</code> argument. Thanks to Demetri Pananos for report #737.</li> <li><code>ordinal::clm()</code>: incorrect standard errors when location and scale parameters are the same. Thanks to MrJerryTAO for report #718.</li> <li>Incorrect label for \"2sd\" comparisons. Thanks to Andy Milne for report #720.</li> <li>Invalid factor levels in <code>datagrid()</code> means <code>newdata</code> argument gets ignored. Thanks to Josh Errickson for report #721.</li> <li>Error in models with only categorical predictors and the <code>by</code> argument. Thanks to Sam Brilleman for report #723.</li> <li>Elasticities are now supported for <code>ordinal::clm()</code> models. Thanks to MrJerryTAO for report #729.</li> <li><code>glmmTMB</code> models with zero-inflated components are supported. Thanks to @Helsinki-Ronan and @strengejacke for report #734.</li> </ul>"},{"location":"NEWS/#0110","title":"0.11.0","text":"<p>Breaking changes:</p> <ul> <li><code>type</code> column is replaced by <code>type</code> attribute.</li> <li><code>predictions()</code> only works with officially supported model types (same list as <code>comparisons()</code> and <code>slopes()</code>).</li> </ul> <p>Renamed arguments (backward compatibility is preserved):</p> <ul> <li><code>transform_pre</code> -&gt; <code>comparison</code></li> <li><code>transform_post</code> -&gt; <code>transform</code></li> </ul> <p>New:</p> <ul> <li><code>p_adjust</code> argument: Adjust p-values for multiple comparisons. </li> <li><code>equivalence</code> argument available everywhere.</li> </ul> <p>Performance:</p> <ul> <li>Much faster results in <code>avg_*()</code> functions for models with only categorical predictors and many rows of data, using deduplication and weights instead of unit-level estimates.</li> <li>Faster predictions in <code>lm()</code> and <code>glm()</code> models using <code>RcppEigen</code>.</li> <li>Bayesian models with many rows. Thanks to Etienne Bacher. #694 </li> <li>Faster predictions, especially with standard errors and large datasets.</li> </ul> <p>Bugs:</p> <ul> <li>Multiple imputation with <code>mira</code> objects was not pooling all datasets. Thanks to  @Generalized for report #711.</li> <li>Support for more models with offsets. Thanks to @mariofiorini for report #705.</li> <li>Error on <code>predictions()</code> with <code>by</code> and <code>wts</code>. Thanks to Noah Greifer for report #695.</li> <li><code>afex</code>: some models generated errors. Thanks to  Daniel L\u00fcdecke for report #696.</li> <li><code>group</code> column name is always forbidden. Thanks to Daniel L\u00fcdecke for report #697.</li> <li>Blank graphs in <code>plot_comparisons()</code> with a list in <code>variables</code>.</li> <li><code>type=\"link\"</code> produced an error with some categorical <code>brms</code> models. Thanks to @shirdekel for report #703.</li> <li>Error on <code>predictions(variables = ...)</code> for <code>glmmTMB</code> models. Thanks to Daniel L\u00fcdecke for report #707.</li> <li><code>by</code> with user-specified function in <code>comparison</code> and factor predictor did not aggregate correctly. Thanks to @joaotedde for report #715.</li> <li><code>ordinal::clm</code>: Support <code>cum.prob</code> and <code>linear.predictor</code> prediction types. Thanks to @MrJerryTAO for report #717.</li> </ul>"},{"location":"NEWS/#0100","title":"0.10.0","text":"<p>Performance:</p> <ul> <li>2-4x faster execution for many calls. Thanks to Etienne Bacher.</li> </ul> <p>New models supported:</p> <ul> <li><code>MCMCglmm::MCMCglmm</code></li> <li><code>Rchoice::hetprob</code></li> <li><code>Rchoice::ivpml</code></li> <li>Multiple imputation using <code>mice</code> and any package which can return a list of imputed data frames (e.g., <code>Amelia</code>, <code>missRanger</code>, etc.)</li> </ul> <p>Plot improvements:</p> <ul> <li>New <code>by</code> argument to display marginal estimates by subgroup.</li> <li>New <code>rug</code> argument to display tick marks in the margins.</li> <li>New <code>points</code> argument in <code>plot_predictions()</code> to display a scatter plot.</li> <li>New <code>gray</code> argument to plot in grayscale using line types and shapes instead of color.</li> <li>The <code>effect</code> argument is renamed to <code>variables</code> in <code>plot_slopes()</code> and <code>plot_comparisons()</code>. This improves consistency with the analogous <code>slopes()</code> and <code>comparisons()</code> functions.</li> <li>The plotting vignette was re-written.</li> </ul> <p>Other:</p> <ul> <li>Support multiple imputation with <code>mice</code> <code>mira</code> objects. The multiple imputation vignette was rewritten.</li> <li>The <code>variables_grid</code> argument in <code>marginal_means()</code> is renamed <code>newdata</code>. Backward compatibility is maintained.</li> <li><code>avg_*()</code> returns an informative error when <code>vcov</code> is \"satterthwaite\" or \"kenward-roger\"</li> <li>\"satterthwaite\" and \"kenward-roger\" are now supported when <code>newdata</code> is not <code>NULL</code></li> <li>Informative error when <code>hypothesis</code> includes a <code>b#</code> larger than the available number of estimates.</li> <li><code>avg_predictions(model, variables = \"x\")</code> computes average counterfactual predictions by subgroups of <code>x</code></li> <li><code>datagrid()</code> and <code>plot_*()</code> functions are faster in datasets with many extraneous columns.</li> <li>In <code>predictions(type = NULL)</code> with <code>glm()</code> and <code>Gam()</code> we first make predictions on the link scale and then backtransform them. Setting <code>type=\"response\"</code> explicitly makes predictions directly on the response scale without backtransformation.</li> <li>Standard errors now supported for more <code>glmmTMB</code> models.</li> <li>Use the <code>numDeriv</code> package for numeric differentiation in the calculation of delta method standard error. A global option can now be passed to <code>numDeriv::jacobian</code>:</li> <li><code>options(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-6)))</code></li> <li><code>options(marginaleffects_numDeriv = list(method = \"Richardson\", method.args = list(eps = 1e-6)))</code></li> <li><code>options(marginaleffects_numDeriv = NULL)</code></li> <li>Print:</li> <li>Print fewer significant digits.</li> <li><code>print.marginaleffects</code> now prints all columns supplied to <code>newdata</code></li> <li>Less redundant labels when using <code>hypothesis</code></li> <li>Many improvements to documentation.</li> </ul> <p>Bugfixes:</p> <ul> <li>Standard errors could be inaccurate in models with non-linear components (and interactions) when some of the coefficients were very small. This was related to the step size used for numerical differentiation for the delta method. Issue #684.</li> <li><code>avg_predictions(by =)</code> did not work when the dataset included a column named <code>term</code>. Issue #683.</li> <li><code>brms</code> models with multivariate outcome collapsed categories in <code>comparisons()</code>. Issue #639.</li> <li><code>hypotheses()</code> now works on lists and in calls to <code>lapply()</code>, <code>purrr::map()</code>, etc. Issue #660.</li> </ul>"},{"location":"NEWS/#090","title":"0.9.0","text":"<p>Breaking changes:</p> <ul> <li>All functions return an <code>estimate</code> column instead of the function-specific <code>predicted</code>, <code>comparisons</code>, <code>dydx</code>, etc. This change only affects unit-level estimates, and not average estimates, which already used the <code>estimate</code> column name.</li> <li>The <code>transform_avg</code> argument in <code>tidy()</code> deprecated. Use <code>transform_post</code> instead.</li> <li><code>plot_*(draw=FALSE)</code> now return the actual variable names supplied to the <code>condition</code> argument, rather than the opaque \"condition1\", \"condition2\", etc.</li> </ul> <p>New models supported:</p> <ul> <li><code>blme</code> package.</li> </ul> <p>New features:</p> <ul> <li>New functions: <code>avg_predictions()</code>, <code>avg_comparisons()</code>, <code>avg_slopes()</code></li> <li>Equivalence, non-inferiority, and non-superiority tests with the <code>hypotheses()</code> function and <code>equivalence</code> argument.</li> <li>New experimental <code>inferences()</code> function: simulation-based inferences and bootstrap using the <code>boot</code>, <code>rsample</code>, and <code>fwb</code> package.</li> <li>New <code>df</code> argument to set degrees of freedom manually for p and CI.</li> <li>Pretty <code>print()</code> for all objects.</li> <li><code>by</code> argument</li> <li><code>TRUE</code> returns average (marginal) predictions, comparisons, or slopes.</li> <li>Supports bayesian models.</li> <li><code>hypothesis</code> argument</li> <li>Numeric value sets the null used in calculating Z and p.</li> <li>Example: <code>comparisons(mod, transform_pre = \"ratio\", hypothesis = 1)</code></li> <li>All arguments from the main functions are now available through <code>tidy()</code>, and <code>summary()</code>: <code>conf_level</code>, <code>transform_post</code>, etc.</li> <li>Bayesian posterior distribution summaries (median, mean, HDI, quantiles) can be customized using global options. See <code>?comparisons</code></li> </ul> <p>Renamed functions (backward-compatibility is maintained by keeping the old function names as aliases):</p> <ul> <li><code>marginaleffects()</code> -&gt; <code>slopes()</code> </li> <li><code>posteriordraws()</code> -&gt; <code>posterior_draws()</code> </li> <li><code>marginalmeans()</code> -&gt; <code>marginal_means()</code> </li> <li><code>plot_cap()</code> -&gt; <code>plot_predictions()</code></li> <li><code>plot_cme()</code> -&gt; <code>plot_slopes()</code></li> <li><code>plot_cco()</code> -&gt; <code>plot_comparisons()</code></li> </ul> <p>Bug fixes:</p> <ul> <li>Incorrect results: In 0.8.1, <code>plot_*()</code> the <code>threenum</code> and <code>minmax</code> labels did not correspond to the correct numeric values.</li> <li>Fix corner case for slopes when the dataset includes infinite values.</li> <li><code>mlogit</code> error with factors.</li> <li>The <code>vcov</code> argument now accepts functions for most models.</li> </ul> <p>Other:</p> <ul> <li>Removed major performance bottleneck for <code>slopes()</code></li> </ul>"},{"location":"NEWS/#081","title":"0.8.1","text":"<ul> <li><code>deltamethod()</code> can run hypothesis tests on objects produced by the <code>comparisons()</code>, <code>marginaleffects()</code>, <code>predictions()</code>, and <code>marginalmeans()</code> functions. This feature relies on <code>match.call()</code>, which means it may not always work when used programmatically, inside functions and nested environments. It is generally safer and more efficient to use the <code>hypothesis</code> argument.</li> <li><code>plot_cme()</code> and <code>plot_cco()</code> accept lists with user-specified values for the regressors, and can display nice labels for shortcut string-functions like \"threenum\" or \"quartile\".</li> <li><code>posterior_draws</code>: new <code>shape</code> argument to return MCMC draws in various formats, including the new <code>rvar</code> structure from the <code>posterior</code> package.</li> <li><code>transform_avg</code> function gets printed in <code>summary()</code> output.</li> <li><code>transform_post</code> and <code>transform_avg</code> support string shortcuts: \"exp\" and \"ln\"</li> <li>Added support for <code>mlm</code> models from <code>lm()</code>. Thanks to Noah Greifer.</li> </ul> <p>Bug fixes:</p> <ul> <li><code>hypothesis</code> argument with bayesian models and <code>tidy()</code> used to raise an error.</li> <li>Missing values for some regressors in the <code>comparisons()</code> output for <code>brms</code> models.</li> </ul>"},{"location":"NEWS/#080","title":"0.8.0","text":"<p>Breaking change:</p> <ul> <li>The <code>interaction</code> argument is deprecated and replaced by the <code>cross</code> argument. This is to reduce ambiguity with respect to the <code>interaction</code> argument in <code>emmeans</code>, which does something completely different, akin to the difference-in-differences illustrated in the Interactions vignette.</li> </ul> <p>71 classes of models supported, including the new:</p> <ul> <li><code>rms::ols</code></li> <li><code>rms::lrm</code></li> <li><code>rms::orm</code></li> </ul> <p>New features:</p> <ul> <li>Plots: <code>plot_cme()</code>, <code>plot_cap()</code>, and <code>plot_cco()</code> are now much more flexible in specifying the comparisons to display. The <code>condition</code> argument accepts lists, functions, and shortcuts for common reference values, such as \"minmax\", \"threenum\", etc.</li> <li><code>variables</code> argument of the <code>comparisons()</code> function is more flexible:</li> <li>Accepts functions to specify custom differences in numeric variables (e.g., forward and backward differencing).</li> <li>Can specify pairs of factors to compare in the <code>variables</code> argument of the <code>comparisons</code> function.</li> <li><code>variables</code> argument of the <code>predictions()</code> function is more flexible:</li> <li>Accepts shortcut strings, functions, and vectors of arbitrary length.</li> <li>Integrate out random effects in bayesian <code>brms</code> models (see Bayesian analysis vignette)</li> </ul> <p>New vignettes:</p> <ul> <li>Experiments</li> <li>Extending marginal effects</li> <li>Integrating out random effects in bayesian models</li> </ul> <p>Bug fixes and minor improvements:</p> <ul> <li>The default value of <code>conf_level</code> in <code>summary()</code> and <code>tidy()</code> is now <code>NULL</code>, which inherits the <code>conf_level</code> value in the original <code>comparisons</code>/<code>marginaleffects</code>/<code>predictions</code> calls.</li> <li>Fix typo in function names for missing \"lnratioavgwts\"</li> <li>Interactions with <code>fixest::i()</code> are parsed properly as categorical variables</li> <li>For <code>betareg</code> objects, inference can now be done on all coefficients using <code>deltamethod()</code>. previously only the location coefficients were available.</li> <li>For objects from <code>crch</code> package, a number of bugs have been fixed; standard errors should now be correct for <code>deltamethod()</code>, <code>marginaleffects()</code>, etc.</li> <li>Fixed a bug in the <code>tidy()</code> function for <code>glmmTMB</code> models without random effects, which caused all t statistics to be identical.</li> </ul>"},{"location":"NEWS/#071","title":"0.7.1","text":"<ul> <li>New supported model class: <code>gamlss</code>. Thanks to Marcio Augusto Diniz.</li> <li><code>marginalmeans()</code> accepts a <code>wts</code> argument with values: \"equal\", \"proportional\", \"cells\".</li> <li><code>by</code> argument </li> <li>accepts data frames for complex groupings.</li> <li>in <code>marginalmeans</code> only accepts data frames.</li> <li>accepts \"group\" to group by response level.</li> <li>works with bayesian models.</li> <li><code>byfun</code> argument for the <code>predictions()</code> function to aggregate using different functions.</li> <li><code>hypothesis</code> argument</li> <li>The matrix column names are used as labels for hypothesis tests.</li> <li>Better labels with \"sequential\", \"reference\", \"pairwise\".</li> <li>new shortcuts \"revpairwise\", \"revsequential\", \"revreference\"</li> <li><code>wts</code> argument is respected in <code>by</code> argument and with <code>*avg</code> shortcuts in the <code>transform_pre</code> argument.</li> <li><code>tidy.predictions()</code> and <code>tidy.marginalmeans()</code> get a new <code>transform_avg</code> argument.</li> <li>New vignettes: </li> <li>Unit-level contrasts in logistic regressions. Thanks to @arthur-albuquerque.</li> <li>Python Numpy models in <code>marginaleffects</code>. Thanks to @timpipeseek.</li> <li>Bootstrap example in standard errors vignette.</li> </ul>"},{"location":"NEWS/#070","title":"0.7.0","text":"<p>Breaking changes:</p> <ul> <li><code>by</code> is deprecated in <code>summary()</code> and <code>tidy()</code>. Use the same <code>by</code> argument in the main functions instead: <code>comparisons()</code>, <code>marginaleffects()</code>, <code>predictions()</code></li> <li>Character vectors are no longer supported in the <code>variables</code> argument of the <code>predictions()</code> function. Use <code>newdata=\"fivenum\"</code> or \"grid\", \"mean\", or \"median\" instead.</li> </ul> <p>Critical bug fix:</p> <ul> <li>Contrasts with interactions were incorrect in version 0.6.0. The error should have been obvious to most analysts in most cases (weird-looking alignment). Thanks to @vmikk. </li> </ul> <p>New supported packages and models:</p> <ul> <li><code>survival::clogit</code></li> <li><code>biglm</code>: The main quantities can be computed, but not the delta method standard errors. See https://github.com/vincentarelbundock/marginaleffects/issues/387</li> </ul> <p>New vignette:</p> <ul> <li>Elasticity</li> <li>Frequently Asked Questions</li> </ul> <p>New features:</p> <ul> <li>Elasticity and semi-elasticity using the new <code>slope</code> argument in <code>marginaleffects()</code>: eyex, dyex, eydx</li> <li><code>datagrid()</code> accepts functions: <code>datagrid(newdata = mtcars, hp = range, mpg = fivenum, wt = sd)</code></li> <li>New <code>datagridcf()</code> function to create counterfactual datasets. This is a shortcut to the <code>datagrid()</code> function with default to <code>grid_type = \"counterfactual\"</code></li> <li>New <code>by</code> arguments in <code>predictions()</code>, <code>comparisons()</code>, <code>marginaleffects()</code></li> <li>New <code>newdata</code> shortcuts: \"tukey\", \"grid\"</li> <li>New string shortcuts for <code>transform_pre</code> in <code>comparisons()</code></li> <li><code>marginalmeans()</code> now back transforms confidence intervals when possible.</li> <li><code>vcov</code> argument string shortcuts are now case-insensitive</li> <li>The default contrast in <code>comparisons()</code> for binary predictors is now a difference between 1 and 0, rather than +1 relative to baseline.</li> <li>documentation improvements</li> </ul>"},{"location":"NEWS/#060","title":"0.6.0","text":"<p>New supported packages and models:</p> <ul> <li><code>tidymodels</code> objects of class <code>tidy_model</code> are supported if the fit engine is supported by <code>marginaleffects</code>.</li> </ul> <p>New function:</p> <ul> <li><code>deltamethod()</code>: Hypothesis tests on functions of parameters</li> <li><code>plot_cco()</code>: Plot conditional contrasts</li> </ul> <p>New arguments:</p> <ul> <li><code>hypothesis</code> for hypothesis tests and custom contrasts</li> <li><code>transform_post</code> in <code>predictions()</code></li> <li><code>wts</code> argument in <code>predictions()</code> only affects average predictions in <code>tidy()</code> or <code>summary()</code>.</li> </ul> <p>New or improved vignettes:</p> <ul> <li>Hypothesis Tests and Custom Contrasts using the Delta Method: https://marginaleffects.com/articles/hypothesis.html</li> <li>Multiple Imputation: https://marginaleffects.com/articles/multiple_imputation.html</li> <li>Causal Inference with the g-Formula: https://marginaleffects.com/articles/gcomputation.html  (Thanks to Rohan Kapre for the idea)</li> </ul> <p>Deprecated or renamed arguments:</p> <ul> <li><code>contrast_factor</code> and <code>contrast_numeric</code> arguments are deprecated in <code>comparisons()</code>. Use a named list in the <code>variables</code> argument instead. Backward compatibility is maintained.</li> <li>The <code>transform_post</code> argument in <code>tidy()</code> and <code>summary()</code> is renamed to <code>transform_avg</code> to disambiguate against the argument of the same name in <code>comparisons()</code>. Backward compatibility is preserved.</li> </ul> <p>Misc:</p> <ul> <li><code>tidy.predictions()</code> computes standard errors using the delta method for average predictions</li> <li>Support <code>gam</code> models with matrix columns.</li> <li><code>eps</code> in <code>marginaleffects()</code> is now \"adaptive\" by default: it equals 0.0001 multiplied the range of the predictor variable</li> <li><code>comparisons()</code> now supports \"log of marginal odds ratio\" in the <code>transform_pre</code> argument. Thanks to Noah Greifer.</li> <li>New <code>transform_pre</code> shortcuts: dydx, expdydx</li> <li><code>tidy.predictions()</code> computes standard errors and confidence intervals for linear models or GLM on the link scale.</li> </ul>"},{"location":"NEWS/#050","title":"0.5.0","text":"<p>Breaking changes:</p> <ul> <li><code>type</code> no longer accepts a character vector. Must be a single string.</li> <li><code>conf.int</code> argument deprecated. Use <code>vcov = FALSE</code> instead.</li> </ul> <p>New supported packages and models: </p> <ul> <li><code>mlogit</code></li> <li><code>mhurdle</code></li> <li><code>tobit1</code></li> <li><code>glmmTMB</code></li> </ul> <p>New features:</p> <ul> <li><code>interaction</code> argument in <code>comparisons()</code> to compute interactions between contrasts (cross-contrasts).</li> <li><code>by</code> argument in <code>tidy()</code> and <code>summary()</code> computes group-average marginal effects and comparisons.</li> <li><code>transform_pre</code> argument can define custom contrasts between adjusted predictions (e.g., log adjusted risk ratios). Available in <code>comparisons()</code>.</li> <li><code>transform_post</code> argument allows back transformation before returning the final results. Available in <code>comparisons()</code>, <code>marginalmeans()</code>, <code>summary()</code>, <code>tidy()</code>.</li> <li>The <code>variables</code> argument of the <code>comparisons()</code> function accepts a named list to specify variable-specific contrast types.</li> <li>Robust standard errors with the <code>vcov</code> argument. This requires version 0.17.1 of the <code>insight</code> package.</li> <li><code>sandwich</code> package shortcuts: <code>vcov = \"HC3\"</code>, <code>\"HC2\"</code>, <code>\"NeweyWest\"</code>, and more.</li> <li>Mixed effects models: <code>vcov = \"satterthwaite\"</code> or <code>\"kenward-roger\"</code></li> <li>One-sided formula to clusters: <code>vcov = ~cluster_variable</code></li> <li>Variance-covariance matrix</li> <li>Function which returns a named squared matrix</li> <li><code>marginalmeans()</code> allows interactions</li> <li>Bayesian Model Averaging for <code>brms</code> models using <code>type = \"average\"</code>. See vignette on the <code>marginaleffects</code> website.</li> <li><code>eps</code> argument for step size of numerical derivative</li> <li><code>marginaleffects</code> and <code>comparisons</code> now report confidence intervals by default.</li> <li>New dependency on the <code>data.table</code> package yields substantial performance improvements.</li> <li>More informative error messages and warnings</li> <li>Bug fixes and performance improvements</li> </ul> <p>New pages on the <code>marginaleffects</code> website: https://marginaleffects.com/</p> <ul> <li>Alternative software packages</li> <li>Robust standard errors (and more)</li> <li>Performance tips</li> <li>Tables and plots</li> <li>Multinomial Logit and Discrete Choice Models</li> <li>Generalized Additive Models</li> <li>Mixed effects models (Bayesian and Frequentist)</li> <li>Transformations and Custom Contrasts: Adjusted Risk Ratio Example</li> </ul> <p>Argument name changes (backward compatibility is preserved:</p> <ul> <li>Everywhere:<ul> <li><code>conf.level</code> -&gt; <code>conf_level</code> </li> </ul> </li> <li><code>datagrid()</code>:<ul> <li><code>FUN.factor</code> -&gt; <code>FUN_factor</code> (same for related arguments)</li> <li><code>grid.type</code> -&gt; <code>grid_type</code></li> </ul> </li> </ul>"},{"location":"NEWS/#041","title":"0.4.1","text":"<p>New supported packages and models: </p> <ul> <li><code>stats::loess</code></li> <li><code>sampleSelection::selection</code></li> <li><code>sampleSelection::heckit</code></li> </ul> <p>Misc:</p> <ul> <li><code>mgcv::bam</code> models allow <code>exclude</code> argument. </li> <li>Gam models allow <code>include_smooth</code> argument. </li> <li>New tests</li> <li>Bug fixes</li> </ul>"},{"location":"NEWS/#040","title":"0.4.0","text":"<p>New function:</p> <ul> <li><code>comparisons()</code> computes contrasts</li> </ul> <p>Misc:</p> <ul> <li>Speed optimizations</li> <li><code>predictions()</code> and <code>plot_cap()</code> include confidence intervals for linear models</li> <li>More robust handling of in-formula functions: factor(), strata(), mo()</li> <li>Do not overwrite user's <code>ggplot2::theme_set()</code> call</li> </ul>"},{"location":"NEWS/#034","title":"0.3.4","text":"<ul> <li>Bug fixes</li> </ul>"},{"location":"NEWS/#033","title":"0.3.3","text":"<p>New supported models:</p> <ul> <li><code>mclogit::mclogit</code></li> <li><code>robust::lmRob</code></li> <li><code>robustlmm::rlmer</code></li> <li><code>fixest</code> confidence intervals in <code>predictions</code></li> </ul> <p>Misc:</p> <ul> <li>Support <code>modelbased::visualisation_matrix</code> in <code>newdata</code> without having to specify <code>x</code> explicitly. </li> <li><code>tidy.predictions()</code> and <code>summary.predictions()</code> methods.</li> <li>Documentation improvements.</li> <li>CRAN test fixes</li> </ul>"},{"location":"NEWS/#032","title":"0.3.2","text":"<p>Support for new models and packages:</p> <ul> <li><code>brglm2::bracl</code></li> <li><code>mclogit::mblogit</code></li> <li><code>scam::scam</code></li> <li><code>lmerTest::lmer</code></li> </ul> <p>Misc:</p> <ul> <li>Drop <code>numDeriv</code> dependency, but make it available via a global option:   options(\"marginaleffects_numDeriv\" = list(method = \"Richardson\", method.args = list(eps = 1e-5, d = 0.0001)))</li> <li>Bugfixes</li> <li>Documentation improvements</li> <li>CRAN tests</li> </ul>"},{"location":"NEWS/#031","title":"0.3.1","text":"<p>documentation bugfix</p>"},{"location":"NEWS/#030","title":"0.3.0","text":"<p>Breaking changes:</p> <ul> <li><code>predictions</code> returns predictions for every observation in the original   dataset instead of <code>newdata=datagrid()</code>.</li> <li><code>marginalmeans</code> objects have new column names, as do the corresponding <code>tidy</code>   and <code>summary</code> outputs.</li> </ul> <p>New supported packages and models:</p> <ul> <li><code>brms::brm</code></li> <li><code>rstanarm::stanglm</code></li> <li><code>brglm2::brmultinom</code></li> <li><code>MASS::glmmPQL</code></li> <li><code>aod::betabin</code></li> </ul> <p>Misc:</p> <ul> <li><code>datagrid</code> function supersedes <code>typical</code> and <code>counterfactual</code> with the <code>grid.type</code>   argument. The <code>typical</code> and <code>counterfactual</code> functions will remain available   and exported, but their use is not encouraged.</li> <li><code>posterior_draws</code> function can be applied to a <code>predictions</code> or a   <code>marginaleffects</code> object to extract draws from the posterior distribution.</li> <li><code>marginalmeans</code> standard errors are now computed using the delta method.</li> <li><code>predictions</code> standard errors are now computed using the delta method when they are not available from <code>insight::get_predicted</code>.</li> <li>New vignette on Bayesian models with <code>brms</code></li> <li>New vignette on Mixed effects models with <code>lme4</code></li> <li>If the <code>data.table</code> package is installed, <code>marginaleffects</code> will automatically use it to speed things up.</li> <li>Contrast definition reported in a separate column of <code>marginaleffects</code> output.</li> <li>Safer handling of the <code>type</code> argument.</li> <li>Comprehensive list of supported and tests models on the website.</li> <li>Many bug fixes</li> <li>Many new tests, including several against <code>emmeans</code></li> </ul>"},{"location":"NEWS/#020","title":"0.2.0","text":"<p>Breaking change:</p> <ul> <li><code>data</code> argument becomes <code>newdata</code> in all functions.</li> </ul> <p>New supported packages and models:</p> <ul> <li><code>lme4:glmer.nb</code></li> <li><code>mgcv::gam</code></li> <li><code>ordinal::clm</code></li> <li><code>mgcv</code></li> </ul> <p><code>marginalmeans</code>:</p> <ul> <li>New <code>variables_grid</code> argument</li> </ul> <p><code>predictions</code>:</p> <ul> <li>Support <code>mgcv</code></li> </ul> <p><code>plot_cap</code></p> <ul> <li>New <code>type</code> argument</li> </ul> <p>Misc:</p> <ul> <li>New validity checks and tests</li> </ul>"},{"location":"NEWS/#010","title":"0.1.0","text":"<p>First release. Bravo!</p> <p>Thanks to Marco Avina Mendoza, Resul Umit, and all those who offered comments and suggestions.</p>"},{"location":"man/comparisons/","title":"comparisons","text":"<p>Source code</p> <p>Comparisons Between Predictions Made With Different Regressor Values</p>"},{"location":"man/comparisons/#description","title":"Description","text":"<p>Predict the outcome variable at different regressor values (e.g., college graduates vs.\u00a0others), and compare those predictions by computing a difference, ratio, or some other function. <code>comparisons()</code> can return many quantities of interest, such as contrasts, differences, risk ratios, changes in log odds, lift, slopes, elasticities, etc.</p> <ul> <li> <code>comparisons()</code>: unit-level (conditional) estimates.  </li> <li> <code>avg_comparisons()</code>: average (marginal) estimates.  </li> </ul> <p><code>variables</code> identifies the focal regressors whose \"effect\" we are interested in. <code>comparison</code> determines how predictions with different regressor values are compared (difference, ratio, odds, etc.). The <code>newdata</code> argument and the <code>datagrid()</code> function control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.</p> <p>See the comparisons vignette and package website for worked examples and case studies:</p> <ul> <li> https://marginaleffects.com/articles/comparisons.html </li> <li> https://marginaleffects.com/ </li> </ul>"},{"location":"man/comparisons/#usage","title":"Usage","text":"<pre><code>comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  comparison = \"difference\",\n  type = NULL,\n  vcov = TRUE,\n  by = FALSE,\n  conf_level = 0.95,\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  vcov = TRUE,\n  by = TRUE,\n  conf_level = 0.95,\n  comparison = \"difference\",\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n</code></pre>"},{"location":"man/comparisons/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>newdata</code>   Grid of predictor values at which we evaluate the comparisons.  <ul> <li>  Warning: Please avoid modifying your dataset between fitting the model and calling a <code>marginaleffects</code> function. This can sometimes lead to unexpected results.  </li> <li> <code>NULL</code> (default): Unit-level contrasts for each observed value in the dataset (empirical distribution). The dataset is retrieved using <code>insight::get_data()</code>, which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.  </li> <li>  data frame: Unit-level contrasts for each row of the <code>newdata</code> data frame.  </li> <li>  string:  <ul> <li>  \"mean\": Contrasts at the Mean. Contrasts when each predictor is held at its mean or mode.  </li> <li>  \"median\": Contrasts at the Median. Contrasts when each predictor is held at its median or mode.  </li> <li>  \"marginalmeans\": Contrasts at Marginal Means.  </li> <li>  \"tukey\": Contrasts at Tukey\u2019s 5 numbers.  </li> <li>  \"grid\": Contrasts on a grid of representative numbers (Tukey\u2019s 5 numbers and unique values of categorical predictors).  </li> </ul> </li> <li> <code>datagrid()</code> call to specify a custom grid of regressors. For example:  <ul> <li> <code>newdata = datagrid(cyl = c(4, 6))</code>: <code>cyl</code> variable equal to 4 and 6 and other regressors fixed at their means or modes.  </li> <li> <code>newdata = datagrid(mpg = fivenum)</code>: <code>mpg</code> variable held at Tukey\u2019s five numbers (using the <code>fivenum</code> function), and other regressors fixed at their means or modes.  </li> <li>  See the Examples section and the datagrid documentation.  </li> </ul> </li> </ul> <code>variables</code>   Focal variables  <ul> <li> <code>NULL</code>: compute comparisons for all the variables in the model object (can be slow).  </li> <li>  Character vector: subset of variables (usually faster).  </li> <li>  Named list: names identify the subset of variables of interest, and values define the type of contrast to compute. Acceptable values depend on the variable type:  <ul> <li>  Factor or character variables:  <ul> <li>  \"reference\": Each factor level is compared to the factor reference (base) level  </li> <li>  \"all\": All combinations of observed levels  </li> <li>  \"sequential\": Each factor level is compared to the previous factor level  </li> <li>  \"pairwise\": Each factor level is compared to all other levels  </li> <li>  \"minmax\": The highest and lowest levels of a factor.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses.  </li> <li>  Vector of length 2 with the two values to compare.  </li> <li>  Data frame with the same number of rows as <code>newdata</code>, with two columns of \"lo\" and \"hi\" values to compare.  </li> <li>  Function that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.  </li> </ul> </li> <li>  Logical variables:  <ul> <li>  NULL: contrast between TRUE and FALSE  </li> <li>  Data frame with the same number of rows as <code>newdata</code>, with two columns of \"lo\" and \"hi\" values to compare.  </li> <li>  Function that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.  </li> </ul> </li> <li>  Numeric variables:  <ul> <li>  Numeric of length 1: Forward contrast for a gap of <code>x</code>, computed between the observed value and the observed value plus <code>x</code>. Users can set a global option to get a \"center\" or \"backward\" contrast instead: <code>options(marginaleffects_contrast_direction=\u201ccenter\u201d)</code> </li> <li>  Numeric vector of length 2: Contrast between the largest and the smallest elements of the <code>x</code> vector.  </li> <li>  Data frame with the same number of rows as <code>newdata</code>, with two columns of \"lo\" and \"hi\" values to compare.  </li> <li>  Function that accepts a vector and returns a data frame with two columns of \"lo\" and \"hi\" values to compare. See examples below.  </li> <li>  \"iqr\": Contrast across the interquartile range of the regressor.  </li> <li>  \"sd\": Contrast across one standard deviation around the regressor mean.  </li> <li>  \"2sd\": Contrast across two standard deviations around the regressor mean.  </li> <li>  \"minmax\": Contrast between the maximum and the minimum values of the regressor.  </li> </ul> </li> <li>  Examples:  <ul> <li> <code>variables = list(gear = \u201cpairwise\u201d, hp = 10)</code> </li> <li> <code>variables = list(gear = \u201csequential\u201d, hp = c(100, 120))</code> </li> <li>  `variables = list(hp = \\(x) data.frame(low = x - 5, high = x + 10))`  </li> <li>  See the Examples section below for more.  </li> </ul> </li> </ul> </li> </ul> <code>comparison</code>   How should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.  <ul> <li>  string: shortcuts to common contrast functions.  <ul> <li>  Supported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts  </li> <li>  See the Comparisons section below for definitions of each transformation.  </li> </ul> </li> <li>  function: accept two equal-length numeric vectors of adjusted predictions (<code>hi</code> and <code>lo</code>) and returns a vector of contrasts of the same length, or a unique numeric value.  <ul> <li>  See the Transformations section below for examples of valid functions.  </li> </ul> </li> </ul> <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>by</code>   Aggregate unit-level estimates (aka, marginalize, average over). Valid inputs:  <ul> <li> <code>FALSE</code>: return the original unit-level estimates.  </li> <li> <code>TRUE</code>: aggregate estimates for each term.  </li> <li>  Character vector of column names in <code>newdata</code> or in the data frame produced by calling the function without the <code>by</code> argument.  </li> <li>  Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  </li> <li>  See examples below.  </li> <li>  For more complex aggregations, you can use the <code>FUN</code> argument of the <code>hypotheses()</code> function. See that function\u2019s documentation and the Hypothesis Test vignettes on the <code>marginaleffects</code> website.  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>transform</code>  string or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"  <code>cross</code> <ul> <li> <code>FALSE</code>: Contrasts represent the change in adjusted predictions when one predictor changes and all other variables are held constant.  </li> <li> <code>TRUE</code>: Contrasts represent the changes in adjusted predictions when all the predictors specified in the <code>variables</code> argument are manipulated simultaneously (a \"cross-contrast\").  </li> </ul> <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>hypothesis</code>   specify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.  <ul> <li>  Numeric:  <ul> <li>  Single value: the null hypothesis used in the computation of Z and p (before applying <code>transform</code>).  </li> <li>  Vector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the <code>hypothesis</code> argument.  </li> <li>  Matrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.  </li> </ul> </li> <li>  String formula to specify linear or non-linear hypothesis tests. If the <code>term</code> column uniquely identifies rows, terms can be used in the formula. Otherwise, use <code>b1</code>, <code>b2</code>, etc. to identify the position of each parameter. The `b*` wildcard can be used to test hypotheses on all estimates. Examples:  <ul> <li> <code>hp = drat</code> </li> <li> <code>hp + drat = 12</code> </li> <li> <code>b1 + b2 + b3 = 0</code> </li> <li>  `b* / b1 = 1`  </li> </ul> </li> <li>  String:  <ul> <li>  \"pairwise\": pairwise differences between estimates in each row.  </li> <li>  \"reference\": differences between the estimates in each row and the estimate in the first row.  </li> <li>  \"sequential\": difference between an estimate and the estimate in the next row.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.  </li> </ul> </li> <li>  See the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html  </li> </ul> <code>equivalence</code>  Numeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.  <code>p_adjust</code>  Adjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust  <code>df</code>  Degrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and <code>Inf</code>. When <code>df</code> is <code>Inf</code>, the normal distribution is used. When <code>df</code> is finite, the <code>t</code> distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: <code>slopes(model, df = insight::get_df(model))</code> <code>eps</code>  NULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When <code>eps</code> is <code>NULL</code>, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing <code>eps</code> may be necessary to avoid numerical problems in certain models.  <code>numderiv</code>   string or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.  <ul> <li>  \"fdforward\": finite difference method with forward differences  </li> <li>  \"fdcenter\": finite difference method with central differences (default)  </li> <li>  \"richardson\": Richardson extrapolation method  </li> <li>  Extra arguments can be specified by passing a list to the <code>numDeriv</code> argument, with the name of the method first and named arguments following, ex: <code>numderiv=list(\u201cfdcenter\u201d, eps = 1e-5)</code>. When an unknown argument is used, <code>marginaleffects</code> prints the list of valid arguments for each method.  </li> </ul> <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/comparisons/#value","title":"Value","text":"<p>A <code>data.frame</code> with one row per observation (per term/group) and several columns:</p> <ul> <li> <code>rowid</code>: row number of the <code>newdata</code> data frame  </li> <li> <code>type</code>: prediction type, as defined by the <code>type</code> argument  </li> <li> <code>group</code>: (optional) value of the grouped outcome (e.g., categorical outcome models)  </li> <li> <code>term</code>: the variable whose marginal effect is computed  </li> <li> <code>dydx</code>: slope of the outcome with respect to the term, for a given combination of predictor values  </li> <li> <code>std.error</code>: standard errors computed by via the delta method.  </li> <li> <code>p.value</code>: p value associated to the <code>estimate</code> column. The null is determined by the <code>hypothesis</code> argument (0 by default), and p values are computed before applying the <code>transform</code> argument.  </li> <li> <code>s.value</code>: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst\u2019s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.\u00a0(2020).  </li> <li> <code>conf.low</code>: lower bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> <li> <code>conf.high</code>: upper bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> </ul> <p>See <code>?print.marginaleffects</code> for printing options.</p>"},{"location":"man/comparisons/#functions","title":"Functions","text":"<ul> <li> <code>avg_comparisons()</code>: Average comparisons  </li> </ul>"},{"location":"man/comparisons/#standard-errors-using-the-delta-method","title":"Standard errors using the delta method","text":"<p>Standard errors for all quantities estimated by <code>marginaleffects</code> can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to <code>1e-8</code>, or to <code>1e-4</code> times the smallest absolute model coefficient, whichever is largest.</p> <p><code>marginaleffects</code> can delegate numeric differentiation to the <code>numDeriv</code> package, which allows more flexibility. To do this, users can pass arguments to the <code>numDeriv::jacobian</code> function through a global option. For example:</p> <ul> <li> <code>options(marginaleffects_numDeriv = list(method = \u201csimple\u201d, method.args = list(eps = 1e-6)))</code> </li> <li> <code>options(marginaleffects_numDeriv = list(method = \u201cRichardson\u201d, method.args = list(eps = 1e-5)))</code> </li> <li> <code>options(marginaleffects_numDeriv = NULL)</code> </li> </ul> <p>See the \"Standard Errors and Confidence Intervals\" vignette on the <code>marginaleffects</code> website for more details on the computation of standard errors:</p> <p>https://marginaleffects.com/articles/uncertainty.html</p> <p>Note that the <code>inferences()</code> function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:</p> <p>https://marginaleffects.com/articles/bootstrap.html</p>"},{"location":"man/comparisons/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/comparisons/#comparison-argument-functions","title":"comparison argument functions","text":"<p>The following transformations can be applied by supplying one of the shortcut strings to the <code>comparison</code> argument. <code>hi</code> is a vector of adjusted predictions for the \"high\" side of the contrast. <code>lo</code> is a vector of adjusted predictions for the \"low\" side of the contrast. <code>y</code> is a vector of adjusted predictions for the original data. <code>x</code> is the predictor in the original data. <code>eps</code> is the step size to use to compute derivatives and elasticities.</p>  Shortcut   Function   difference   (hi, lo) hi - lo   differenceavg   (hi, lo) mean(hi - lo)   dydx   (hi, lo, eps) (hi - lo)/eps   eyex   (hi, lo, eps, y, x) (hi - lo)/eps \\* (x/y)   eydx   (hi, lo, eps, y, x) ((hi - lo)/eps)/y   dyex   (hi, lo, eps, x) ((hi - lo)/eps) \\* x   dydxavg   (hi, lo, eps) mean((hi - lo)/eps)   eyexavg   (hi, lo, eps, y, x) mean((hi - lo)/eps \\* (x/y))   eydxavg   (hi, lo, eps, y, x) mean(((hi - lo)/eps)/y)   dyexavg   (hi, lo, eps, x) mean(((hi - lo)/eps) \\* x)   ratio   (hi, lo) hi/lo   ratioavg   (hi, lo) mean(hi)/mean(lo)   lnratio   (hi, lo) log(hi/lo)   lnratioavg   (hi, lo) log(mean(hi)/mean(lo))   lnor   (hi, lo) log((hi/(1 - hi))/(lo/(1 - lo)))   lnoravg   (hi, lo) log((mean(hi)/(1 - mean(hi)))/(mean(lo)/(1 - mean(lo))))   lift   (hi, lo) (hi - lo)/lo   liftavg   (hi, lo) (mean(hi - lo))/mean(lo)   expdydx   (hi, lo, eps) ((exp(hi) - exp(lo))/exp(eps))/eps   expdydxavg   (hi, lo, eps) mean(((exp(hi) - exp(lo))/exp(eps))/eps)"},{"location":"man/comparisons/#bayesian-posterior-summaries","title":"Bayesian posterior summaries","text":"<p>By default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201ceti\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201chdi\u201d)</code></p> <p>By default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmean\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmedian\u201d)</code></p> <p>When estimates are averaged using the <code>by</code> argument, the <code>tidy()</code> function, or the <code>summary()</code> function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in <code>by</code> argument or <code>tidy()/summary()</code> functions. Then, we identify the center of the resulting posterior using the function supplied to the <code>\u201cmarginaleffects_posterior_center\u201d</code> option (the median by default).</p>"},{"location":"man/comparisons/#equivalence-inferiority-superiority","title":"Equivalence, Inferiority, Superiority","text":"<p>\u03b8 is an estimate, \u03c3<sub>\u03b8</sub> its estimated standard error, and [a,b] are the bounds of the interval supplied to the <code>equivalence</code> argument.</p> <p>Non-inferiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2264\u2004*a*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&gt;\u2004*a*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*a*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Upper-tail probability  </li> </ul> <p>Non-superiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2265\u2004*b*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&lt;\u2004*b*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*b*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Lower-tail probability  </li> </ul> <p>Equivalence: Two One-Sided Tests (TOST)</p> <ul> <li>  p: Maximum of the non-inferiority and non-superiority p values.  </li> </ul> <p>Thanks to Russell V. Lenth for the excellent <code>emmeans</code> package and documentation which inspired this feature.</p>"},{"location":"man/comparisons/#prediction-types","title":"Prediction types","text":"<p>The <code>type</code> argument determines the scale of the predictions used to compute quantities of interest with functions from the <code>marginaleffects</code> package. Admissible values for <code>type</code> depend on the model object. When users specify an incorrect value for <code>type</code>, <code>marginaleffects</code> will raise an informative error with a list of valid <code>type</code> values for the specific model object. The first entry in the list in that error message is the default type.</p> <p>The <code>invlink(link)</code> is a special type defined by <code>marginaleffects</code>. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with <code>type=\u201cinvlink(link)\u201d</code> will not always be equivalent to the average of estimates with <code>type=\u201cresponse\u201d</code>.</p> <p>Some of the most common <code>type</code> values are:</p> <p>response, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob</p>"},{"location":"man/comparisons/#references","title":"References","text":"<ul> <li>  Greenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106\u2013114.  </li> <li>  Cole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191\u201393. https://doi.org/10.1093/aje/kwaa136  </li> </ul>"},{"location":"man/comparisons/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\n\n# Linear model\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\navg_comparisons(mod, variables = list(cyl = \"reference\"))\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n\n# GLM with different scale types\nmod &lt;- glm(am ~ factor(gear), data = mtcars)\navg_comparisons(mod, type = \"response\")\navg_comparisons(mod, type = \"link\")\n\n# Contrasts at the mean\ncomparisons(mod, newdata = \"mean\")\n\n# Contrasts between marginal means\ncomparisons(mod, newdata = \"marginalmeans\")\n\n# Contrasts at user-specified values\ncomparisons(mod, newdata = datagrid(am = 0, gear = tmp$gear))\ncomparisons(mod, newdata = datagrid(am = unique, gear = max))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\ncomparisons(m, variables = \"hp\", newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Numeric contrasts\nmod &lt;- lm(mpg ~ hp, data = mtcars)\navg_comparisons(mod, variables = list(hp = 1))\navg_comparisons(mod, variables = list(hp = 5))\navg_comparisons(mod, variables = list(hp = c(90, 100)))\navg_comparisons(mod, variables = list(hp = \"iqr\"))\navg_comparisons(mod, variables = list(hp = \"sd\"))\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n\n# using a function to specify a custom difference in one regressor\ndat &lt;- mtcars\ndat$new_hp &lt;- 49 * (dat$hp - min(dat$hp)) / (max(dat$hp) - min(dat$hp)) + 1\nmodlog &lt;- lm(mpg ~ log(new_hp) + factor(cyl), data = dat)\nfdiff &lt;- \\(x) data.frame(x, x + 10)\navg_comparisons(modlog, variables = list(new_hp = fdiff))\n\n# Adjusted Risk Ratio: see the contrasts vignette\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\navg_comparisons(mod, comparison = \"lnratioavg\", transform = exp)\n\n# Adjusted Risk Ratio: Manual specification of the `comparison`\navg_comparisons(\n     mod,\n     comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n     transform = exp)\n# cross contrasts\nmod &lt;- lm(mpg ~ factor(cyl) * factor(gear) + hp, data = mtcars)\navg_comparisons(mod, variables = c(\"cyl\", \"gear\"), cross = TRUE)\n\n# variable-specific contrasts\navg_comparisons(mod, variables = list(gear = \"sequential\", hp = 10))\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n# Effect of a 1 group-wise standard deviation change\n# First we calculate the SD in each group of `cyl`\n# Second, we use that SD as the treatment size in the `variables` argument\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\ntmp &lt;- mtcars %&gt;%\n    group_by(cyl) %&gt;%\n    mutate(hp_sd = sd(hp))\navg_comparisons(mod, variables = list(hp = tmp$hp_sd), by = \"cyl\")\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\ncomparisons(mod, by = TRUE)\n\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\navg_comparisons(mod, variables = \"hp\", by = c(\"vs\", \"am\"))\n\nlibrary(nnet)\nmod &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\ncomparisons(mod, type = \"probs\", by = by)\n</code></pre>"},{"location":"man/datagrid/","title":"datagrid","text":"<p>Source code</p> <p>Data grids</p>"},{"location":"man/datagrid/#description","title":"Description","text":"<p>Generate a data grid of user-specified values for use in the <code>newdata</code> argument of the <code>predictions()</code>, <code>comparisons()</code>, and <code>slopes()</code> functions. This is useful to define where in the predictor space we want to evaluate the quantities of interest. Ex: the predicted outcome or slope for a 37 year old college graduate.</p> <ul> <li> <code>datagrid()</code> generates data frames with combinations of \"typical\" or user-supplied predictor values.  </li> <li> <code>datagridcf()</code> generates \"counter-factual\" data frames, by replicating the entire dataset once for every combination of predictor values supplied by the user.  </li> </ul>"},{"location":"man/datagrid/#usage","title":"Usage","text":"<pre><code>datagrid(\n  ...,\n  model = NULL,\n  newdata = NULL,\n  by = NULL,\n  FUN_character = get_mode,\n  FUN_factor = get_mode,\n  FUN_logical = get_mode,\n  FUN_numeric = function(x) mean(x, na.rm = TRUE),\n  FUN_integer = function(x) round(mean(x, na.rm = TRUE)),\n  FUN_other = function(x) mean(x, na.rm = TRUE),\n  grid_type = \"typical\"\n)\n\ndatagridcf(..., model = NULL, newdata = NULL)\n</code></pre>"},{"location":"man/datagrid/#arguments","title":"Arguments","text":"<code>\u2026</code>   named arguments with vectors of values or functions for user-specified variables.  <ul> <li>  Functions are applied to the variable in the <code>model</code> dataset or <code>newdata</code>, and must return a vector of the appropriate type.  </li> <li>  Character vectors are automatically transformed to factors if necessary. +The output will include all combinations of these variables (see Examples below.)  </li> </ul> <code>model</code>  Model object  <code>newdata</code>  data.frame (one and only one of the <code>model</code> and <code>newdata</code> arguments can be used.)  <code>by</code>  character vector with grouping variables within which `FUN_*` functions are applied to create \"sub-grids\" with unspecified variables.  <code>FUN_character</code>  the function to be applied to character variables.  <code>FUN_factor</code>  the function to be applied to factor variables.  <code>FUN_logical</code>  the function to be applied to logical variables.  <code>FUN_numeric</code>  the function to be applied to numeric variables.  <code>FUN_integer</code>  the function to be applied to integer variables.  <code>FUN_other</code>  the function to be applied to other variable types.  <code>grid_type</code>   character  <ul> <li>  \"typical\": variables whose values are not explicitly specified by the user in <code>\u2026</code> are set to their mean or mode, or to the output of the functions supplied to <code>FUN_type</code> arguments.  </li> <li>  \"counterfactual\": the entire dataset is duplicated for each combination of the variable values specified in <code>\u2026</code>. Variables not explicitly supplied to <code>datagrid()</code> are set to their observed values in the original dataset.  </li> </ul>"},{"location":"man/datagrid/#details","title":"Details","text":"<p>If <code>datagrid</code> is used in a <code>predictions()</code>, <code>comparisons()</code>, or <code>slopes()</code> call as the <code>newdata</code> argument, the model is automatically inserted in the <code>model</code> argument of <code>datagrid()</code> call, and users do not need to specify either the <code>model</code> or <code>newdata</code> arguments.</p> <p>If users supply a model, the data used to fit that model is retrieved using the <code>insight::get_data</code> function.</p>"},{"location":"man/datagrid/#value","title":"Value","text":"<p>A <code>data.frame</code> in which each row corresponds to one combination of the named predictors supplied by the user via the <code>\u2026</code> dots. Variables which are not explicitly defined are held at their mean or mode.</p>"},{"location":"man/datagrid/#functions","title":"Functions","text":"<ul> <li> <code>datagridcf()</code>: Counterfactual data grid  </li> </ul>"},{"location":"man/datagrid/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\n# The output only has 2 rows, and all the variables except `hp` are at their\n# mean or mode.\ndatagrid(newdata = mtcars, hp = c(100, 110))\n</code></pre> <pre><code>       mpg    cyl     disp     drat      wt     qsec     vs      am   gear\n1 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n2 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n    carb  hp\n1 2.8125 100\n2 2.8125 110\n</code></pre> <pre><code># We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndatagrid(model = mod, hp = c(100, 110))\n</code></pre> <pre><code>       mpg  hp\n1 20.09062 100\n2 20.09062 110\n</code></pre> <pre><code># Use in `marginaleffects` to compute \"Typical Marginal Effects\". When used\n# in `slopes()` or `predictions()` we do not need to specify the\n#`model` or `newdata` arguments.\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n</code></pre> <pre><code> Term  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp 100  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp 110  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response\n</code></pre> <pre><code># datagrid accepts functions\ndatagrid(hp = range, cyl = unique, newdata = mtcars)\n</code></pre> <pre><code>       mpg     disp     drat      wt     qsec     vs      am   gear   carb  hp\n1 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n2 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n3 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n4 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n5 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n6 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n  cyl\n1   6\n2   4\n3   8\n4   6\n5   4\n6   8\n</code></pre> <pre><code>comparisons(mod, newdata = datagrid(hp = fivenum))\n</code></pre> <pre><code> Term Contrast  hp Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n   hp       +1  52  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1  96  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 123  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 180  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n   hp       +1 335  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, hp, predicted_lo, predicted_hi, predicted, mpg \nType:  response\n</code></pre> <pre><code># The full dataset is duplicated with each observation given counterfactual\n# values of 100 and 110 for the `hp` variable. The original `mtcars` includes\n# 32 rows, so the resulting dataset includes 64 rows.\ndg &lt;- datagrid(newdata = mtcars, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n</code></pre> <pre><code>[1] 64\n</code></pre> <pre><code># We get the same result by feeding a model instead of a data.frame\nmod &lt;- lm(mpg ~ hp, mtcars)\ndg &lt;- datagrid(model = mod, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n</code></pre> <pre><code>[1] 64\n</code></pre>"},{"location":"man/hypotheses/","title":"hypotheses","text":"<p>Source code</p> <p>(Non-)Linear Tests for Null Hypotheses, Joint Hypotheses, Equivalence, Non Superiority, and Non Inferiority</p>"},{"location":"man/hypotheses/#description","title":"Description","text":"<p>Uncertainty estimates are calculated as first-order approximate standard errors for linear or non-linear functions of a vector of random variables with known or estimated covariance matrix. In that sense, <code>hypotheses</code> emulates the behavior of the excellent and well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models; requires fewer dependencies; expands the range of tests to equivalence and superiority/inferiority; and offers convenience features like robust standard errors.</p> <p>To learn more, read the hypothesis tests vignette, visit the package website, or scroll down this page for a full list of vignettes:</p> <ul> <li> https://marginaleffects.com/articles/hypothesis.html </li> <li> https://marginaleffects.com/ </li> </ul> <p>Warning #1: Tests are conducted directly on the scale defined by the <code>type</code> argument. For some models, it can make sense to conduct hypothesis or equivalence tests on the <code>\u201clink\u201d</code> scale instead of the <code>\u201cresponse\u201d</code> scale which is often the default.</p> <p>Warning #2: For hypothesis tests on objects produced by the <code>marginaleffects</code> package, it is safer to use the <code>hypothesis</code> argument of the original function. Using <code>hypotheses()</code> may not work in certain environments, in lists, or when working programmatically with *apply style functions.</p> <p>Warning #3: The tests assume that the <code>hypothesis</code> expression is (approximately) normally distributed, which for non-linear functions of the parameters may not be realistic. More reliable confidence intervals can be obtained using the <code>inferences()</code> function with <code>method = \u201cboot\u201d</code>.</p>"},{"location":"man/hypotheses/#usage","title":"Usage","text":"<pre><code>hypotheses(\n  model,\n  hypothesis = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  df = Inf,\n  equivalence = NULL,\n  joint = FALSE,\n  joint_test = \"f\",\n  FUN = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n</code></pre>"},{"location":"man/hypotheses/#arguments","title":"Arguments","text":"<code>model</code>  Model object or object generated by the <code>comparisons()</code>, <code>slopes()</code>, <code>predictions()</code>, or <code>marginal_means()</code> functions.  <code>hypothesis</code>   specify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.  <ul> <li>  Numeric:  <ul> <li>  Single value: the null hypothesis used in the computation of Z and p (before applying <code>transform</code>).  </li> <li>  Vector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the <code>hypothesis</code> argument.  </li> <li>  Matrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.  </li> </ul> </li> <li>  String formula to specify linear or non-linear hypothesis tests. If the <code>term</code> column uniquely identifies rows, terms can be used in the formula. Otherwise, use <code>b1</code>, <code>b2</code>, etc. to identify the position of each parameter. The `b*` wildcard can be used to test hypotheses on all estimates. Examples:  <ul> <li> <code>hp = drat</code> </li> <li> <code>hp + drat = 12</code> </li> <li> <code>b1 + b2 + b3 = 0</code> </li> <li>  `b* / b1 = 1`  </li> </ul> </li> <li>  String:  <ul> <li>  \"pairwise\": pairwise differences between estimates in each row.  </li> <li>  \"reference\": differences between the estimates in each row and the estimate in the first row.  </li> <li>  \"sequential\": difference between an estimate and the estimate in the next row.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.  </li> </ul> </li> <li>  See the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html  </li> </ul> <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>df</code>  Degrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and <code>Inf</code>. When <code>df</code> is <code>Inf</code>, the normal distribution is used. When <code>df</code> is finite, the <code>t</code> distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: <code>slopes(model, df = insight::get_df(model))</code> <code>equivalence</code>  Numeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.  <code>joint</code>   Joint test of statistical significance. The null hypothesis value can be set using the <code>hypothesis</code> argument.  <ul> <li>  FALSE: Hypotheses are not tested jointly.  </li> <li>  TRUE: All parameters are tested jointly.  </li> <li>  String: A regular expression to match parameters to be tested jointly. <code>grep(joint, perl = TRUE)</code> </li> <li>  Character vector of parameter names to be tested. Characters refer to the names of the vector returned by <code>coef(object)</code>.  </li> <li>  Integer vector of indices. Which parameters positions to test jointly.  </li> </ul> <code>joint_test</code>  A character string specifying the type of test, either \"f\" or \"chisq\". The null hypothesis is set by the <code>hypothesis</code> argument, with default null equal to 0 for all parameters.  <code>FUN</code> <code>NULL</code> or function.  <ul> <li> <code>NULL</code> (default): hypothesis test on a model\u2019s coefficients, or on the quantities estimated by one of the <code>marginaleffects</code> package functions.  </li> <li>  Function which accepts a model object and returns a numeric vector or a data.frame with two columns called <code>term</code> and <code>estimate</code>. This argument can be useful when users want to conduct a hypothesis test on an arbitrary function of quantities held in a model object. See examples below.  </li> </ul> <code>numderiv</code>   string or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.  <ul> <li>  \"fdforward\": finite difference method with forward differences  </li> <li>  \"fdcenter\": finite difference method with central differences (default)  </li> <li>  \"richardson\": Richardson extrapolation method  </li> <li>  Extra arguments can be specified by passing a list to the <code>numDeriv</code> argument, with the name of the method first and named arguments following, ex: <code>numderiv=list(\u201cfdcenter\u201d, eps = 1e-5)</code>. When an unknown argument is used, <code>marginaleffects</code> prints the list of valid arguments for each method.  </li> </ul> <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/hypotheses/#joint-hypothesis-tests","title":"Joint hypothesis tests","text":"<p>The test statistic for the joint Wald test is calculated as (R * theta_hat - r)\u2019 * inv(R * V_hat * R\u2019) * (R * theta_hat - r) / Q, where theta_hat is the vector of estimated parameters, V_hat is the estimated covariance matrix, R is a Q x P matrix for testing Q hypotheses on P parameters, r is a Q x 1 vector for the null hypothesis, and Q is the number of rows in R. If the test is a Chi-squared test, the test statistic is not normalized.</p> <p>The p-value is then calculated based on either the F-distribution (for F-test) or the Chi-squared distribution (for Chi-squared test). For the F-test, the degrees of freedom are Q and (n - P), where n is the sample size and P is the number of parameters. For the Chi-squared test, the degrees of freedom are Q.</p>"},{"location":"man/hypotheses/#equivalence-inferiority-superiority","title":"Equivalence, Inferiority, Superiority","text":"<p>\u03b8 is an estimate, \u03c3<sub>\u03b8</sub> its estimated standard error, and [a,b] are the bounds of the interval supplied to the <code>equivalence</code> argument.</p> <p>Non-inferiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2264\u2004*a*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&gt;\u2004*a*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*a*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Upper-tail probability  </li> </ul> <p>Non-superiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2265\u2004*b*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&lt;\u2004*b*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*b*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Lower-tail probability  </li> </ul> <p>Equivalence: Two One-Sided Tests (TOST)</p> <ul> <li>  p: Maximum of the non-inferiority and non-superiority p values.  </li> </ul> <p>Thanks to Russell V. Lenth for the excellent <code>emmeans</code> package and documentation which inspired this feature.</p>"},{"location":"man/hypotheses/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\n# When `FUN` and `hypotheses` are `NULL`, `hypotheses()` returns a data.frame of parameters\nhypotheses(mod)\n</code></pre> <pre><code>         Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># Test of equality between coefficients\nhypotheses(mod, hypothesis = \"hp = wt\")\n</code></pre> <pre><code>    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># Non-linear function\nhypotheses(mod, hypothesis = \"exp(hp + wt) = 0.1\")\n</code></pre> <pre><code>               Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># Robust standard errors\nhypotheses(mod, hypothesis = \"hp = wt\", vcov = \"HC3\")\n</code></pre> <pre><code>    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># b1, b2, ... shortcuts can be used to identify the position of the\n# parameters of interest in the output of FUN\nhypotheses(mod, hypothesis = \"b2 = b3\")\n</code></pre> <pre><code>    Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># wildcard\nhypotheses(mod, hypothesis = \"b* / b2 = 1\")\n</code></pre> <pre><code>        Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n b1 / b2 = 1    -1551      764.0 -2.03   0.0423 4.6 -3048.9    -54\n b2 / b2 = 1        0         NA    NA       NA  NA      NA     NA\n b3 / b2 = 1      137       78.1  1.75   0.0804 3.6   -16.6    290\n b4 / b2 = 1      144      111.0  1.30   0.1938 2.4   -73.3    362\n b5 / b2 = 1      137      151.9  0.90   0.3679 1.4  -161.0    435\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># term names with special characters have to be enclosed in backticks\nhypotheses(mod, hypothesis = \"`factor(cyl)6` = `factor(cyl)8`\")\n</code></pre> <pre><code>                            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %\n `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41\n 97.5 %\n   3.07\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code>mod2 &lt;- lm(mpg ~ hp * drat, data = mtcars)\nhypotheses(mod2, hypothesis = \"`hp:drat` = drat\")\n</code></pre> <pre><code>             Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n `hp:drat` = drat    -6.08       2.89 -2.1   0.0357 4.8 -11.8 -0.405\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># predictions(), comparisons(), and slopes()\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\ncmp &lt;- comparisons(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b1 = b2\")\n</code></pre> <pre><code>  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 %  97.5 %\n b1=b2    -0.28      0.104 -2.7  0.00684 7.2 -0.483 -0.0771\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code>mfx &lt;- slopes(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b2 = 0.2\")\n</code></pre> <pre><code>   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n b2=0.2   0.0938      0.109 0.857    0.391 1.4 -0.121  0.308\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code>pre &lt;- predictions(mod, newdata = datagrid(hp = 110, mpg = c(30, 35)))\nhypotheses(pre, hypothesis = \"b1 = b2\")\n</code></pre> <pre><code>  Term  Estimate Std. Error      z Pr(&gt;|z|)   S     2.5 %   97.5 %\n b1=b2 -3.57e-05   0.000172 -0.207    0.836 0.3 -0.000373 0.000302\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># The `FUN` argument can be used to compute standard errors for fitted values\nmod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) predict(x, type = \"link\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n</code></pre> <pre><code> Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n    1   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    2   -1.098      0.716 -1.534    0.125 3.0 -2.50  0.305\n    3    0.233      0.781  0.299    0.765 0.4 -1.30  1.764\n    4   -0.595      0.647 -0.919    0.358 1.5 -1.86  0.674\n    5   -0.418      0.647 -0.645    0.519 0.9 -1.69  0.851\n    6   -5.026      2.195 -2.290    0.022 5.5 -9.33 -0.725\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code>f &lt;- function(x) predict(x, type = \"response\", newdata = mtcars)\np &lt;- hypotheses(mod, FUN = f)\nhead(p)\n</code></pre> <pre><code> Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n    1  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    2  0.25005     0.1343 1.862  0.06257 4.0 -0.0131 0.5132\n    3  0.55803     0.1926 2.898  0.00376 8.1  0.1806 0.9355\n    4  0.35560     0.1483 2.398  0.01648 5.9  0.0650 0.6462\n    5  0.39710     0.1550 2.562  0.01041 6.6  0.0933 0.7009\n    6  0.00652     0.0142 0.459  0.64653 0.6 -0.0213 0.0344\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># Complex aggregation\n# Step 1: Collapse predicted probabilities by outcome level, for each individual\n# Step 2: Take the mean of the collapsed probabilities by group and `cyl`\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\n\naggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 &amp; 4\", \"5\")) |&gt;\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n</code></pre> <pre><code> Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n 3 &amp; 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n 3 &amp; 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n 3 &amp; 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n 5        6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n 5        4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n 5        8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n\nColumns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code># Equivalence, non-inferiority, and non-superiority tests\nmod &lt;- lm(mpg ~ hp + factor(gear), data = mtcars)\np &lt;- predictions(mod, newdata = \"median\")\nhypotheses(p, equivalence = c(17, 18))\n</code></pre> <pre><code> Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf)\n     19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404\n p (Equiv)  hp gear\n     0.951 123    3\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response\n</code></pre> <pre><code>mfx &lt;- avg_slopes(mod, variables = \"hp\")\nhypotheses(mfx, equivalence = c(-.1, .1))\n</code></pre> <pre><code> Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup)\n   hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001\n p (NonInf) p (Equiv)\n    0.00135   0.00135\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response\n</code></pre> <pre><code>cmp &lt;- avg_comparisons(mod, variables = \"gear\", hypothesis = \"pairwise\")\nhypotheses(cmp, equivalence = c(0, 10))\n</code></pre> <pre><code>              Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n (4 - 3) - (5 - 3)    -3.94       2.05 -1.92   0.0543 4.2 -7.95 0.0727\n p (NonSup) p (NonInf) p (Equiv)\n     &lt;0.001      0.973     0.973\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response\n</code></pre> <pre><code># joint hypotheses: character vector\nmodel &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n</code></pre> <pre><code>Joint hypothesis test:\nas.factor(cyl)6:hp = 0\nas.factor(cyl)8:hp = 0\n\n    F Pr(&gt;|F|) Df 1 Df 2\n 2.11    0.142    2   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre> <pre><code># joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n</code></pre> <pre><code>Joint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n as.factor(cyl)6:hp = 0\n as.factor(cyl)8:hp = 0\n\n   F Pr(&gt;|F|) Df 1 Df 2\n 5.7  0.00197    4   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre> <pre><code># joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n</code></pre> <pre><code>Joint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n\n    F Pr(&gt;|F|) Df 1 Df 2\n 6.12  0.00665    2   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre> <pre><code># joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n</code></pre> <pre><code>Joint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 1\n\n    F Pr(&gt;|F|) Df 1 Df 2\n 6.84  0.00411    2   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre> <pre><code>hypotheses(model, joint = 2:3, hypothesis = 1:2)\n</code></pre> <pre><code>Joint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 2\n\n    F Pr(&gt;|F|) Df 1 Df 2\n 7.47  0.00273    2   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre> <pre><code># joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n</code></pre> <pre><code>Joint hypothesis test:\n cyl 6 - 4 = 0\n cyl 8 - 4 = 0\n\n   F Pr(&gt;|F|) Df 1 Df 2\n 1.6    0.221    2   26\n\nColumns: statistic, p.value, df1, df2\n</code></pre>"},{"location":"man/inferences/","title":"inferences","text":"<p>Source code</p> <p>(EXPERIMENTAL) Bootstrap, Conformal, and Simulation-Based Inference</p>"},{"location":"man/inferences/#description","title":"Description","text":"<p>Warning: This function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other <code>marginaleffects</code> functions.</p> <p>Apply this function to a <code>marginaleffects</code> object to change the inferential method used to compute uncertainty estimates.</p>"},{"location":"man/inferences/#usage","title":"Usage","text":"<pre><code>inferences(\n  x,\n  method,\n  R = 1000,\n  conf_type = \"perc\",\n  conformal_test = NULL,\n  conformal_calibration = NULL,\n  conformal_score = \"residual_abs\",\n  ...\n)\n</code></pre>"},{"location":"man/inferences/#arguments","title":"Arguments","text":"<code>x</code>  Object produced by one of the core <code>marginaleffects</code> functions.  <code>method</code>   String  <ul> <li>  \"delta\": delta method standard errors  </li> <li>  \"boot\" package  </li> <li>  \"fwb\": fractional weighted bootstrap  </li> <li>  \"rsample\" package  </li> <li>  \"simulation\" from a multivariate normal distribution (Krinsky &amp; Robb, 1986)  </li> <li>  \"mi\" multiple imputation for missing data  </li> <li>  \"conformal_split\": prediction intervals using split conformal prediction (see Angelopoulos &amp; Bates, 2022)  </li> <li>  \"conformal_cv+\": prediction intervals using cross-validation+ conformal prediction (see Barber et al., 2020)  </li> </ul> <code>R</code>  Number of resamples, simulations, or cross-validation folds.  <code>conf_type</code>   String: type of bootstrap interval to construct.  <ul> <li> <code>boot</code>: \"perc\", \"norm\", \"basic\", or \"bca\"  </li> <li> <code>fwb</code>: \"perc\", \"norm\", \"basic\", \"bc\", or \"bca\"  </li> <li> <code>rsample</code>: \"perc\" or \"bca\"  </li> <li> <code>simulation</code>: argument ignored.  </li> </ul> <code>conformal_test</code>  Data frame of test data for conformal prediction.  <code>conformal_calibration</code>  Data frame of calibration data for split conformal prediction (`method=\"conformal_split`).  <code>conformal_score</code>   String. Warning: The <code>type</code> argument in <code>predictions()</code> must generate predictions which are on the same scale as the outcome variable. Typically, this means that <code>type</code> must be \"response\" or \"probs\".  <ul> <li>  \"residual_abs\" or \"residual_sq\" for regression tasks (numeric outcome)  </li> <li>  \"softmax\" for classification tasks (when <code>predictions()</code> returns a <code>group</code> columns, such as multinomial or ordinal logit models.  </li> </ul> <code>\u2026</code> <ul> <li>  If <code>method=\u201cboot\u201d</code>, additional arguments are passed to <code>boot::boot()</code>.  </li> <li>  If <code>method=\u201cfwb\u201d</code>, additional arguments are passed to <code>fwb::fwb()</code>.  </li> <li>  If <code>method=\u201crsample\u201d</code>, additional arguments are passed to <code>rsample::bootstraps()</code>.  </li> <li>  Additional arguments are ignored for all other methods.  </li> </ul>"},{"location":"man/inferences/#details","title":"Details","text":"<p>When <code>method=\u201csimulation\u201d</code>, we conduct simulation-based inference following the method discussed in Krinsky &amp; Robb (1986):</p> <ol> <li>  Draw <code>R</code> sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model\u2019s estimated coefficients and variance equal to the model\u2019s variance-covariance matrix (classical, \"HC3\", or other).  </li> <li>  Use the <code>R</code> sets of coefficients to compute <code>R</code> sets of estimands: predictions, comparisons, slopes, or hypotheses.  </li> <li>  Take quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.  </li> </ol> <p>When <code>method=\u201cfwb\u201d</code>, drawn weights are supplied to the model fitting function\u2019s <code>weights</code> argument; if the model doesn\u2019t accept non-integer weights, this method should not be used. If weights were included in the original model fit, they are extracted by <code>weights()</code> and multiplied by the drawn weights. These weights are supplied to the <code>wts</code> argument of the estimation function (e.g., <code>comparisons()</code>).</p>"},{"location":"man/inferences/#value","title":"Value","text":"<p>A <code>marginaleffects</code> object with simulation or bootstrap resamples and objects attached.</p>"},{"location":"man/inferences/#references","title":"References","text":"<p>Krinsky, I., and A. L. Robb. 1986. \u201cOn Approximating the Statistical Properties of Elasticities.\u201d Review of Economics and Statistics 68 (4): 715\u20139.</p> <p>King, Gary, Michael Tomz, and Jason Wittenberg. \"Making the most of statistical analyses: Improving interpretation and presentation.\" American journal of political science (2000): 347-361</p> <p>Dowd, Bryan E., William H. Greene, and Edward C. Norton. \"Computation of standard errors.\" Health services research 49.2 (2014): 731-750.</p> <p>Angelopoulos, Anastasios N., and Stephen Bates. 2022. \"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.\" arXiv. https://doi.org/10.48550/arXiv.2107.07511.</p> <p>Barber, Rina Foygel, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. 2020. \u201cPredictive Inference with the Jackknife+.\u201d arXiv. http://arxiv.org/abs/1905.02928.</p>"},{"location":"man/inferences/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\nlibrary(magrittr)\nset.seed(1024)\nmod &lt;- lm(Sepal.Length ~ Sepal.Width * Species, data = iris)\n\n# bootstrap\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"boot\")\n\navg_predictions(mod, by = \"Species\") %&gt;%\n  inferences(method = \"rsample\")\n\n# Fractional (bayesian) bootstrap\navg_slopes(mod, by = \"Species\") %&gt;%\n  inferences(method = \"fwb\") %&gt;%\n  posterior_draws(\"rvar\") %&gt;%\n  data.frame()\n\n# Simulation-based inference\nslopes(mod) %&gt;%\n  inferences(method = \"simulation\") %&gt;%\n  head()\n</code></pre>"},{"location":"man/marginal_means/","title":"marginal_means","text":"<p>Source code</p> <p>Marginal Means</p>"},{"location":"man/marginal_means/#description","title":"Description","text":"<p>Marginal means are adjusted predictions, averaged across a grid of categorical predictors, holding other numeric predictors at their means. To learn more, read the marginal means vignette, visit the package website, or scroll down this page for a full list of vignettes:</p> <ul> <li> https://marginaleffects.com/articles/marginalmeans.html </li> <li> https://marginaleffects.com/ </li> </ul>"},{"location":"man/marginal_means/#usage","title":"Usage","text":"<pre><code>marginal_means(\n  model,\n  variables = NULL,\n  newdata = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  transform = NULL,\n  cross = FALSE,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  wts = \"equal\",\n  by = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n</code></pre>"},{"location":"man/marginal_means/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>variables</code>   Focal variables  <ul> <li>  Character vector of variable names: compute marginal means for each category of the listed variables.  </li> <li> <code>NULL</code>: calculate marginal means for all logical, character, or factor variables in the dataset used to fit <code>model</code>. Hint: Set <code>cross=TRUE</code> to compute marginal means for combinations of focal variables.  </li> </ul> <code>newdata</code>   Grid of predictor values over which we marginalize.  <ul> <li>  Warning: Please avoid modifying your dataset between fitting the model and calling a <code>marginaleffects</code> function. This can sometimes lead to unexpected results.  </li> <li> <code>NULL</code> create a grid with all combinations of all categorical predictors in the model. Warning: can be expensive.  </li> <li>  Character vector: subset of categorical variables to use when building the balanced grid of predictors. Other variables are held to their mean or mode.  </li> <li>  Data frame: A data frame which includes all the predictors in the original model. The full dataset is replicated once for every combination of the focal variables in the <code>variables</code> argument, using the <code>datagridcf()</code> function.  </li> </ul> <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>type</code>  string indicates the type (scale) of the predictions used to compute marginal effects or contrasts. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>transform</code>  A function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.  <code>cross</code>   TRUE or FALSE  <ul> <li> <code>FALSE</code> (default): Marginal means are computed for each predictor individually.  </li> <li> <code>TRUE</code>: Marginal means are computed for each combination of predictors specified in the <code>variables</code> argument.  </li> </ul> <code>hypothesis</code>   specify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.  <ul> <li>  Numeric:  <ul> <li>  Single value: the null hypothesis used in the computation of Z and p (before applying <code>transform</code>).  </li> <li>  Vector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the <code>hypothesis</code> argument.  </li> <li>  Matrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.  </li> </ul> </li> <li>  String formula to specify linear or non-linear hypothesis tests. If the <code>term</code> column uniquely identifies rows, terms can be used in the formula. Otherwise, use <code>b1</code>, <code>b2</code>, etc. to identify the position of each parameter. The `b*` wildcard can be used to test hypotheses on all estimates. Examples:  <ul> <li> <code>hp = drat</code> </li> <li> <code>hp + drat = 12</code> </li> <li> <code>b1 + b2 + b3 = 0</code> </li> <li>  `b* / b1 = 1`  </li> </ul> </li> <li>  String:  <ul> <li>  \"pairwise\": pairwise differences between estimates in each row.  </li> <li>  \"reference\": differences between the estimates in each row and the estimate in the first row.  </li> <li>  \"sequential\": difference between an estimate and the estimate in the next row.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.  </li> </ul> </li> <li>  See the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html  </li> </ul> <code>equivalence</code>  Numeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.  <code>p_adjust</code>  Adjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust  <code>df</code>  Degrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and <code>Inf</code>. When <code>df</code> is <code>Inf</code>, the normal distribution is used. When <code>df</code> is finite, the <code>t</code> distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: <code>slopes(model, df = insight::get_df(model))</code> <code>wts</code>   character value. Weights to use in the averaging.  <ul> <li>  \"equal\": each combination of variables in <code>newdata</code> gets equal weight.  </li> <li>  \"cells\": each combination of values for the variables in the <code>newdata</code> gets a weight proportional to its frequency in the original data.  </li> <li>  \"proportional\": each combination of values for the variables in <code>newdata</code> \u2013 except for those in the <code>variables</code> argument \u2013 gets a weight proportional to its frequency in the original data.  </li> </ul> <code>by</code>  Collapse marginal means into categories. Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  <code>numderiv</code>   string or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.  <ul> <li>  \"fdforward\": finite difference method with forward differences  </li> <li>  \"fdcenter\": finite difference method with central differences (default)  </li> <li>  \"richardson\": Richardson extrapolation method  </li> <li>  Extra arguments can be specified by passing a list to the <code>numDeriv</code> argument, with the name of the method first and named arguments following, ex: <code>numderiv=list(\u201cfdcenter\u201d, eps = 1e-5)</code>. When an unknown argument is used, <code>marginaleffects</code> prints the list of valid arguments for each method.  </li> </ul> <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/marginal_means/#details","title":"Details","text":"<p>This function begins by calling the <code>predictions</code> function to obtain a grid of predictors, and adjusted predictions for each cell. The grid includes all combinations of the categorical variables listed in the <code>variables</code> and <code>newdata</code> arguments, or all combinations of the categorical variables used to fit the model if <code>newdata</code> is <code>NULL</code>. In the prediction grid, numeric variables are held at their means.</p> <p>After constructing the grid and filling the grid with adjusted predictions, <code>marginal_means</code> computes marginal means for the variables listed in the <code>variables</code> argument, by average across all categories in the grid.</p> <p><code>marginal_means</code> can only compute standard errors for linear models, or for predictions on the link scale, that is, with the <code>type</code> argument set to \"link\".</p> <p>The <code>marginaleffects</code> website compares the output of this function to the popular <code>emmeans</code> package, which provides similar but more advanced functionality: https://marginaleffects.com/</p>"},{"location":"man/marginal_means/#value","title":"Value","text":"<p>Data frame of marginal means with one row per variable-value combination.</p>"},{"location":"man/marginal_means/#standard-errors-using-the-delta-method","title":"Standard errors using the delta method","text":"<p>Standard errors for all quantities estimated by <code>marginaleffects</code> can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to <code>1e-8</code>, or to <code>1e-4</code> times the smallest absolute model coefficient, whichever is largest.</p> <p><code>marginaleffects</code> can delegate numeric differentiation to the <code>numDeriv</code> package, which allows more flexibility. To do this, users can pass arguments to the <code>numDeriv::jacobian</code> function through a global option. For example:</p> <ul> <li> <code>options(marginaleffects_numDeriv = list(method = \u201csimple\u201d, method.args = list(eps = 1e-6)))</code> </li> <li> <code>options(marginaleffects_numDeriv = list(method = \u201cRichardson\u201d, method.args = list(eps = 1e-5)))</code> </li> <li> <code>options(marginaleffects_numDeriv = NULL)</code> </li> </ul> <p>See the \"Standard Errors and Confidence Intervals\" vignette on the <code>marginaleffects</code> website for more details on the computation of standard errors:</p> <p>https://marginaleffects.com/articles/uncertainty.html</p> <p>Note that the <code>inferences()</code> function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:</p> <p>https://marginaleffects.com/articles/bootstrap.html</p>"},{"location":"man/marginal_means/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/marginal_means/#bayesian-posterior-summaries","title":"Bayesian posterior summaries","text":"<p>By default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201ceti\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201chdi\u201d)</code></p> <p>By default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmean\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmedian\u201d)</code></p> <p>When estimates are averaged using the <code>by</code> argument, the <code>tidy()</code> function, or the <code>summary()</code> function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in <code>by</code> argument or <code>tidy()/summary()</code> functions. Then, we identify the center of the resulting posterior using the function supplied to the <code>\u201cmarginaleffects_posterior_center\u201d</code> option (the median by default).</p>"},{"location":"man/marginal_means/#equivalence-inferiority-superiority","title":"Equivalence, Inferiority, Superiority","text":"<p>\u03b8 is an estimate, \u03c3<sub>\u03b8</sub> its estimated standard error, and [a,b] are the bounds of the interval supplied to the <code>equivalence</code> argument.</p> <p>Non-inferiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2264\u2004*a*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&gt;\u2004*a*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*a*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Upper-tail probability  </li> </ul> <p>Non-superiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2265\u2004*b*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&lt;\u2004*b*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*b*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Lower-tail probability  </li> </ul> <p>Equivalence: Two One-Sided Tests (TOST)</p> <ul> <li>  p: Maximum of the non-inferiority and non-superiority p values.  </li> </ul> <p>Thanks to Russell V. Lenth for the excellent <code>emmeans</code> package and documentation which inspired this feature.</p>"},{"location":"man/marginal_means/#prediction-types","title":"Prediction types","text":"<p>The <code>type</code> argument determines the scale of the predictions used to compute quantities of interest with functions from the <code>marginaleffects</code> package. Admissible values for <code>type</code> depend on the model object. When users specify an incorrect value for <code>type</code>, <code>marginaleffects</code> will raise an informative error with a list of valid <code>type</code> values for the specific model object. The first entry in the list in that error message is the default type.</p> <p>The <code>invlink(link)</code> is a special type defined by <code>marginaleffects</code>. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with <code>type=\u201cinvlink(link)\u201d</code> will not always be equivalent to the average of estimates with <code>type=\u201cresponse\u201d</code>.</p> <p>Some of the most common <code>type</code> values are:</p> <p>response, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob</p>"},{"location":"man/marginal_means/#references","title":"References","text":"<ul> <li>  Greenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106\u2013114.  </li> <li>  Cole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191\u201393. https://doi.org/10.1093/aje/kwaa136  </li> </ul>"},{"location":"man/marginal_means/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\n\n# simple marginal means for each level of `cyl`\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ carb + cyl + am, dat)\n\nmarginal_means(\n  mod,\n  variables = \"cyl\")\n</code></pre> <pre><code> Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  cyl     4 23.1       1.66 13.9   &lt;0.001 144.3  19.9   26.4\n  cyl     6 20.4       1.34 15.2   &lt;0.001 171.9  17.8   23.0\n  cyl     8 16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># collapse levels of cyl by averaging\nby &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  by = c(\"4 &amp; 6\", \"4 &amp; 6\", \"8\"))\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by)\n</code></pre> <pre><code>    By Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 4 &amp; 6 21.7       1.13 19.2   &lt;0.001 270.8  19.5   24.0\n 8     16.2       1.07 15.1   &lt;0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># pairwise differences between collapsed levels\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by,\n  hypothesis = \"pairwise\")\n</code></pre> <pre><code>      Term Mean Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n 4 &amp; 6 - 8 5.54       1.51 3.66   &lt;0.001 12.0  2.57    8.5\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># cross\nmarginal_means(mod,\n  variables = c(\"cyl\", \"carb\"),\n  cross = TRUE)\n</code></pre> <pre><code> Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n 25.8       1.26 20.43   &lt;0.001 305.7 23.34   28.3\n 25.6       1.17 21.93   &lt;0.001 351.8 23.30   27.9\n 25.3       2.37 10.71   &lt;0.001  86.6 20.70   30.0\n 21.9       1.90 11.51   &lt;0.001  99.4 18.15   25.6\n 20.3       3.77  5.39   &lt;0.001  23.7 12.91   27.7\n 19.8       3.81  5.18   &lt;0.001  22.1 12.29   27.2\n 23.1       1.77 13.08   &lt;0.001 127.4 19.63   26.5\n 22.9       1.87 12.24   &lt;0.001 112.0 19.20   26.5\n 22.6       2.37  9.56   &lt;0.001  69.5 17.98   27.2\n 19.1       1.34 14.31   &lt;0.001 151.8 16.53   21.8\n 17.6       3.00  5.85   &lt;0.001  27.6 11.68   23.5\n 17.0       3.48  4.89   &lt;0.001  19.9 10.21   23.9\n 18.9       1.94  9.74   &lt;0.001  72.1 15.11   22.7\n 18.7       1.57 11.90   &lt;0.001 106.0 15.61   21.8\n 18.4       1.83 10.07   &lt;0.001  76.8 14.85   22.0\n 15.0       1.20 12.53   &lt;0.001 117.2 12.63   17.3\n 13.4       3.36  3.99   &lt;0.001  13.9  6.81   20.0\n 12.9       3.00  4.28   &lt;0.001  15.7  6.98   18.8\n\nResults averaged over levels of: am \nColumns: cyl, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># collapsed cross\nby &lt;- expand.grid(\n  cyl = unique(mtcars$cyl),\n  carb = unique(mtcars$carb))\nby$by &lt;- ifelse(\n  by$cyl == 4,\n  paste(\"Control:\", by$carb),\n  paste(\"Treatment:\", by$carb))\n\n\n# Convert numeric variables to categorical before fitting the model\ndat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$carb &lt;- as.factor(dat$carb)\nmod &lt;- lm(mpg ~ hp + am + carb, data = dat)\n\n# Compute and summarize marginal means\nmarginal_means(mod)\n</code></pre> <pre><code> Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n am   FALSE 17.9      1.244 14.37   &lt;0.001 153.0  15.4   20.3\n am   TRUE  23.1      0.974 23.72   &lt;0.001 410.9  21.2   25.0\n carb 1     22.0      1.345 16.35   &lt;0.001 197.2  19.4   24.6\n carb 2     21.5      1.025 20.95   &lt;0.001 321.5  19.5   23.5\n carb 3     20.6      1.780 11.55   &lt;0.001 100.1  17.1   24.0\n carb 4     18.8      1.042 18.06   &lt;0.001 239.9  16.8   20.9\n carb 6     18.5      3.019  6.12   &lt;0.001  30.0  12.6   24.4\n carb 8     21.6      4.055  5.33   &lt;0.001  23.3  13.7   29.6\n\nResults averaged over levels of: hp, am, carb \nColumns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># Contrast between marginal means (carb2 - carb1), or \"is the 1st marginal means equal to the 2nd?\"\n# see the vignette on \"Hypothesis Tests and Custom Contrasts\" on the `marginaleffects` website.\nlc &lt;- c(-1, 1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = \"b2 = b1\")\n</code></pre> <pre><code>  Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b2=b1 -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code>marginal_means(mod, variables = \"carb\", hypothesis = lc)\n</code></pre> <pre><code>   Term   Mean Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n custom -0.514       1.48 -0.348    0.728 0.5 -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code># Multiple custom contrasts\nlc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    -1, 1, 0, 0, 0, 0\n    ),\n  ncol = 2,\n  dimnames = list(NULL, c(\"A\", \"B\")))\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n</code></pre> <pre><code> Term   Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    A  1.199       6.15  0.195    0.845 0.2 -10.85  13.25\n    B -0.514       1.48 -0.348    0.728 0.5  -3.41   2.38\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre>"},{"location":"man/plot_comparisons/","title":"plot_comparisons","text":"<p>Source code</p> <p>Plot Conditional or Marginal Comparisons</p>"},{"location":"man/plot_comparisons/#description","title":"Description","text":"<p>Plot comparisons on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).</p> <p>The <code>by</code> argument is used to plot marginal comparisons, that is, comparisons made on the original data, but averaged by subgroups. This is analogous to using the <code>by</code> argument in the <code>comparisons()</code> function.</p> <p>The <code>condition</code> argument is used to plot conditional comparisons, that is, comparisons made on a user-specified grid. This is analogous to using the <code>newdata</code> argument and <code>datagrid()</code> function in a <code>comparisons()</code> call.</p> <p>All unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the <code>variables</code> argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:</p> <ul> <li>  https://marginaleffects.com/articles/plot.html  </li> <li>  https://marginaleffects.com  </li> </ul>"},{"location":"man/plot_comparisons/#usage","title":"Usage","text":"<pre><code>plot_comparisons(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  comparison = \"difference\",\n  transform = NULL,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n</code></pre>"},{"location":"man/plot_comparisons/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>variables</code>  Name of the variable whose contrast we want to plot on the y-axis.  <code>condition</code>   Conditional slopes  <ul> <li>  Character vector (max length 4): Names of the predictors to display.  </li> <li>  Named list (max length 4): List names correspond to predictors. List elements can be:  <ul> <li>  Numeric vector  </li> <li>  Function which returns a numeric vector or a set of unique categorical values  </li> <li>  Shortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"  </li> </ul> </li> <li>  1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).  </li> <li>  Numeric variables in positions 2 and 3 are summarized by Tukey\u2019s five numbers <code>?stats::fivenum</code>.  </li> </ul> <code>by</code>   Aggregate unit-level estimates (aka, marginalize, average over). Valid inputs:  <ul> <li> <code>FALSE</code>: return the original unit-level estimates.  </li> <li> <code>TRUE</code>: aggregate estimates for each term.  </li> <li>  Character vector of column names in <code>newdata</code> or in the data frame produced by calling the function without the <code>by</code> argument.  </li> <li>  Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  </li> <li>  See examples below.  </li> <li>  For more complex aggregations, you can use the <code>FUN</code> argument of the <code>hypotheses()</code> function. See that function\u2019s documentation and the Hypothesis Test vignettes on the <code>marginaleffects</code> website.  </li> </ul> <code>newdata</code>  When <code>newdata</code> is <code>NULL</code>, the grid is determined by the <code>condition</code> argument. When <code>newdata</code> is not <code>NULL</code>, the argument behaves in the same way as in the <code>comparisons()</code> function.  <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>comparison</code>   How should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.  <ul> <li>  string: shortcuts to common contrast functions.  <ul> <li>  Supported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, lift, liftavg, expdydx, expdydxavg, expdydxavgwts  </li> <li>  See the Comparisons section below for definitions of each transformation.  </li> </ul> </li> <li>  function: accept two equal-length numeric vectors of adjusted predictions (<code>hi</code> and <code>lo</code>) and returns a vector of contrasts of the same length, or a unique numeric value.  <ul> <li>  See the Transformations section below for examples of valid functions.  </li> </ul> </li> </ul> <code>transform</code>  string or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"  <code>rug</code>  TRUE displays tick marks on the axes to mark the distribution of raw data.  <code>gray</code>  FALSE grayscale or color plot  <code>draw</code> <code>TRUE</code> returns a <code>ggplot2</code> plot. <code>FALSE</code> returns a <code>data.frame</code> of the underlying data.  <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/plot_comparisons/#value","title":"Value","text":"<p>A <code>ggplot2</code> object</p>"},{"location":"man/plot_comparisons/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/plot_comparisons/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_comparisons(mod, variables = \"hp\", condition = \"drat\")\n</code></pre> <pre><code>plot_comparisons(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n</code></pre> <pre><code>plot_comparisons(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n</code></pre> <pre><code>plot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n</code></pre> <pre><code>plot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))\n</code></pre>"},{"location":"man/plot_predictions/","title":"plot_predictions","text":"<p>Source code</p> <p>Plot Conditional or Marginal Predictions</p>"},{"location":"man/plot_predictions/#description","title":"Description","text":"<p>Plot predictions on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).</p> <p>The <code>by</code> argument is used to plot marginal predictions, that is, predictions made on the original data, but averaged by subgroups. This is analogous to using the <code>by</code> argument in the <code>predictions()</code> function.</p> <p>The <code>condition</code> argument is used to plot conditional predictions, that is, predictions made on a user-specified grid. This is analogous to using the <code>newdata</code> argument and <code>datagrid()</code> function in a <code>predictions()</code> call.</p> <p>All unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the <code>variables</code> argument, or supply model-specific arguments to compute population-level estimates. See details below.</p> <p>See the \"Plots\" vignette and website for tutorials and information on how to customize plots:</p> <ul> <li>  https://marginaleffects.com/articles/plot.html  </li> <li>  https://marginaleffects.com  </li> </ul>"},{"location":"man/plot_predictions/#usage","title":"Usage","text":"<pre><code>plot_predictions(\n  model,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  transform = NULL,\n  points = 0,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n</code></pre>"},{"location":"man/plot_predictions/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>condition</code>   Conditional predictions  <ul> <li>  Character vector (max length 4): Names of the predictors to display.  </li> <li>  Named list (max length 4): List names correspond to predictors. List elements can be:  <ul> <li>  Numeric vector  </li> <li>  Function which returns a numeric vector or a set of unique categorical values  </li> <li>  Shortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"  </li> </ul> </li> <li>  1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).  </li> <li>  Numeric variables in positions 2 and 3 are summarized by Tukey\u2019s five numbers <code>?stats::fivenum</code> </li> </ul> <code>by</code>   Marginal predictions  <ul> <li>  Character vector (max length 3): Names of the categorical predictors to marginalize across.  </li> <li>  1: x-axis. 2: color. 3: facets.  </li> </ul> <code>newdata</code>  When <code>newdata</code> is <code>NULL</code>, the grid is determined by the <code>condition</code> argument. When <code>newdata</code> is not <code>NULL</code>, the argument behaves in the same way as in the <code>predictions()</code> function.  <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>transform</code>  A function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.  <code>points</code>  Number between 0 and 1 which controls the transparency of raw data points. 0 (default) does not display any points.  <code>rug</code>  TRUE displays tick marks on the axes to mark the distribution of raw data.  <code>gray</code>  FALSE grayscale or color plot  <code>draw</code> <code>TRUE</code> returns a <code>ggplot2</code> plot. <code>FALSE</code> returns a <code>data.frame</code> of the underlying data.  <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/plot_predictions/#value","title":"Value","text":"<p>A <code>ggplot2</code> object or data frame (if <code>draw=FALSE</code>)</p>"},{"location":"man/plot_predictions/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/plot_predictions/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\nplot_predictions(mod, condition = \"wt\")\n</code></pre> <pre><code>mod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\nplot_predictions(mod, condition = c(\"hp\", \"wt\"))\n</code></pre> <pre><code>plot_predictions(mod, condition = list(\"hp\", wt = \"threenum\"))\n</code></pre> <pre><code>plot_predictions(mod, condition = list(\"hp\", wt = range))\n</code></pre>"},{"location":"man/plot_slopes/","title":"plot_slopes","text":"<p>Source code</p> <p>Plot Conditional or Marginal Slopes</p>"},{"location":"man/plot_slopes/#description","title":"Description","text":"<p>Plot slopes on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).</p> <p>The <code>by</code> argument is used to plot marginal slopes, that is, slopes made on the original data, but averaged by subgroups. This is analogous to using the <code>by</code> argument in the <code>slopes()</code> function.</p> <p>The <code>condition</code> argument is used to plot conditional slopes, that is, slopes made on a user-specified grid. This is analogous to using the <code>newdata</code> argument and <code>datagrid()</code> function in a <code>slopes()</code> call.</p> <p>All unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the <code>variables</code> argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:</p> <ul> <li>  https://marginaleffects.com/articles/plot.html  </li> <li>  https://marginaleffects.com  </li> </ul>"},{"location":"man/plot_slopes/#usage","title":"Usage","text":"<pre><code>plot_slopes(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  slope = \"dydx\",\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)\n</code></pre>"},{"location":"man/plot_slopes/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>variables</code>  Name of the variable whose marginal effect (slope) we want to plot on the y-axis.  <code>condition</code>   Conditional slopes  <ul> <li>  Character vector (max length 4): Names of the predictors to display.  </li> <li>  Named list (max length 4): List names correspond to predictors. List elements can be:  <ul> <li>  Numeric vector  </li> <li>  Function which returns a numeric vector or a set of unique categorical values  </li> <li>  Shortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"  </li> </ul> </li> <li>  1: x-axis. 2: color/shape. 3: facet (wrap if no fourth variable, otherwise cols of grid). 4: facet (rows of grid).  </li> <li>  Numeric variables in positions 2 and 3 are summarized by Tukey\u2019s five numbers <code>?stats::fivenum</code>.  </li> </ul> <code>by</code>   Aggregate unit-level estimates (aka, marginalize, average over). Valid inputs:  <ul> <li> <code>FALSE</code>: return the original unit-level estimates.  </li> <li> <code>TRUE</code>: aggregate estimates for each term.  </li> <li>  Character vector of column names in <code>newdata</code> or in the data frame produced by calling the function without the <code>by</code> argument.  </li> <li>  Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  </li> <li>  See examples below.  </li> <li>  For more complex aggregations, you can use the <code>FUN</code> argument of the <code>hypotheses()</code> function. See that function\u2019s documentation and the Hypothesis Test vignettes on the <code>marginaleffects</code> website.  </li> </ul> <code>newdata</code>  When <code>newdata</code> is <code>NULL</code>, the grid is determined by the <code>condition</code> argument. When <code>newdata</code> is not <code>NULL</code>, the argument behaves in the same way as in the <code>slopes()</code> function.  <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>slope</code>   string indicates the type of slope or (semi-)elasticity to compute:  <ul> <li>  \"dydx\": dY/dX  </li> <li>  \"eyex\": dY/dX \\* Y / X  </li> <li>  \"eydx\": dY/dX \\* Y  </li> <li>  \"dyex\": dY/dX / X  </li> <li>  Y is the predicted value of the outcome; X is the observed value of the predictor.  </li> </ul> <code>rug</code>  TRUE displays tick marks on the axes to mark the distribution of raw data.  <code>gray</code>  FALSE grayscale or color plot  <code>draw</code> <code>TRUE</code> returns a <code>ggplot2</code> plot. <code>FALSE</code> returns a <code>data.frame</code> of the underlying data.  <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/plot_slopes/#value","title":"Value","text":"<p>A <code>ggplot2</code> object</p>"},{"location":"man/plot_slopes/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/plot_slopes/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"drat\")\n</code></pre> <pre><code>plot_slopes(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n</code></pre> <pre><code>plot_slopes(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n</code></pre> <pre><code>plot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n</code></pre> <pre><code>plot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))\n</code></pre>"},{"location":"man/posterior_draws/","title":"posterior_draws","text":"<p>Source code</p> <p>Extract Posterior Draws or Bootstrap Resamples from <code>marginaleffects</code> Objects</p>"},{"location":"man/posterior_draws/#description","title":"Description","text":"<p>Extract Posterior Draws or Bootstrap Resamples from <code>marginaleffects</code> Objects</p>"},{"location":"man/posterior_draws/#usage","title":"Usage","text":"<pre><code>posterior_draws(x, shape = \"long\")\n</code></pre>"},{"location":"man/posterior_draws/#arguments","title":"Arguments","text":"<code>x</code>  An object produced by a <code>marginaleffects</code> package function, such as <code>predictions()</code>, <code>avg_slopes()</code>, <code>hypotheses()</code>, etc.  <code>shape</code>   string indicating the shape of the output format:  <ul> <li>  \"long\": long format data frame  </li> <li>  \"DxP\": Matrix with draws as rows and parameters as columns  </li> <li>  \"PxD\": Matrix with draws as rows and parameters as columns  </li> <li>  \"rvar\": Random variable datatype (see <code>posterior</code> package documentation).  </li> </ul>"},{"location":"man/posterior_draws/#value","title":"Value","text":"<p>A data.frame with <code>drawid</code> and <code>draw</code> columns.</p>"},{"location":"man/predictions/","title":"predictions","text":"<p>Source code</p> <p>Predictions</p>"},{"location":"man/predictions/#description","title":"Description","text":"<p>Outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \"reference grid\").</p> <ul> <li> <code>predictions()</code>: unit-level (conditional) estimates.  </li> <li> <code>avg_predictions()</code>: average (marginal) estimates.  </li> </ul> <p>The <code>newdata</code> argument and the <code>datagrid()</code> function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.</p> <p>See the predictions vignette and package website for worked examples and case studies:</p> <ul> <li> https://marginaleffects.com/articles/predictions.html </li> <li> https://marginaleffects.com/ </li> </ul>"},{"location":"man/predictions/#usage","title":"Usage","text":"<pre><code>predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = FALSE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = TRUE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  numderiv = \"fdforward\",\n  ...\n)\n</code></pre>"},{"location":"man/predictions/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>newdata</code>   Grid of predictor values at which we evaluate predictions.  <ul> <li>  Warning: Please avoid modifying your dataset between fitting the model and calling a <code>marginaleffects</code> function. This can sometimes lead to unexpected results.  </li> <li> <code>NULL</code> (default): Unit-level predictions for each observed value in the dataset (empirical distribution). The dataset is retrieved using <code>insight::get_data()</code>, which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.  </li> <li>  string:  <ul> <li>  \"mean\": Predictions at the Mean. Predictions when each predictor is held at its mean or mode.  </li> <li>  \"median\": Predictions at the Median. Predictions when each predictor is held at its median or mode.  </li> <li>  \"marginalmeans\": Predictions at Marginal Means. See Details section below.  </li> <li>  \"tukey\": Predictions at Tukey\u2019s 5 numbers.  </li> <li>  \"grid\": Predictions on a grid of representative numbers (Tukey\u2019s 5 numbers and unique values of categorical predictors).  </li> </ul> </li> <li> <code>datagrid()</code> call to specify a custom grid of regressors. For example:  <ul> <li> <code>newdata = datagrid(cyl = c(4, 6))</code>: <code>cyl</code> variable equal to 4 and 6 and other regressors fixed at their means or modes.  </li> <li>  See the Examples section and the <code>datagrid()</code> documentation.  </li> </ul> </li> </ul> <code>variables</code>   Counterfactual variables.  <ul> <li>  Output:  <ul> <li> <code>predictions()</code>: The entire dataset is replicated once for each unique combination of <code>variables</code>, and predictions are made.  </li> <li> <code>avg_predictions()</code>: The entire dataset is replicated, predictions are made, and they are marginalized by <code>variables</code> categories.  </li> <li>  Warning: This can be expensive in large datasets.  </li> <li>  Warning: Users who need \"conditional\" predictions should use the <code>newdata</code> argument instead of <code>variables</code>.  </li> </ul> </li> <li>  Input:  <ul> <li> <code>NULL</code>: computes one prediction per row of <code>newdata</code> </li> <li>  Character vector: the dataset is replicated once of every combination of unique values of the variables identified in <code>variables</code>.  </li> <li>  Named list: names identify the subset of variables of interest and their values. For numeric variables, the <code>variables</code> argument supports functions and string shortcuts:  <ul> <li>  A function which returns a numeric value  </li> <li>  Numeric vector: Contrast between the 2nd element and the 1st element of the <code>x</code> vector.  </li> <li>  \"iqr\": Contrast across the interquartile range of the regressor.  </li> <li>  \"sd\": Contrast across one standard deviation around the regressor mean.  </li> <li>  \"2sd\": Contrast across two standard deviations around the regressor mean.  </li> <li>  \"minmax\": Contrast between the maximum and the minimum values of the regressor.  </li> <li>  \"threenum\": mean and 1 standard deviation on both sides  </li> <li>  \"fivenum\": Tukey\u2019s five numbers  </li> </ul> </li> </ul> </li> </ul> <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>by</code>   Aggregate unit-level estimates (aka, marginalize, average over). Valid inputs:  <ul> <li> <code>FALSE</code>: return the original unit-level estimates.  </li> <li> <code>TRUE</code>: aggregate estimates for each term.  </li> <li>  Character vector of column names in <code>newdata</code> or in the data frame produced by calling the function without the <code>by</code> argument.  </li> <li>  Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  </li> <li>  See examples below.  </li> <li>  For more complex aggregations, you can use the <code>FUN</code> argument of the <code>hypotheses()</code> function. See that function\u2019s documentation and the Hypothesis Test vignettes on the <code>marginaleffects</code> website.  </li> </ul> <code>byfun</code>  A function such as <code>mean()</code> or <code>sum()</code> used to aggregate estimates within the subgroups defined by the <code>by</code> argument. <code>NULL</code> uses the <code>mean()</code> function. Must accept a numeric vector and return a single numeric value. This is sometimes used to take the sum or mean of predicted probabilities across outcome or predictor levels. See examples section.  <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>transform</code>  A function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.  <code>hypothesis</code>   specify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.  <ul> <li>  Numeric:  <ul> <li>  Single value: the null hypothesis used in the computation of Z and p (before applying <code>transform</code>).  </li> <li>  Vector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the <code>hypothesis</code> argument.  </li> <li>  Matrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.  </li> </ul> </li> <li>  String formula to specify linear or non-linear hypothesis tests. If the <code>term</code> column uniquely identifies rows, terms can be used in the formula. Otherwise, use <code>b1</code>, <code>b2</code>, etc. to identify the position of each parameter. The `b*` wildcard can be used to test hypotheses on all estimates. Examples:  <ul> <li> <code>hp = drat</code> </li> <li> <code>hp + drat = 12</code> </li> <li> <code>b1 + b2 + b3 = 0</code> </li> <li>  `b* / b1 = 1`  </li> </ul> </li> <li>  String:  <ul> <li>  \"pairwise\": pairwise differences between estimates in each row.  </li> <li>  \"reference\": differences between the estimates in each row and the estimate in the first row.  </li> <li>  \"sequential\": difference between an estimate and the estimate in the next row.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.  </li> </ul> </li> <li>  See the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html  </li> </ul> <code>equivalence</code>  Numeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.  <code>p_adjust</code>  Adjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust  <code>df</code>  Degrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and <code>Inf</code>. When <code>df</code> is <code>Inf</code>, the normal distribution is used. When <code>df</code> is finite, the <code>t</code> distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: <code>slopes(model, df = insight::get_df(model))</code> <code>numderiv</code>   string or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.  <ul> <li>  \"fdforward\": finite difference method with forward differences  </li> <li>  \"fdcenter\": finite difference method with central differences (default)  </li> <li>  \"richardson\": Richardson extrapolation method  </li> <li>  Extra arguments can be specified by passing a list to the <code>numDeriv</code> argument, with the name of the method first and named arguments following, ex: <code>numderiv=list(\u201cfdcenter\u201d, eps = 1e-5)</code>. When an unknown argument is used, <code>marginaleffects</code> prints the list of valid arguments for each method.  </li> </ul> <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/predictions/#value","title":"Value","text":"<p>A <code>data.frame</code> with one row per observation and several columns:</p> <ul> <li> <code>rowid</code>: row number of the <code>newdata</code> data frame  </li> <li> <code>type</code>: prediction type, as defined by the <code>type</code> argument  </li> <li> <code>group</code>: (optional) value of the grouped outcome (e.g., categorical outcome models)  </li> <li> <code>estimate</code>: predicted outcome  </li> <li> <code>std.error</code>: standard errors computed using the delta method.  </li> <li> <code>p.value</code>: p value associated to the <code>estimate</code> column. The null is determined by the <code>hypothesis</code> argument (0 by default), and p values are computed before applying the <code>transform</code> argument. For models of class <code>feglm</code>, <code>Gam</code>, <code>glm</code> and <code>negbin</code>, p values are computed on the link scale by default unless the <code>type</code> argument is specified explicitly.  </li> <li> <code>s.value</code>: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst\u2019s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.\u00a0(2020).  </li> <li> <code>conf.low</code>: lower bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> <li> <code>conf.high</code>: upper bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> </ul> <p>See <code>?print.marginaleffects</code> for printing options.</p>"},{"location":"man/predictions/#functions","title":"Functions","text":"<ul> <li> <code>avg_predictions()</code>: Average predictions  </li> </ul>"},{"location":"man/predictions/#standard-errors-using-the-delta-method","title":"Standard errors using the delta method","text":"<p>Standard errors for all quantities estimated by <code>marginaleffects</code> can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to <code>1e-8</code>, or to <code>1e-4</code> times the smallest absolute model coefficient, whichever is largest.</p> <p><code>marginaleffects</code> can delegate numeric differentiation to the <code>numDeriv</code> package, which allows more flexibility. To do this, users can pass arguments to the <code>numDeriv::jacobian</code> function through a global option. For example:</p> <ul> <li> <code>options(marginaleffects_numDeriv = list(method = \u201csimple\u201d, method.args = list(eps = 1e-6)))</code> </li> <li> <code>options(marginaleffects_numDeriv = list(method = \u201cRichardson\u201d, method.args = list(eps = 1e-5)))</code> </li> <li> <code>options(marginaleffects_numDeriv = NULL)</code> </li> </ul> <p>See the \"Standard Errors and Confidence Intervals\" vignette on the <code>marginaleffects</code> website for more details on the computation of standard errors:</p> <p>https://marginaleffects.com/articles/uncertainty.html</p> <p>Note that the <code>inferences()</code> function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:</p> <p>https://marginaleffects.com/articles/bootstrap.html</p>"},{"location":"man/predictions/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/predictions/#bayesian-posterior-summaries","title":"Bayesian posterior summaries","text":"<p>By default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201ceti\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201chdi\u201d)</code></p> <p>By default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmean\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmedian\u201d)</code></p> <p>When estimates are averaged using the <code>by</code> argument, the <code>tidy()</code> function, or the <code>summary()</code> function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in <code>by</code> argument or <code>tidy()/summary()</code> functions. Then, we identify the center of the resulting posterior using the function supplied to the <code>\u201cmarginaleffects_posterior_center\u201d</code> option (the median by default).</p>"},{"location":"man/predictions/#equivalence-inferiority-superiority","title":"Equivalence, Inferiority, Superiority","text":"<p>\u03b8 is an estimate, \u03c3<sub>\u03b8</sub> its estimated standard error, and [a,b] are the bounds of the interval supplied to the <code>equivalence</code> argument.</p> <p>Non-inferiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2264\u2004*a*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&gt;\u2004*a*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*a*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Upper-tail probability  </li> </ul> <p>Non-superiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2265\u2004*b*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&lt;\u2004*b*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*b*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Lower-tail probability  </li> </ul> <p>Equivalence: Two One-Sided Tests (TOST)</p> <ul> <li>  p: Maximum of the non-inferiority and non-superiority p values.  </li> </ul> <p>Thanks to Russell V. Lenth for the excellent <code>emmeans</code> package and documentation which inspired this feature.</p>"},{"location":"man/predictions/#prediction-types","title":"Prediction types","text":"<p>The <code>type</code> argument determines the scale of the predictions used to compute quantities of interest with functions from the <code>marginaleffects</code> package. Admissible values for <code>type</code> depend on the model object. When users specify an incorrect value for <code>type</code>, <code>marginaleffects</code> will raise an informative error with a list of valid <code>type</code> values for the specific model object. The first entry in the list in that error message is the default type.</p> <p>The <code>invlink(link)</code> is a special type defined by <code>marginaleffects</code>. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with <code>type=\u201cinvlink(link)\u201d</code> will not always be equivalent to the average of estimates with <code>type=\u201cresponse\u201d</code>.</p> <p>Some of the most common <code>type</code> values are:</p> <p>response, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob</p>"},{"location":"man/predictions/#references","title":"References","text":"<ul> <li>  Greenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106\u2013114.  </li> <li>  Cole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191\u201393. https://doi.org/10.1093/aje/kwaa136  </li> </ul>"},{"location":"man/predictions/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\n# Adjusted Prediction for every row of the original dataset\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\npred &lt;- predictions(mod)\nhead(pred)\n\n# Adjusted Predictions at User-Specified Values of the Regressors\npredictions(mod, newdata = datagrid(hp = c(100, 120), cyl = 4))\n\nm &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n# Average Adjusted Predictions (AAP)\nlibrary(dplyr)\nmod &lt;- lm(mpg ~ hp * am * vs, mtcars)\n\navg_predictions(mod)\n\npredictions(mod, by = \"am\")\n\n# Conditional Adjusted Predictions\nplot_predictions(mod, condition = \"hp\")\n\n# Counterfactual predictions with the `variables` argument\n# the `mtcars` dataset has 32 rows\n\nmod &lt;- lm(mpg ~ hp + am, data = mtcars)\np &lt;- predictions(mod)\nhead(p)\nnrow(p)\n\n# average counterfactual predictions\navg_predictions(mod, variables = \"am\")\n\n# counterfactual predictions obtained by replicating the entire for different\n# values of the predictors\np &lt;- predictions(mod, variables = list(hp = c(90, 110)))\nnrow(p)\n\n\n# hypothesis test: is the prediction in the 1st row equal to the prediction in the 2nd row\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 = b2\")\n\n# same hypothesis test using row indices\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = lc)\n\n\n# `by` argument\nmod &lt;- lm(mpg ~ hp * am * vs, data = mtcars)\npredictions(mod, by = c(\"am\", \"vs\"))\n\nlibrary(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n# first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n\n# average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n\nby &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n\n# sum of predicted probabilities for combined response levels\nmod &lt;- multinom(factor(cyl) ~ mpg + am, data = mtcars, trace = FALSE)\nby &lt;- data.frame(\n    by = c(\"4,6\", \"4,6\", \"8\"),\n    group = as.character(c(4, 6, 8)))\npredictions(mod, newdata = \"mean\", byfun = sum, by = by)\n</code></pre>"},{"location":"man/print.marginaleffects/","title":"print.marginaleffects","text":"<p>Print <code>marginaleffects</code> objects</p>"},{"location":"man/print.marginaleffects/#description","title":"Description","text":"<p>This function controls the text which is printed to the console when one of the core <code>marginalefffects</code> functions is called and the object is returned: <code>predictions()</code>, <code>comparisons()</code>, <code>slopes()</code>, <code>marginal_means()</code>, <code>hypotheses()</code>, <code>avg_predictions()</code>, <code>avg_comparisons()</code>, <code>avg_slopes()</code>.</p> <p>All of those functions return standard data frames. Columns can be extracted by name, <code>predictions(model)$estimate</code>, and all the usual data manipulation functions work out-of-the-box: <code>colnames()</code>, <code>head()</code>, <code>subset()</code>, <code>dplyr::filter()</code>, <code>dplyr::arrange()</code>, etc.</p> <p>Some of the data columns are not printed by default. You can disable pretty printing and print the full results as a standard data frame using the <code>style</code> argument or by applying <code>as.data.frame()</code> on the object. See examples below.</p>"},{"location":"man/print.marginaleffects/#usage","title":"Usage","text":"<pre><code>## S3 method for class 'marginaleffects'\nprint(\n  x,\n  digits = getOption(\"marginaleffects_print_digits\", default = 3),\n  p_eps = getOption(\"marginaleffects_print_p_eps\", default = 0.001),\n  topn = getOption(\"marginaleffects_print_topn\", default = 5),\n  nrows = getOption(\"marginaleffects_print_nrows\", default = 30),\n  ncols = getOption(\"marginaleffects_print_ncols\", default = 30),\n  style = getOption(\"marginaleffects_print_style\", default = \"summary\"),\n  type = getOption(\"marginaleffects_print_type\", default = TRUE),\n  column_names = getOption(\"marginaleffects_print_column_names\", default = TRUE),\n  ...\n)\n</code></pre>"},{"location":"man/print.marginaleffects/#arguments","title":"Arguments","text":"<code>x</code>  An object produced by one of the <code>marginaleffects</code> package functions.  <code>digits</code>  The number of digits to display.  <code>p_eps</code>  p values smaller than this number are printed in \"\\&lt;0.001\" style.  <code>topn</code>  The number of rows to be printed from the beginning and end of tables with more than <code>nrows</code> rows.  <code>nrows</code>  The number of rows which will be printed before truncation.  <code>ncols</code>  The maximum number of column names to display at the bottom of the printed output.  <code>style</code>  \"summary\" or \"data.frame\"  <code>type</code>  boolean: should the type be printed?  <code>column_names</code>  boolean: should the column names be printed?  <code>\u2026</code>  Other arguments are currently ignored."},{"location":"man/print.marginaleffects/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\nlibrary(marginaleffects)\nmod &lt;- lm(mpg ~ hp + am + factor(gear), data = mtcars)\np &lt;- predictions(mod, by = c(\"am\", \"gear\"))\np\n</code></pre> <pre><code> am gear Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n  0    3     16.1      0.759 21.2   &lt;0.001 329.6  14.6   17.6\n  0    4     21.0      1.470 14.3   &lt;0.001 152.1  18.2   23.9\n  1    4     26.3      1.039 25.3   &lt;0.001 466.1  24.2   28.3\n  1    5     21.4      1.315 16.3   &lt;0.001 195.2  18.8   24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code>subset(p, am == 1)\n</code></pre> <pre><code> Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n     26.3       1.04 25.3   &lt;0.001 466.1   24.2    28.3\n     21.4       1.31 16.3   &lt;0.001 195.2   18.8    24.0\n\nColumns: am, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code>print(p, style = \"data.frame\")\n</code></pre> <pre><code>  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.047015e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697592  14.32207  1.592966e-46 152.1370 18.16932\n3  1    4 26.27500 1.0392746  25.28206 5.032257e-141 466.0607 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93068\n3  28.31194\n4  23.95655\n</code></pre> <pre><code>data.frame(p)\n</code></pre> <pre><code>  am gear estimate std.error statistic       p.value  s.value conf.low\n1  0    3 16.10667 0.7589789  21.22150 6.047015e-100 329.5966 14.61910\n2  0    4 21.05000 1.4697592  14.32207  1.592966e-46 152.1370 18.16932\n3  1    4 26.27500 1.0392746  25.28206 5.032257e-141 466.0607 24.23806\n4  1    5 21.38000 1.3145900  16.26363  1.788354e-59 195.1551 18.80345\n  conf.high\n1  17.59424\n2  23.93068\n3  28.31194\n4  23.95655\n</code></pre>"},{"location":"man/slopes/","title":"slopes","text":"<p>Source code</p> <p>Slopes (aka Partial derivatives, Marginal Effects, or Trends)</p>"},{"location":"man/slopes/#description","title":"Description","text":"<p>Partial derivative of the regression equation with respect to a regressor of interest.</p> <ul> <li> <code>slopes()</code>: unit-level (conditional) estimates.  </li> <li> <code>avg_slopes()</code>: average (marginal) estimates.  </li> </ul> <p>The <code>newdata</code> argument and the <code>datagrid()</code> function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.</p> <p>See the slopes vignette and package website for worked examples and case studies:</p> <ul> <li> https://marginaleffects.com/articles/slopes.html </li> <li> https://marginaleffects.com/ </li> </ul>"},{"location":"man/slopes/#usage","title":"Usage","text":"<pre><code>slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = FALSE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n\navg_slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = TRUE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  numderiv = \"fdforward\",\n  ...\n)\n</code></pre>"},{"location":"man/slopes/#arguments","title":"Arguments","text":"<code>model</code>  Model object  <code>newdata</code>   Grid of predictor values at which we evaluate the slopes.  <ul> <li>  Warning: Please avoid modifying your dataset between fitting the model and calling a <code>marginaleffects</code> function. This can sometimes lead to unexpected results.  </li> <li> <code>NULL</code> (default): Unit-level slopes for each observed value in the dataset (empirical distribution). The dataset is retrieved using <code>insight::get_data()</code>, which tries to extract data from the environment. This may produce unexpected results if the original data frame has been altered since fitting the model.  </li> <li> <code>datagrid()</code> call to specify a custom grid of regressors. For example:  <ul> <li> <code>newdata = datagrid(cyl = c(4, 6))</code>: <code>cyl</code> variable equal to 4 and 6 and other regressors fixed at their means or modes.  </li> <li>  See the Examples section and the <code>datagrid()</code> documentation.  </li> </ul> </li> <li>  string:  <ul> <li>  \"mean\": Marginal Effects at the Mean. Slopes when each predictor is held at its mean or mode.  </li> <li>  \"median\": Marginal Effects at the Median. Slopes when each predictor is held at its median or mode.  </li> <li>  \"marginalmeans\": Marginal Effects at Marginal Means. See Details section below.  </li> <li>  \"tukey\": Marginal Effects at Tukey\u2019s 5 numbers.  </li> <li>  \"grid\": Marginal Effects on a grid of representative numbers (Tukey\u2019s 5 numbers and unique values of categorical predictors).  </li> </ul> </li> </ul> <code>variables</code>   Focal variables  <ul> <li> <code>NULL</code>: compute slopes or comparisons for all the variables in the model object (can be slow).  </li> <li>  Character vector: subset of variables (usually faster).  </li> </ul> <code>type</code>  string indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When <code>type</code> is <code>NULL</code>, the first entry in the error message is used by default.  <code>by</code>   Aggregate unit-level estimates (aka, marginalize, average over). Valid inputs:  <ul> <li> <code>FALSE</code>: return the original unit-level estimates.  </li> <li> <code>TRUE</code>: aggregate estimates for each term.  </li> <li>  Character vector of column names in <code>newdata</code> or in the data frame produced by calling the function without the <code>by</code> argument.  </li> <li>  Data frame with a <code>by</code> column of group labels, and merging columns shared by <code>newdata</code> or the data frame produced by calling the same function without the <code>by</code> argument.  </li> <li>  See examples below.  </li> <li>  For more complex aggregations, you can use the <code>FUN</code> argument of the <code>hypotheses()</code> function. See that function\u2019s documentation and the Hypothesis Test vignettes on the <code>marginaleffects</code> website.  </li> </ul> <code>vcov</code>   Type of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:  <ul> <li>  FALSE: Do not compute standard errors. This can speed up computation considerably.  </li> <li>  TRUE: Unit-level standard errors using the default <code>vcov(model)</code> variance-covariance matrix.  </li> <li>  String which indicates the kind of uncertainty estimates to return.  <ul> <li>  Heteroskedasticity-consistent: <code>\u201cHC\u201d</code>, <code>\u201cHC0\u201d</code>, <code>\u201cHC1\u201d</code>, <code>\u201cHC2\u201d</code>, <code>\u201cHC3\u201d</code>, <code>\u201cHC4\u201d</code>, <code>\u201cHC4m\u201d</code>, <code>\u201cHC5\u201d</code>. See <code>?sandwich::vcovHC</code> </li> <li>  Heteroskedasticity and autocorrelation consistent: <code>\u201cHAC\u201d</code> </li> <li>  Mixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"  </li> <li>  Other: <code>\u201cNeweyWest\u201d</code>, <code>\u201cKernHAC\u201d</code>, <code>\u201cOPG\u201d</code>. See the <code>sandwich</code> package documentation.  </li> </ul> </li> <li>  One-sided formula which indicates the name of cluster variables (e.g., <code>~unit_id</code>). This formula is passed to the <code>cluster</code> argument of the <code>sandwich::vcovCL</code> function.  </li> <li>  Square covariance matrix  </li> <li>  Function which returns a covariance matrix (e.g., <code>stats::vcov(model)</code>)  </li> </ul> <code>conf_level</code>  numeric value between 0 and 1. Confidence level to use to build a confidence interval.  <code>slope</code>   string indicates the type of slope or (semi-)elasticity to compute:  <ul> <li>  \"dydx\": dY/dX  </li> <li>  \"eyex\": dY/dX \\* Y / X  </li> <li>  \"eydx\": dY/dX \\* Y  </li> <li>  \"dyex\": dY/dX / X  </li> <li>  Y is the predicted value of the outcome; X is the observed value of the predictor.  </li> </ul> <code>wts</code>   string or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in `avg_*()` or with the <code>by</code> argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the <code>weighted.mean()</code> function.  <ul> <li>  string: column name of the weights variable in <code>newdata</code>. When supplying a column name to <code>wts</code>, it is recommended to supply the original data (including the weights variable) explicitly to <code>newdata</code>.  </li> <li>  numeric: vector of length equal to the number of rows in the original data or in <code>newdata</code> (if supplied).  </li> </ul> <code>hypothesis</code>   specify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.  <ul> <li>  Numeric:  <ul> <li>  Single value: the null hypothesis used in the computation of Z and p (before applying <code>transform</code>).  </li> <li>  Vector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the <code>hypothesis</code> argument.  </li> <li>  Matrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.  </li> </ul> </li> <li>  String formula to specify linear or non-linear hypothesis tests. If the <code>term</code> column uniquely identifies rows, terms can be used in the formula. Otherwise, use <code>b1</code>, <code>b2</code>, etc. to identify the position of each parameter. The `b*` wildcard can be used to test hypotheses on all estimates. Examples:  <ul> <li> <code>hp = drat</code> </li> <li> <code>hp + drat = 12</code> </li> <li> <code>b1 + b2 + b3 = 0</code> </li> <li>  `b* / b1 = 1`  </li> </ul> </li> <li>  String:  <ul> <li>  \"pairwise\": pairwise differences between estimates in each row.  </li> <li>  \"reference\": differences between the estimates in each row and the estimate in the first row.  </li> <li>  \"sequential\": difference between an estimate and the estimate in the next row.  </li> <li>  \"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.  </li> </ul> </li> <li>  See the Examples section below and the vignette: https://marginaleffects.com/articles/hypothesis.html  </li> </ul> <code>equivalence</code>  Numeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.  <code>p_adjust</code>  Adjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust  <code>df</code>  Degrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and <code>Inf</code>. When <code>df</code> is <code>Inf</code>, the normal distribution is used. When <code>df</code> is finite, the <code>t</code> distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: <code>slopes(model, df = insight::get_df(model))</code> <code>eps</code>  NULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When <code>eps</code> is <code>NULL</code>, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing <code>eps</code> may be necessary to avoid numerical problems in certain models.  <code>numderiv</code>   string or list of strings indicating the method to use to for the numeric differentiation used in to compute delta method standard errors.  <ul> <li>  \"fdforward\": finite difference method with forward differences  </li> <li>  \"fdcenter\": finite difference method with central differences (default)  </li> <li>  \"richardson\": Richardson extrapolation method  </li> <li>  Extra arguments can be specified by passing a list to the <code>numDeriv</code> argument, with the name of the method first and named arguments following, ex: <code>numderiv=list(\u201cfdcenter\u201d, eps = 1e-5)</code>. When an unknown argument is used, <code>marginaleffects</code> prints the list of valid arguments for each method.  </li> </ul> <code>\u2026</code>  Additional arguments are passed to the <code>predict()</code> method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the <code>marginaleffects</code> website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the <code>?marginaleffects</code> documentation for a non-exhaustive list of available arguments."},{"location":"man/slopes/#details","title":"Details","text":"<p>A \"slope\" or \"marginal effect\" is the partial derivative of the regression equation with respect to a variable in the model. This function uses automatic differentiation to compute slopes for a vast array of models, including non-linear models with transformations (e.g., polynomials). Uncertainty estimates are computed using the delta method.</p> <p>Numerical derivatives for the <code>slopes</code> function are calculated using a simple epsilon difference approach: \u2202Y/\u2202X\u2004=\u2004(f(X+\u03b5/2)\u2212f(X\u2212\u03b5/2))/\u03b5, where f is the <code>predict()</code> method associated with the model class, and \u03b5 is determined by the <code>eps</code> argument.</p>"},{"location":"man/slopes/#value","title":"Value","text":"<p>A <code>data.frame</code> with one row per observation (per term/group) and several columns:</p> <ul> <li> <code>rowid</code>: row number of the <code>newdata</code> data frame  </li> <li> <code>type</code>: prediction type, as defined by the <code>type</code> argument  </li> <li> <code>group</code>: (optional) value of the grouped outcome (e.g., categorical outcome models)  </li> <li> <code>term</code>: the variable whose marginal effect is computed  </li> <li> <code>dydx</code>: slope of the outcome with respect to the term, for a given combination of predictor values  </li> <li> <code>std.error</code>: standard errors computed by via the delta method.  </li> <li> <code>p.value</code>: p value associated to the <code>estimate</code> column. The null is determined by the <code>hypothesis</code> argument (0 by default), and p values are computed before applying the <code>transform</code> argument. For models of class <code>feglm</code>, <code>Gam</code>, <code>glm</code> and <code>negbin</code>, p values are computed on the link scale by default unless the <code>type</code> argument is specified explicitly.  </li> <li> <code>s.value</code>: Shannon information transforms of p values. How many consecutive \"heads\" tosses would provide the same amount of evidence (or \"surprise\") against the null hypothesis that the coin is fair? The purpose of S is to calibrate the analyst\u2019s intuition about the strength of evidence encoded in p against a well-known physical phenomenon. See Greenland (2019) and Cole et al.\u00a0(2020).  </li> <li> <code>conf.low</code>: lower bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> <li> <code>conf.high</code>: upper bound of the confidence interval (or equal-tailed interval for bayesian models)  </li> </ul> <p>See <code>?print.marginaleffects</code> for printing options.</p>"},{"location":"man/slopes/#functions","title":"Functions","text":"<ul> <li> <code>avg_slopes()</code>: Average slopes  </li> </ul>"},{"location":"man/slopes/#standard-errors-using-the-delta-method","title":"Standard errors using the delta method","text":"<p>Standard errors for all quantities estimated by <code>marginaleffects</code> can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to <code>1e-8</code>, or to <code>1e-4</code> times the smallest absolute model coefficient, whichever is largest.</p> <p><code>marginaleffects</code> can delegate numeric differentiation to the <code>numDeriv</code> package, which allows more flexibility. To do this, users can pass arguments to the <code>numDeriv::jacobian</code> function through a global option. For example:</p> <ul> <li> <code>options(marginaleffects_numDeriv = list(method = \u201csimple\u201d, method.args = list(eps = 1e-6)))</code> </li> <li> <code>options(marginaleffects_numDeriv = list(method = \u201cRichardson\u201d, method.args = list(eps = 1e-5)))</code> </li> <li> <code>options(marginaleffects_numDeriv = NULL)</code> </li> </ul> <p>See the \"Standard Errors and Confidence Intervals\" vignette on the <code>marginaleffects</code> website for more details on the computation of standard errors:</p> <p>https://marginaleffects.com/articles/uncertainty.html</p> <p>Note that the <code>inferences()</code> function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:</p> <p>https://marginaleffects.com/articles/bootstrap.html</p>"},{"location":"man/slopes/#model-specific-arguments","title":"Model-Specific Arguments","text":"<p>Some model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific <code>predict()</code> arguments on Github so we can add them to the table below.</p> <p>https://github.com/vincentarelbundock/marginaleffects/issues</p>  Package   Class   Argument   Documentation  <code>brms</code> <code>brmsfit</code> <code>ndraws</code>  brms::posterior_predict  <code>re_formula</code>  brms::posterior_predict  <code>lme4</code> <code>merMod</code> <code>re.form</code>  lme4::predict.merMod  <code>allow.new.levels</code>  lme4::predict.merMod  <code>glmmTMB</code> <code>glmmTMB</code> <code>re.form</code>  glmmTMB::predict.glmmTMB  <code>allow.new.levels</code>  glmmTMB::predict.glmmTMB  <code>zitype</code>  glmmTMB::predict.glmmTMB  <code>mgcv</code> <code>bam</code> <code>exclude</code>  mgcv::predict.bam  <code>robustlmm</code> <code>rlmerMod</code> <code>re.form</code>  robustlmm::predict.rlmerMod  <code>allow.new.levels</code>  robustlmm::predict.rlmerMod  <code>MCMCglmm</code> <code>MCMCglmm</code> <code>ndraws</code>"},{"location":"man/slopes/#bayesian-posterior-summaries","title":"Bayesian posterior summaries","text":"<p>By default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201ceti\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_interval\u201d = \u201chdi\u201d)</code></p> <p>By default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:</p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmean\u201d)</code></p> <p><code>options(\u201cmarginaleffects_posterior_center\u201d = \u201cmedian\u201d)</code></p> <p>When estimates are averaged using the <code>by</code> argument, the <code>tidy()</code> function, or the <code>summary()</code> function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in <code>by</code> argument or <code>tidy()/summary()</code> functions. Then, we identify the center of the resulting posterior using the function supplied to the <code>\u201cmarginaleffects_posterior_center\u201d</code> option (the median by default).</p>"},{"location":"man/slopes/#equivalence-inferiority-superiority","title":"Equivalence, Inferiority, Superiority","text":"<p>\u03b8 is an estimate, \u03c3<sub>\u03b8</sub> its estimated standard error, and [a,b] are the bounds of the interval supplied to the <code>equivalence</code> argument.</p> <p>Non-inferiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2264\u2004*a*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&gt;\u2004*a*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*a*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Upper-tail probability  </li> </ul> <p>Non-superiority:</p> <ul> <li>  *H*<sub>0</sub>: *\u03b8*\u2004\u2265\u2004*b*  </li> <li>  *H*<sub>1</sub>: *\u03b8*\u2004\\&lt;\u2004*b*  </li> <li>  *t*\u2004=\u2004(*\u03b8*\u2212*b*)/*\u03c3*<sub>*\u03b8*</sub> </li> <li>  p: Lower-tail probability  </li> </ul> <p>Equivalence: Two One-Sided Tests (TOST)</p> <ul> <li>  p: Maximum of the non-inferiority and non-superiority p values.  </li> </ul> <p>Thanks to Russell V. Lenth for the excellent <code>emmeans</code> package and documentation which inspired this feature.</p>"},{"location":"man/slopes/#prediction-types","title":"Prediction types","text":"<p>The <code>type</code> argument determines the scale of the predictions used to compute quantities of interest with functions from the <code>marginaleffects</code> package. Admissible values for <code>type</code> depend on the model object. When users specify an incorrect value for <code>type</code>, <code>marginaleffects</code> will raise an informative error with a list of valid <code>type</code> values for the specific model object. The first entry in the list in that error message is the default type.</p> <p>The <code>invlink(link)</code> is a special type defined by <code>marginaleffects</code>. It is available for some (but not all) models and functions. With this link type, we first compute predictions on the link scale, then we use the inverse link function to backtransform the predictions to the response scale. This is useful for models with non-linear link functions as it can ensure that confidence intervals stay within desirable bounds, ex: 0 to 1 for a logit model. Note that an average of estimates with <code>type=\u201cinvlink(link)\u201d</code> will not always be equivalent to the average of estimates with <code>type=\u201cresponse\u201d</code>.</p> <p>Some of the most common <code>type</code> values are:</p> <p>response, link, E, Ep, average, class, conditional, count, cum.prob, cumprob, density, disp, ev, expected, expvalue, fitted, invlink(link), latent, linear.predictor, linpred, location, lp, mean, numeric, p, ppd, pr, precision, prediction, prob, probability, probs, quantile, risk, scale, survival, unconditional, utility, variance, xb, zero, zlink, zprob</p>"},{"location":"man/slopes/#references","title":"References","text":"<ul> <li>  Greenland S. 2019. \"Valid P-Values Behave Exactly as They Should: Some Misleading Criticisms of P-Values and Their Resolution With S-Values.\" The American Statistician. 73(S1): 106\u2013114.  </li> <li>  Cole, Stephen R, Jessie K Edwards, and Sander Greenland. 2020. \"Surprise!\" American Journal of Epidemiology 190 (2): 191\u201393. https://doi.org/10.1093/aje/kwaa136  </li> </ul>"},{"location":"man/slopes/#examples","title":"Examples","text":"<pre><code>library(marginaleffects)\n\n\n\n\n# Unit-level (conditional) Marginal Effects\nmod &lt;- glm(am ~ hp * wt, data = mtcars, family = binomial)\nmfx &lt;- slopes(mod)\nhead(mfx)\n\n# Average Marginal Effect (AME)\navg_slopes(mod, by = TRUE)\n\n\n# Marginal Effect at the Mean (MEM)\nslopes(mod, newdata = datagrid())\n\n# Marginal Effect at User-Specified Values\n# Variables not explicitly included in `datagrid()` are held at their means\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n# Group-Average Marginal Effects (G-AME)\n# Calculate marginal effects for each observation, and then take the average\n# marginal effect within each subset of observations with different observed\n# values for the `cyl` variable:\nmod2 &lt;- lm(mpg ~ hp * cyl, data = mtcars)\navg_slopes(mod2, variables = \"hp\", by = \"cyl\")\n\n# Marginal Effects at User-Specified Values (counterfactual)\n# Variables not explicitly included in `datagrid()` are held at their\n# original values, and the whole dataset is duplicated once for each\n# combination of the values in `datagrid()`\nmfx &lt;- slopes(mod,\n              newdata = datagrid(hp = c(100, 110),\n              grid_type = \"counterfactual\"))\nhead(mfx)\n\n# Heteroskedasticity robust standard errors\nmfx &lt;- slopes(mod, vcov = sandwich::vcovHC(mod))\nhead(mfx)\n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod &lt;- lm(mpg ~ wt + drat, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n# same hypothesis test using row indices\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n# same hypothesis test using numeric vector of weights\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n# two custom contrasts using a matrix of weights\nlc &lt;- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n</code></pre>"},{"location":"vignettes/bootstrap/","title":"Bootstrap &amp; Simulation","text":"<p><code>marginaleffects</code> offers an <code>inferences()</code> function to compute uncertainty estimates using the bootstrap and simulation-based inference.</p> <p>WARNING: The <code>inferences()</code> function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other <code>marginaleffects</code> functions.</p> <p>Consider a simple model:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(Sepal.Length ~ Petal.Width * Petal.Length + factor(Species), data = iris)\n</code></pre> <p>We will compute uncertainty estimates around the output of <code>comparisons()</code>, but note that the same approach works with the <code>predictions()</code> and <code>slopes()</code> functions as well.</p>"},{"location":"vignettes/bootstrap/#delta-method","title":"Delta method","text":"<p>The default strategy to compute standard errors and confidence intervals is the delta method. This is what we obtain by calling:</p> <pre><code>avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>Since this is the default method, we obtain the same results if we add the <code>inferences()</code> call in the chain:</p> <pre><code>avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"delta\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/bootstrap/#bootstrap","title":"Bootstrap","text":"<p><code>marginaleffects</code> supports three bootstrap frameworks in <code>R</code>: the well-established <code>boot</code> package, the newer <code>rsample</code> package, and the so-called \u201cbayesian bootstrap\u201d in <code>fwb</code>.</p>"},{"location":"vignettes/bootstrap/#boot","title":"<code>boot</code>","text":"<pre><code>avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"boot\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.271 -0.606  0.442\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.167 -0.334  0.299\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.187 -0.343  0.368\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, std.error, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>All unknown arguments that we feed to <code>inferences()</code> are pushed forward to <code>boot::boot()</code>:</p> <pre><code>est &lt;- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"boot\", sim = \"balanced\", R = 500, conf_type = \"bca\")\nest\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.269 -0.688  0.396\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.158 -0.316  0.299\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.182 -0.335  0.393\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, std.error, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can extract the original <code>boot</code> object from an attribute:</p> <pre><code>attr(est, \"inferences\")\n#&gt; \n#&gt; BALANCED BOOTSTRAP\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; bootstrap_boot(model = model, INF_FUN = INF_FUN, newdata = ..1, \n#&gt;     vcov = ..2, variables = ..3, type = ..4, by = ..5, conf_level = ..6, \n#&gt;     cross = ..7, comparison = ..8, transform = ..9, wts = ..10, \n#&gt;     hypothesis = ..11, eps = ..12)\n#&gt; \n#&gt; \n#&gt; Bootstrap Statistics :\n#&gt;        original      bias    std. error\n#&gt; t1* -0.11025325 0.010638159   0.2687425\n#&gt; t2* -0.02006005 0.004258223   0.1576333\n#&gt; t3*  0.02158742 0.001312235   0.1824467\n</code></pre> <p>Or we can extract the individual draws with the <code>posterior_draws()</code> function:</p> <pre><code>posterior_draws(est) |&gt; head()\n#&gt;   drawid       draw        term contrast    Species    estimate predicted_lo predicted_hi predicted std.error   conf.low conf.high\n#&gt; 1      1 -0.2997203 Petal.Width mean(+1)     setosa -0.11025325     4.957514     4.845263  4.957514 0.2687425 -0.6875060 0.3956670\n#&gt; 2      1 -0.1804525 Petal.Width mean(+1) versicolor -0.02006005     6.327949     6.322072  6.327949 0.1576333 -0.3162988 0.2985886\n#&gt; 3      1 -0.1253796 Petal.Width mean(+1)  virginica  0.02158742     7.015513     7.051542  7.015513 0.1824467 -0.3353508 0.3933245\n#&gt; 4      2 -0.2651303 Petal.Width mean(+1)     setosa -0.11025325     4.957514     4.845263  4.957514 0.2687425 -0.6875060 0.3956670\n#&gt; 5      2 -0.2034462 Petal.Width mean(+1) versicolor -0.02006005     6.327949     6.322072  6.327949 0.1576333 -0.3162988 0.2985886\n#&gt; 6      2 -0.1749630 Petal.Width mean(+1)  virginica  0.02158742     7.015513     7.051542  7.015513 0.1824467 -0.3353508 0.3933245\n\nposterior_draws(est, shape = \"DxP\") |&gt; dim()\n#&gt; [1] 500   3\n</code></pre>"},{"location":"vignettes/bootstrap/#rsample","title":"<code>rsample</code>","text":"<p>As before, we can pass arguments to <code>rsample::bootstraps()</code> through <code>inferences()</code>. For example, for stratified resampling:</p> <pre><code>est &lt;- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"rsample\", R = 100, strata = \"Species\")\nest\n#&gt; \n#&gt;         Term Contrast    Species Estimate  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103 -0.639  0.264\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201 -0.311  0.311\n#&gt;  Petal.Width mean(+1) virginica    0.0216 -0.308  0.358\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, predicted_lo, predicted_hi, predicted, conf.low, conf.high \n#&gt; Type:  response\n\nattr(est, \"inferences\")\n#&gt; # Bootstrap sampling using stratification with apparent sample \n#&gt; # A tibble: 101 \u00d7 3\n#&gt;    splits           id           estimates       \n#&gt;    &lt;list&gt;           &lt;chr&gt;        &lt;list&gt;          \n#&gt;  1 &lt;split [150/50]&gt; Bootstrap001 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  2 &lt;split [150/55]&gt; Bootstrap002 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  3 &lt;split [150/55]&gt; Bootstrap003 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  4 &lt;split [150/60]&gt; Bootstrap004 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  5 &lt;split [150/52]&gt; Bootstrap005 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  6 &lt;split [150/52]&gt; Bootstrap006 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  7 &lt;split [150/54]&gt; Bootstrap007 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  8 &lt;split [150/59]&gt; Bootstrap008 &lt;tibble [3 \u00d7 7]&gt;\n#&gt;  9 &lt;split [150/61]&gt; Bootstrap009 &lt;tibble [3 \u00d7 7]&gt;\n#&gt; 10 &lt;split [150/53]&gt; Bootstrap010 &lt;tibble [3 \u00d7 7]&gt;\n#&gt; # \u2139 91 more rows\n</code></pre> <p>Or we can extract the individual draws with the <code>posterior_draws()</code> function:</p> <pre><code>posterior_draws(est) |&gt; head()\n#&gt;   drawid         draw        term contrast    Species    estimate predicted_lo predicted_hi predicted   conf.low conf.high\n#&gt; 1      1 -0.018347483 Petal.Width mean(+1)     setosa -0.11025325     4.957514     4.845263  4.957514 -0.6393702 0.2639397\n#&gt; 2      1  0.009371202 Petal.Width mean(+1) versicolor -0.02006005     6.327949     6.322072  6.327949 -0.3108048 0.3108965\n#&gt; 3      1  0.022170537 Petal.Width mean(+1)  virginica  0.02158742     7.015513     7.051542  7.015513 -0.3079096 0.3581024\n#&gt; 4      2 -0.681668871 Petal.Width mean(+1)     setosa -0.11025325     4.957514     4.845263  4.957514 -0.6393702 0.2639397\n#&gt; 5      2 -0.522346181 Petal.Width mean(+1) versicolor -0.02006005     6.327949     6.322072  6.327949 -0.3108048 0.3108965\n#&gt; 6      2 -0.448777591 Petal.Width mean(+1)  virginica  0.02158742     7.015513     7.051542  7.015513 -0.3079096 0.3581024\n\nposterior_draws(est, shape = \"PxD\") |&gt; dim()\n#&gt; [1]   3 100\n</code></pre>"},{"location":"vignettes/bootstrap/#fractional-weighted-bootstrap-aka-bayesian-bootstrap","title":"Fractional Weighted Bootstrap (aka Bayesian Bootstrap)","text":"<p>The <code>fwb</code> package implements fractional weighted bootstrap (aka Bayesian bootstrap):</p> <p>\u201cfwb implements the fractional weighted bootstrap (FWB), also known as the Bayesian bootstrap, following the treatment by Xu et al.\u00a0(2020). The FWB involves generating sets of weights from a uniform Dirichlet distribution to be used in estimating statistics of interest, which yields a posterior distribution that can be interpreted in the same way the traditional (resampling-based) bootstrap distribution can be.\u201d -Noah Greifer</p> <p>The <code>inferences()</code> function makes it easy to apply this inference strategy to <code>marginaleffects</code> objects:</p> <pre><code>avg_comparisons(mod) |&gt; inferences(method = \"fwb\")\n#&gt; \n#&gt;          Term            Contrast Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Length +1                    0.8929      0.079  0.726  1.047\n#&gt;  Petal.Width  +1                   -0.0362      0.159 -0.329  0.302\n#&gt;  Species      versicolor - setosa  -1.4629      0.324 -2.141 -0.841\n#&gt;  Species      virginica - setosa   -1.9842      0.384 -2.730 -1.252\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/bootstrap/#simulation-based-inference","title":"Simulation-based inference","text":"<p>This simulation-based strategy to compute confidence intervals was described in Krinsky &amp; Robb (1986) and popularized by King, Tomz, Wittenberg (2000). We proceed in 3 steps:</p> <ol> <li>Draw <code>R</code> sets of simulated coefficients from a multivariate normal     distribution with mean equal to the original model\u2019s estimated     coefficients and variance equal to the model\u2019s variance-covariance     matrix (classical, \u201cHC3\u201d, or other).</li> <li>Use the <code>R</code> sets of coefficients to compute <code>R</code> sets of estimands:     predictions, comparisons, or slopes.</li> <li>Take quantiles of the resulting distribution of estimands to obtain     a confidence interval and the standard deviation of simulated     estimates to estimate the standard error.</li> </ol> <p>Here are a few examples:</p> <pre><code>library(ggplot2)\nlibrary(ggdist)\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"simulation\")\n#&gt; \n#&gt;         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#&gt;  Petal.Width mean(+1) setosa      -0.1103      0.275 -0.685  0.398\n#&gt;  Petal.Width mean(+1) versicolor  -0.0201      0.159 -0.348  0.283\n#&gt;  Petal.Width mean(+1) virginica    0.0216      0.171 -0.307  0.355\n#&gt; \n#&gt; Columns: term, contrast, Species, estimate, std.error, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx \n#&gt; Type:  response\n</code></pre> <p>Since simulation based inference generates <code>R</code> estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the <code>posterior_draws()</code> function, and plot their distributions using packages like<code>ggplot2</code> and <code>ggdist</code>:</p> <pre><code>avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |&gt;\n  inferences(method = \"simulation\") |&gt;\n  posterior_draws(\"rvar\") |&gt;\n  ggplot(aes(y = Species, xdist = rvar)) +\n  stat_slabinterval()\n</code></pre> <p></p>"},{"location":"vignettes/bootstrap/#multiple-imputation-and-missing-data","title":"Multiple imputation and missing data","text":"<p>The same workflow and the same <code>inferences</code> function can be used to estimate models with multiple imputation for missing data.</p>"},{"location":"vignettes/brms/","title":"Bayes","text":"<p>The <code>marginaleffects</code> package offers convenience functions to compute and display predictions, contrasts, and marginal effects from bayesian models estimated by the <code>brms</code> package. To compute these quantities, <code>marginaleffects</code> relies on workhorse functions from the <code>brms</code> package to draw from the posterior distribution. The type of draws used is controlled by using the <code>type</code> argument of the <code>predictions</code> or <code>slopes</code> functions:</p> <ul> <li><code>type = \"response\"</code>: Compute posterior draws of the expected value     using the <code>brms::posterior_epred</code> function.</li> <li><code>type = \"link\"</code>: Compute posterior draws of the linear predictor     using the <code>brms::posterior_linpred</code> function.</li> <li><code>type = \"prediction\"</code>: Compute posterior draws of the posterior     predictive distribution using the <code>brms::posterior_predict</code>     function.</li> </ul> <p>The <code>predictions</code> and <code>slopes</code> functions can also pass additional arguments to the <code>brms</code> prediction functions via the <code>...</code> ellipsis. For example, if <code>mod</code> is a mixed-effects model, then this command will compute 10 draws from the posterior predictive distribution, while ignoring all group-level effects:</p> <pre><code>predictions(mod, type = \"prediction\", ndraws = 10, re_formula = NA)\n</code></pre> <p>See the <code>brms</code> documentation for a list of available arguments:</p> <pre><code>?brms::posterior_epred\n?brms::posterior_linpred\n?brms::posterior_predict\n</code></pre>"},{"location":"vignettes/brms/#logistic-regression-with-multiplicative-interactions","title":"Logistic regression with multiplicative interactions","text":"<p>Load libraries and download data on passengers of the Titanic from the Rdatasets archive:</p> <pre><code>library(marginaleffects)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/carData/TitanicSurvival.csv\")\ndat$survived &lt;- ifelse(dat$survived == \"yes\", 1, 0)\ndat$woman &lt;- ifelse(dat$sex == \"female\", 1, 0)\n</code></pre> <p>Fit a logit model with a multiplicative interaction:</p> <pre><code>mod &lt;- brm(survived ~ woman * age + passengerClass,\n           family = bernoulli(link = \"logit\"),\n           data = dat)\n</code></pre>"},{"location":"vignettes/brms/#adjusted-predictions","title":"Adjusted predictions","text":"<p>We can compute adjusted predicted values of the outcome variable (i.e., probability of survival aboard the Titanic) using the <code>predictions</code> function. By default, this function calculates predictions for each row of the dataset:</p> <pre><code>predictions(mod)\n#&gt; \n#&gt;  Estimate  2.5 % 97.5 %\n#&gt;    0.9367 0.9070 0.9590\n#&gt;    0.8493 0.7453 0.9187\n#&gt;    0.9433 0.8949 0.9704\n#&gt;    0.5131 0.4302 0.6000\n#&gt;    0.9375 0.9080 0.9601\n#&gt; --- 1036 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;    0.0376 0.0235 0.0581\n#&gt;    0.5859 0.5017 0.6663\n#&gt;    0.1043 0.0801 0.1337\n#&gt;    0.1017 0.0779 0.1307\n#&gt;    0.0916 0.0691 0.1189\n#&gt; Columns: rowid, estimate, conf.low, conf.high, survived, woman, age, passengerClass \n#&gt; Type:  response\n</code></pre> <p>To visualize the relationship between the outcome and one of the regressors, we can plot conditional adjusted predictions with the <code>plot_predictions</code> function:</p> <pre><code>plot_predictions(mod, condition = \"age\")\n</code></pre> <p></p> <p>Compute adjusted predictions for some user-specified values of the regressors, using the <code>newdata</code> argument and the <code>datagrid</code> function:</p> <pre><code>pred &lt;- predictions(mod,\n                    newdata = datagrid(woman = 0:1,\n                                       passengerClass = c(\"1st\", \"2nd\", \"3rd\")))\npred\n#&gt; \n#&gt;  woman passengerClass Estimate  2.5 % 97.5 %\n#&gt;      0            1st   0.5149 0.4319  0.602\n#&gt;      0            2nd   0.2013 0.1536  0.261\n#&gt;      0            3rd   0.0875 0.0656  0.114\n#&gt;      1            1st   0.9364 0.9066  0.959\n#&gt;      1            2nd   0.7783 0.7090  0.835\n#&gt;      1            3rd   0.5701 0.4938  0.644\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, survived, age, woman, passengerClass \n#&gt; Type:  response\n</code></pre> <p>The <code>posterior_draws</code> function samples from the posterior distribution of the model, and produces a data frame with <code>drawid</code> and <code>draw</code> columns.</p> <pre><code>pred &lt;- posterior_draws(pred)\nhead(pred)\n#&gt;   drawid       draw rowid   estimate   conf.low conf.high  survived      age woman passengerClass\n#&gt; 1      1 0.46566713     1 0.51492993 0.43192231 0.6018749 0.4082218 29.88113     0            1st\n#&gt; 2      1 0.16658900     2 0.20128833 0.15362308 0.2613351 0.4082218 29.88113     0            2nd\n#&gt; 3      1 0.08750961     3 0.08750369 0.06555724 0.1141134 0.4082218 29.88113     0            3rd\n#&gt; 4      1 0.93735755     4 0.93641346 0.90660921 0.9587589 0.4082218 29.88113     1            1st\n#&gt; 5      1 0.77437334     5 0.77829290 0.70896643 0.8346419 0.4082218 29.88113     1            2nd\n#&gt; 6      1 0.62216334     6 0.57010265 0.49377997 0.6441967 0.4082218 29.88113     1            3rd\n</code></pre> <p>This \u201clong\u201d format makes it easy to plots results:</p> <pre><code>ggplot(pred, aes(x = draw, fill = factor(woman))) +\n    geom_density() +\n    facet_grid(~ passengerClass, labeller = label_both) +\n    labs(x = \"Predicted probability of survival\", y = \"\", fill = \"Woman\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#marginal-effects","title":"Marginal effects","text":"<p>Use <code>slopes()</code> to compute marginal effects (slopes of the regression equation) for each row of the dataset, and use <code>)</code> to compute \u201cAverage Marginal Effects\u201d, that is, the average of all observation-level marginal effects:</p> <pre><code>mfx &lt;- slopes(mod)\nmfx\n#&gt; \n#&gt;   Term Contrast  Estimate     2.5 %    97.5 %\n#&gt;  age      dY/dX -0.000237 -0.001335  0.000880\n#&gt;  age      dY/dX -0.007257 -0.008973 -0.005265\n#&gt;  age      dY/dX -0.000214 -0.000831  0.001242\n#&gt;  age      dY/dX -0.014258 -0.018487 -0.010306\n#&gt;  age      dY/dX -0.000234 -0.001242  0.000923\n#&gt; --- 4174 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n#&gt;  woman    1 - 0  0.516022  0.401674  0.630788\n#&gt;  woman    1 - 0  0.395843  0.307400  0.486515\n#&gt;  woman    1 - 0  0.468892  0.401425  0.536243\n#&gt;  woman    1 - 0  0.471069  0.403598  0.538028\n#&gt;  woman    1 - 0  0.478699  0.410060  0.547549\n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx, survived, woman, age, passengerClass \n#&gt; Type:  response\n</code></pre> <p>Compute marginal effects with some regressors fixed at user-specified values, and other regressors held at their means:</p> <pre><code>slopes(\n    mod,\n    newdata = datagrid(\n        woman = 1,\n        passengerClass = \"1st\"))\n#&gt; \n#&gt;            Term  Contrast woman passengerClass  Estimate    2.5 %    97.5 %\n#&gt;  age            dY/dX         1            1st -0.000238 -0.00136  0.000871\n#&gt;  passengerClass 2nd - 1st     1            1st -0.157442 -0.22327 -0.102890\n#&gt;  passengerClass 3rd - 1st     1            1st -0.365376 -0.43832 -0.294769\n#&gt;  woman          1 - 0         1            1st  0.420368  0.34697  0.490373\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, woman, passengerClass, predicted_lo, predicted_hi, predicted, tmp_idx, survived, age \n#&gt; Type:  response\n</code></pre> <p>Compute and plot conditional marginal effects:</p> <pre><code>plot_slopes(mod, variables = \"woman\", condition = \"age\")\n</code></pre> <p></p> <p>The <code>posterior_draws</code> produces a dataset with <code>drawid</code> and <code>draw</code> columns:</p> <pre><code>draws &lt;- posterior_draws(mfx)\n\ndim(draws)\n#&gt; [1] 16736000       16\n\nhead(draws)\n#&gt;   drawid          draw rowid term contrast      estimate      conf.low     conf.high predicted_lo predicted_hi predicted tmp_idx survived woman     age passengerClass\n#&gt; 1      1 -0.0001793450     1  age    dY/dX -0.0002373819 -0.0013354352  0.0008803236    0.9366624    0.9366585 0.9366604       1        1     1 29.0000            1st\n#&gt; 2      1 -0.0082459626     2  age    dY/dX -0.0072572604 -0.0089728266 -0.0052650726    0.8493348    0.8492752 0.8493050       2        1     0  0.9167            1st\n#&gt; 3      1 -0.0001667655     3  age    dY/dX -0.0002137451 -0.0008314784  0.0012415040    0.9433319    0.9433267 0.9433293       3        0     1  2.0000            1st\n#&gt; 4      1 -0.0160434697     4  age    dY/dX -0.0142578648 -0.0184866253 -0.0103056297    0.5131552    0.5130514 0.5131011       4        0     0 30.0000            1st\n#&gt; 5      1 -0.0001774318     5  age    dY/dX -0.0002336788 -0.0012419718  0.0009233460    0.9374947    0.9374927 0.9374937       5        0     1 25.0000            1st\n#&gt; 6      1 -0.0108173828     6  age    dY/dX -0.0112764204 -0.0143192681 -0.0085783113    0.2730949    0.2730116 0.2730542       6        1     0 48.0000            1st\n</code></pre> <p>We can use this dataset to plot our results. For example, to plot the posterior density of the marginal effect of <code>age</code> when the <code>woman</code> variable is equal to 0 or 1:</p> <pre><code>mfx &lt;- slopes(mod,\n    variables = \"age\",\n    newdata = datagrid(woman = 0:1)) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(woman))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal Effect of Age on Survival\",\n         y = \"Posterior density\",\n         fill = \"Woman\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#random-effects-model","title":"Random effects model","text":"<p>This section replicates some of the analyses of a random effects model published in Andrew Heiss\u2019 blog post: \u201cA guide to correctly calculating posterior predictions and average marginal effects with multilevel Bayesian models.\u201d The objective is mainly to illustrate the use of <code>marginaleffects</code>. Please refer to the original post for a detailed discussion of the quantities computed below.</p> <p>Load libraries and download data:</p> <pre><code>library(brms)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nvdem_2015 &lt;- read.csv(\"https://github.com/vincentarelbundock/marginaleffects/raw/main/data-raw/vdem_2015.csv\")\n\nhead(vdem_2015)\n#&gt;   country_name country_text_id year                           region media_index party_autonomy_ord polyarchy civil_liberties party_autonomy\n#&gt; 1       Mexico             MEX 2015  Latin America and the Caribbean       0.837                  3     0.631           0.704           TRUE\n#&gt; 2     Suriname             SUR 2015  Latin America and the Caribbean       0.883                  4     0.777           0.887           TRUE\n#&gt; 3       Sweden             SWE 2015 Western Europe and North America       0.956                  4     0.915           0.968           TRUE\n#&gt; 4  Switzerland             CHE 2015 Western Europe and North America       0.939                  4     0.901           0.960           TRUE\n#&gt; 5        Ghana             GHA 2015               Sub-Saharan Africa       0.858                  4     0.724           0.921           TRUE\n#&gt; 6 South Africa             ZAF 2015               Sub-Saharan Africa       0.898                  4     0.752           0.869           TRUE\n</code></pre> <p>Fit a basic model:</p> <pre><code>mod &lt;- brm(\n  bf(media_index ~ party_autonomy + civil_liberties + (1 | region),\n     phi ~ (1 | region)),\n  data = vdem_2015,\n  family = Beta(),\n  control = list(adapt_delta = 0.9))\n</code></pre>"},{"location":"vignettes/brms/#posterior-predictions","title":"Posterior predictions","text":"<p>To compute posterior predictions for specific values of the regressors, we use the <code>newdata</code> argument and the <code>datagrid</code> function. We also use the <code>type</code> argument to compute two types of predictions: accounting for residual (observation-level) residual variance (<code>prediction</code>) or ignoring it (<code>response</code>).</p> <pre><code>nd = datagrid(model = mod,\n              party_autonomy = c(TRUE, FALSE),\n              civil_liberties = .5,\n              region = \"Middle East and North Africa\")\np1 &lt;- predictions(mod, type = \"response\", newdata = nd) |&gt;\n    posterior_draws() |&gt;\n    transform(type = \"Response\")\np2 &lt;- predictions(mod, type = \"prediction\", newdata = nd) |&gt;\n    posterior_draws() |&gt;\n    transform(type = \"Prediction\")\npred &lt;- rbind(p1, p2)\n</code></pre> <p>Extract posterior draws and plot them:</p> <pre><code>ggplot(pred, aes(x = draw, fill = party_autonomy)) +\n    stat_halfeye(alpha = .5) +\n    facet_wrap(~ type) +\n    labs(x = \"Media index (predicted)\", \n         y = \"Posterior density\",\n         fill = \"Party autonomy\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#marginal-effects-and-contrasts","title":"Marginal effects and contrasts","text":"<p>As noted in the Marginal Effects vignette, there should be one distinct marginal effect for each combination of regressor values. Here, we consider only one combination of regressor values, where <code>region</code> is \u201cMiddle East and North Africa\u201d, and <code>civil_liberties</code> is 0.5. Then, we calculate the mean of the posterior distribution of marginal effects:</p> <pre><code>mfx &lt;- slopes(mod,\n                       newdata = datagrid(civil_liberties = .5,\n                                          region = \"Middle East and North Africa\"))\nmfx\n#&gt; \n#&gt;             Term     Contrast civil_liberties                       region Estimate 2.5 % 97.5 %\n#&gt;  civil_liberties dY/dX                    0.5 Middle East and North Africa    0.816 0.621  1.007\n#&gt;  party_autonomy  TRUE - FALSE             0.5 Middle East and North Africa    0.252 0.166  0.336\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, civil_liberties, region, predicted_lo, predicted_hi, predicted, tmp_idx, media_index, party_autonomy \n#&gt; Type:  response\n</code></pre> <p>Use the <code>posterior_draws()</code> to extract draws from the posterior distribution of marginal effects, and plot them:</p> <pre><code>mfx &lt;- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, y = term)) +\n  stat_halfeye() +\n  labs(x = \"Marginal effect\", y = \"\")\n</code></pre> <p></p> <p>Plot marginal effects, conditional on a regressor:</p> <pre><code>plot_slopes(mod,\n         variables = \"civil_liberties\",\n         condition = \"party_autonomy\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#continuous-predictors","title":"Continuous predictors","text":"<pre><code>pred &lt;- predictions(mod,\n                    newdata = datagrid(party_autonomy = FALSE,\n                                       region = \"Middle East and North Africa\",\n                                       civil_liberties = seq(0, 1, by = 0.05))) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n</code></pre> <p>The slope of this line for different values of civil liberties can be obtained with:</p> <pre><code>mfx &lt;- slopes(mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\")\nmfx\n#&gt; \n#&gt;             Term civil_liberties party_autonomy                       region Estimate 2.5 % 97.5 %\n#&gt;  civil_liberties             0.2          FALSE Middle East and North Africa    0.490 0.361  0.639\n#&gt;  civil_liberties             0.5          FALSE Middle East and North Africa    0.807 0.612  0.993\n#&gt;  civil_liberties             0.8          FALSE Middle East and North Africa    0.807 0.675  0.934\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, civil_liberties, party_autonomy, region, predicted_lo, predicted_hi, predicted, tmp_idx, media_index \n#&gt; Type:  response\n</code></pre> <p>And plotted:</p> <pre><code>mfx &lt;- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n</code></pre> <p></p> <p>The <code>slopes</code> function can use the ellipsis (<code>...</code>) to push any argument forward to the <code>posterior_predict</code> function. This can alter the types of predictions returned. For example, the <code>re_formula=NA</code> argument of the <code>posterior_predict.brmsfit</code> method will compute marginaleffects without including any group-level effects:</p> <pre><code>mfx &lt;- slopes(\n    mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\",\n    re_formula = NA) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#global-grand-mean","title":"Global grand mean","text":"<pre><code>pred &lt;- predictions(\n    mod,\n    re_formula = NA,\n    newdata = datagrid(party_autonomy = c(TRUE, FALSE))) |&gt;\n    posterior_draws()\n\nmfx &lt;- slopes(\n    mod,\n    re_formula = NA,\n    variables = \"party_autonomy\") |&gt;\n    posterior_draws()\n\nplot1 &lt;- ggplot(pred, aes(x = draw, fill = party_autonomy)) +\n         stat_halfeye(slab_alpha = .5) +\n         labs(x = \"Media index (Predicted)\",\n              y = \"Posterior density\",\n              fill = \"Party autonomy\")\n\nplot2 &lt;- ggplot(mfx, aes(x = draw)) +\n         stat_halfeye(slab_alpha = .5)  +\n         labs(x = \"Contrast: Party autonomy TRUE - FALSE\",\n              y = \"\",\n              fill = \"Party autonomy\")\n\n## combine plots using the `patchwork` package\nplot1 + plot2\n</code></pre>"},{"location":"vignettes/brms/#region-specific-predictions-and-contrasts","title":"Region-specific predictions and contrasts","text":"<p>Predicted media index by region and level of civil liberties:</p> <pre><code>pred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = FALSE, \n                                       civil_liberties = seq(0, 1, length.out = 100))) |&gt; \n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    facet_wrap(~ region) +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n</code></pre> <p></p> <p>Predicted media index by region and level of civil liberties:</p> <pre><code>pred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       civil_liberties = c(.2, .8),\n                                      party_autonomy = FALSE)) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    facet_wrap(~ region) +\n    labs(x = \"Media index (predicted)\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n</code></pre> <p></p> <p>Predicted media index by region and party autonomy:</p> <pre><code>pred &lt;- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = c(TRUE, FALSE),\n                                       civil_liberties = .5)) |&gt;\n        posterior_draws()\n\nggplot(pred, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n</code></pre> <p></p> <p>TRUE/FALSE contrasts (marginal effects) of party autonomy by region:</p> <pre><code>mfx &lt;- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    newdata = datagrid(\n        region = vdem_2015$region,\n        civil_liberties = .5)) |&gt;\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#hypothetical-groups","title":"Hypothetical groups","text":"<p>We can also obtain predictions or marginal effects for a hypothetical group instead of one of the observed regions. To achieve this, we create a dataset with <code>NA</code> in the <code>region</code> column. Then we call the <code>marginaleffects</code> or <code>predictions</code> functions with the <code>allow_new_levels</code> argument. This argument is pushed through via the ellipsis (<code>...</code>) to the <code>posterior_epred</code> function of the <code>brms</code> package:</p> <pre><code>dat &lt;- data.frame(civil_liberties = .5,\n                  party_autonomy = FALSE,\n                  region = \"New Region\")\n\nmfx &lt;- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    allow_new_levels = TRUE,\n    newdata = dat)\n\ndraws &lt;- posterior_draws(mfx)\n\nggplot(draws, aes(x = draw)) +\n     stat_halfeye() +\n     labs(x = \"Marginal effect of party autonomy in a generic world region\", y = \"\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#averaging-marginalizing-integrating-random-effects","title":"Averaging, marginalizing, integrating random effects","text":"<p>Consider a logistic regression model with random effects:</p> <pre><code>dat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/plm/EmplUK.csv\")\ndat$x &lt;- as.numeric(dat$output &gt; median(dat$output))\ndat$y &lt;- as.numeric(dat$emp &gt; median(dat$emp))\nmod &lt;- brm(y ~ x + (1 | firm), data = dat, backend = \"cmdstanr\", family = \"bernoulli\")\n</code></pre> <p>We can compute adjusted predictions for a given value of <code>x</code> and for each firm (random effects) as follows:</p> <pre><code>p &lt;- predictions(mod, newdata = datagrid(x = 0, firm = unique))\nhead(p)\n#&gt; \n#&gt;  x firm Estimate    2.5 % 97.5 %\n#&gt;  0    1  1.0e+00 9.01e-01 1.0000\n#&gt;  0    2  1.0e+00 8.95e-01 1.0000\n#&gt;  0    3  1.0e+00 9.12e-01 1.0000\n#&gt;  0    4  1.0e+00 7.97e-01 1.0000\n#&gt;  0    5  1.0e+00 9.09e-01 1.0000\n#&gt;  0    6  4.9e-08 8.42e-21 0.0019\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, y, x, firm \n#&gt; Type:  response\n</code></pre> <p>We can average/marginalize/integrate across random effects with the <code>avg_predictions()</code> function or the <code>by</code> argument:</p> <pre><code>avg_predictions(mod, newdata = datagrid(x = 0, firm = unique))\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;     0.454  0.44  0.468\n#&gt; \n#&gt; Columns: estimate, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(x = 0:1, firm = unique), by = \"x\")\n#&gt; \n#&gt;  x Estimate 2.5 % 97.5 %\n#&gt;  0    0.454 0.440  0.468\n#&gt;  1    0.557 0.546  0.570\n#&gt; \n#&gt; Columns: x, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can also draw from the (assumed gaussian) population distribution of random effects, by asking <code>predictions()</code> to make predictions for new \u201clevels\u201d of the random effects. If we then take an average of predictions using <code>avg_predictions()</code> or the <code>by</code> argument, we will have \u201cintegrated out the random effects\u201d, as described in the <code>brmsmargins</code> package vignette. In the code below, we make predictions for 100 firm identifiers which were not in the original dataset. We also ask <code>predictions()</code> to push forward the <code>allow_new_levels</code> and <code>sample_new_levels</code> arguments to the <code>brms::posterior_epred</code> function:</p> <pre><code>predictions(\n    mod,\n    newdata = datagrid(x = 0:1, firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\",\n    by = \"x\")\n#&gt; \n#&gt;  x Estimate 2.5 % 97.5 %\n#&gt;  0    0.452 0.338  0.568\n#&gt;  1    0.551 0.437  0.666\n#&gt; \n#&gt; Columns: x, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can \u201cintegrate out\u201d random effects in the other <code>slopes</code> functions too. For instance,</p> <pre><code>avg_comparisons(\n    mod,\n    newdata = datagrid(firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n#&gt; \n#&gt;  Term Contrast Estimate  2.5 % 97.5 %\n#&gt;     x    1 - 0   0.0965 0.0474  0.163\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>This is nearly equivalent the <code>brmsmargins</code> command output (with slight variations due to different random seeds):</p> <pre><code>library(brmsmargins)\nbm &lt;- brmsmargins(\n  k = 100,\n  object = mod,\n  at = data.frame(x = c(0, 1)),\n  CI = .95,\n  CIType = \"ETI\",\n  contrasts = cbind(\"AME x\" = c(-1, 1)),\n  effects = \"integrateoutRE\")\nbm$ContrastSummary |&gt; data.frame()\n#&gt;            M        Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#&gt; 1 0.09855037 0.09647303 0.04909088 0.1627466          NA         NA 0.95    ETI &lt;NA&gt; &lt;NA&gt; AME x\n</code></pre> <p>See the alternative software vignette for more information on <code>brmsmargins</code>.</p>"},{"location":"vignettes/brms/#multinomial-logit","title":"Multinomial logit","text":"<p>Fit a model with categorical outcome (heating system choice in California houses) and logit link:</p> <pre><code>dat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Heating.csv\"\ndat &lt;- read.csv(dat)\nmod &lt;- brm(depvar ~ ic.gc + oc.gc,\n           data = dat,\n           family = categorical(link = \"logit\"))\n</code></pre>"},{"location":"vignettes/brms/#adjusted-predictions_1","title":"Adjusted predictions","text":"<p>Compute predicted probabilities for each level of the outcome variable:</p> <pre><code>pred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Group Estimate  2.5 % 97.5 %\n#&gt;     ec   0.0663 0.0447 0.0930\n#&gt;     ec   0.0768 0.0590 0.0974\n#&gt;     ec   0.1030 0.0618 0.1585\n#&gt;     ec   0.0634 0.0459 0.0838\n#&gt;     ec   0.0745 0.0574 0.0947\n#&gt;     ec   0.0709 0.0455 0.1036\n#&gt; \n#&gt; Columns: rowid, group, estimate, conf.low, conf.high, depvar, ic.gc, oc.gc \n#&gt; Type:  response\n</code></pre> <p>Extract posterior draws and plot them:</p> <pre><code>draws &lt;- posterior_draws(pred)\n\nggplot(draws, aes(x = draw, fill = group)) +\n    geom_density(alpha = .2, color = \"white\") +\n    labs(x = \"Predicted probability\",\n         y = \"Density\",\n         fill = \"Heating system\")\n</code></pre> <p></p> <p>Use the <code>plot_predictions</code> function to plot conditional adjusted predictions for each level of the outcome variable <code>gear</code>, conditional on the value of the <code>mpg</code> regressor:</p> <pre><code>plot_predictions(mod, condition = \"oc.gc\") +\n    facet_wrap(~ group) +\n    labs(y = \"Predicted probability\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#marginal-effects_1","title":"Marginal effects","text":"<pre><code>avg_slopes(mod)\n#&gt; \n#&gt;  Group  Term  Estimate     2.5 %   97.5 %\n#&gt;     ec ic.gc -1.77e-04 -3.96e-04 2.37e-05\n#&gt;     er ic.gc  1.65e-05 -2.26e-04 2.51e-04\n#&gt;     gc ic.gc  1.38e-05 -3.72e-04 4.00e-04\n#&gt;     gr ic.gc  4.24e-05 -2.37e-04 3.29e-04\n#&gt;     hp ic.gc  1.07e-04 -7.73e-05 2.97e-04\n#&gt;     ec oc.gc  4.88e-04 -4.04e-04 1.45e-03\n#&gt;     er oc.gc -1.02e-03 -2.07e-03 2.98e-05\n#&gt;     gc oc.gc  1.04e-03 -7.39e-04 2.78e-03\n#&gt;     gr oc.gc  9.46e-05 -1.19e-03 1.34e-03\n#&gt;     hp oc.gc -5.85e-04 -1.45e-03 2.30e-04\n#&gt; \n#&gt; Columns: term, group, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/brms/#hurdle-models","title":"Hurdle models","text":"<p>This section replicates some analyses from yet another amazing blog post by Andrew Heiss.</p> <p>To begin, we estimate a hurdle model in <code>brms</code> with random effects, using data from the <code>gapminder</code> package: 704G</p> <pre><code>library(gapminder)\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(cmdstanr)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nset.seed(1024)\n\nCHAINS &lt;- 4\nITER &lt;- 2000\nWARMUP &lt;- 1000\nBAYES_SEED &lt;- 1234\n\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero) |&gt; \n  # Make a logged version of GDP per capita\n  mutate(log_gdpPercap = log1p(gdpPercap)) |&gt; \n  mutate(is_zero = gdpPercap == 0)\n\nmod &lt;- brm(\n  bf(gdpPercap ~ lifeExp + year + (1 + lifeExp + year | continent),\n     hu ~ lifeExp),\n  data = gapminder,\n  backend = \"cmdstanr\",\n  family = hurdle_lognormal(),\n  cores = 2,\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2)\n</code></pre>"},{"location":"vignettes/brms/#adjusted-predictions_2","title":"Adjusted predictions","text":"<p>Adjusted predictions for every observation in the original data:</p> <pre><code>predictions(mod) |&gt; head()\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;       143   103    219\n#&gt;       168   125    256\n#&gt;       202   153    304\n#&gt;       251   197    373\n#&gt;       312   250    454\n#&gt;       398   325    567\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  response\n</code></pre> <p>Adjusted predictions for the <code>hu</code> parameter:</p> <pre><code>predictions(mod, dpar = \"hu\") |&gt; head()\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 %\n#&gt;     0.574 0.475  0.652\n#&gt;     0.537 0.442  0.611\n#&gt;     0.496 0.407  0.566\n#&gt;     0.446 0.366  0.511\n#&gt;     0.396 0.325  0.454\n#&gt;     0.341 0.282  0.391\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  response\n</code></pre> <p>Predictions on a different scale:</p> <pre><code>predictions(mod, type = \"link\", dpar = \"hu\") |&gt; head()\n#&gt; \n#&gt;  Estimate  2.5 %  97.5 %\n#&gt;    0.2980 -0.101  0.6259\n#&gt;    0.1463 -0.235  0.4527\n#&gt;   -0.0178 -0.377  0.2673\n#&gt;   -0.2189 -0.551  0.0424\n#&gt;   -0.4234 -0.730 -0.1857\n#&gt;   -0.6573 -0.933 -0.4443\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent \n#&gt; Type:  link\n</code></pre> <p>Plot adjusted predictions as a function of <code>lifeExp</code>:</p> <pre><code>plot_predictions(\n    mod,\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\nplot_predictions(\n    mod,\n    dpar = \"hu\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n</code></pre> <p></p> <p>Predictions with more than one condition and the <code>re_formula</code> argument from <code>brms</code>:</p> <pre><code>plot_predictions(\n    mod,\n    re_formula = NULL,\n    condition = c(\"lifeExp\", \"continent\"))\n</code></pre> <p></p>"},{"location":"vignettes/brms/#extract-draws-with-posterior_draws","title":"Extract draws with <code>posterior_draws()</code>","text":"<p>The <code>posterior_draws()</code> function extract raw samples from the posterior from objects produced by <code>marginaleffects</code>. This allows us to use richer geoms and summaries, such as those in the <code>ggdist</code> package:</p> <pre><code>predictions(\n    mod,\n    re_formula = NULL,\n    newdata = datagrid(model = mod,\n                       continent = gapminder$continent,\n                       year = c(1952, 2007),\n                       lifeExp = seq(30, 80, 1))) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(lifeExp, draw, fill = continent, color = continent)) +\n    stat_lineribbon(alpha = .25) +\n    facet_grid(year ~ continent)\n</code></pre> <p></p>"},{"location":"vignettes/brms/#average-contrasts","title":"Average Contrasts","text":"<p>What happens to <code>gdpPercap</code> when <code>lifeExp</code> increases by one?</p> <pre><code>avg_comparisons(mod)\n#&gt; \n#&gt;     Term Contrast Estimate 2.5 % 97.5 %\n#&gt;  lifeExp       +1    759.7 535.8  862.8\n#&gt;  year          +1    -63.5 -83.9  -40.9\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>What happens to <code>gdpPercap</code> when <code>lifeExp</code> increases by one standard deviation?</p> <pre><code>avg_comparisons(mod, variables = list(lifeExp = \"sd\"))\n#&gt; \n#&gt;     Term                Contrast Estimate 2.5 % 97.5 %\n#&gt;  lifeExp (x + sd/2) - (x - sd/2)     4050  3718   4741\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>What happens to <code>gdpPercap</code> when <code>lifeExp</code> increases from 50 to 60 and <code>year</code> simultaneously increases its min to its max?</p> <pre><code>avg_comparisons(\n    mod,\n    variables = list(lifeExp = c(50, 60), year = \"minmax\"),\n    cross = TRUE)\n#&gt; \n#&gt;  Estimate 2.5 % 97.5 % C: lifeExp   C: year\n#&gt;       835   523   1404    60 - 50 Max - Min\n#&gt; \n#&gt; Columns: term, contrast_lifeExp, contrast_year, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Plot draws from the posterior distribution of average contrasts (not the same thing as draws from the posterior distribution of contrasts):</p> <pre><code>avg_comparisons(mod) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(estimate, term)) +\n    stat_dotsinterval() +\n    labs(x = \"Posterior distribution of average contrasts\", y = \"\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#marginal-effects-slopes","title":"Marginal effects (slopes)","text":"<p>Average Marginal Effect of <code>lifeExp</code> on different scales and for different parameters:</p> <pre><code>avg_slopes(mod)\n#&gt; \n#&gt;     Term Estimate 2.5 % 97.5 %\n#&gt;  lifeExp    718.5 515.4  811.4\n#&gt;  year       -63.8 -84.4  -41.1\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, type = \"link\")\n#&gt; \n#&gt;     Term Estimate   2.5 %   97.5 %\n#&gt;  lifeExp  0.08249  0.0742  0.08856\n#&gt;  year    -0.00937 -0.0120 -0.00632\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  link\n\navg_slopes(mod, dpar = \"hu\")\n#&gt; \n#&gt;     Term Estimate    2.5 %   97.5 %\n#&gt;  lifeExp -0.00817 -0.00937 -0.00669\n#&gt;  year     0.00000  0.00000  0.00000\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, dpar = \"hu\", type = \"link\")\n#&gt; \n#&gt;     Term Estimate  2.5 %  97.5 %\n#&gt;  lifeExp  -0.0993 -0.113 -0.0838\n#&gt;  year      0.0000  0.000  0.0000\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  link\n</code></pre> <p>Plot Conditional Marginal Effects</p> <pre><code>plot_slopes(\n    mod,\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\n\nplot_slopes(\n    mod,\n    dpar = \"hu\",\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n</code></pre> <p></p> <p>Or we can call <code>slopes()</code> or <code>comparisons()</code> with <code>posterior_draws()</code> function to have even more control:</p> <pre><code>comparisons(\n    mod,\n    type = \"link\",\n    variables = \"lifeExp\",\n    newdata = datagrid(lifeExp = c(40, 70), continent = gapminder$continent)) |&gt;\n    posterior_draws() |&gt;\n    ggplot(aes(draw, continent, fill = continent)) +\n    stat_dotsinterval() +\n    facet_grid(lifeExp ~ .) +\n    labs(x = \"Effect of a 1 unit change in Life Expectancy\")\n</code></pre> <p></p>"},{"location":"vignettes/brms/#bayesian-estimates-and-credible-intervals","title":"Bayesian estimates and credible intervals","text":"<p>For bayesian models like those produced by the <code>brms</code> or <code>rstanarm</code> packages, the <code>marginaleffects</code> package functions report the median of the posterior distribution as their main estimates.</p> <p>The default credible intervals are equal-tailed intervals (quantiles), and the default function to identify the center of the distribution is the median. Users can customize the type of intervals reported by setting global options. Note that both the reported estimate and the intervals change slightly:</p> <pre><code>library(insight)\nlibrary(marginaleffects)\n\nmod &lt;- insight::download_model(\"brms_1\")\n\noptions(marginaleffects_posterior_interval = \"hdi\")\noptions(marginaleffects_posterior_center = mean)\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   cyl       +1    -1.50 -2.38 -0.677\n#&gt;   wt        +1    -3.21 -4.70 -1.570\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_posterior_interval = \"eti\")\noptions(marginaleffects_posterior_center = stats::median)\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate 2.5 % 97.5 %\n#&gt;   cyl       +1    -1.49 -2.36 -0.636\n#&gt;   wt        +1    -3.20 -4.79 -1.645\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/brms/#random-variables-posterior-and-ggdist","title":"Random variables: <code>posterior</code> and <code>ggdist</code>","text":"<p>Recent versions of the <code>posterior</code>, <code>brms</code>, and <code>ggdist</code> packages make it easy to draw, summarize and plot random variables. The <code>posterior_draws()</code> can produce objects of class <code>rvar</code> which make it easy to use those features by returning a data frame with a column of type <code>rvar</code>:</p> <pre><code>library(brms)\nlibrary(ggdist)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nmod &lt;- brm(am ~ mpg + hp, data = mtcars, family = bernoulli)\n</code></pre> <pre><code>avg_comparisons(mod) |&gt;\n  posterior_draws(shape = \"rvar\") |&gt;\n  ggplot(aes(y = term, xdist = rvar)) + \n  stat_slabinterval()\n</code></pre> <p></p>"},{"location":"vignettes/brms/#non-linear-hypothesis-testing","title":"Non-linear hypothesis testing","text":"<p>We begin by estimating a model:</p> <pre><code>mod &lt;- brm(am ~ mpg + hp, data = mtcars, family = bernoulli(),\n           seed = 1024, silent = 2, chains = 4, iter = 1000)\n</code></pre> <p>Notice that we can compute average contrasts in two different ways, using the <code>avg_comparisons()</code> function or the <code>comparison</code> argument:</p> <pre><code>avg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate   2.5 %  97.5 %\n#&gt;   hp        +1  0.00601 0.00289 0.00895\n#&gt;   mpg       +1  0.13942 0.08464 0.18139\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, comparison = \"differenceavg\")\n#&gt; \n#&gt;  Term Contrast Estimate   2.5 %  97.5 %\n#&gt;   hp  mean(+1)  0.00601 0.00289 0.00895\n#&gt;   mpg mean(+1)  0.13942 0.08464 0.18139\n#&gt; \n#&gt; Columns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx \n#&gt; Type:  response\n</code></pre> <p>Now, we use the <code>hypothesis</code> argument to compare the first to the second rows of the <code>comparisons()</code> output:</p> <pre><code>comparisons(\n    mod,\n    comparison = \"differenceavg\",\n    hypothesis = \"b2 - b1 = 0.2\")\n#&gt; \n#&gt;       Term Estimate  2.5 % 97.5 %\n#&gt;  b2-b1=0.2  -0.0665 -0.119 -0.027\n#&gt; \n#&gt; Columns: term, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The <code>hypothesis()</code> function of the <code>brms</code> package can also perform non-linear hypothesis testing, and it generates some convenient statistics and summaries. This function accepts a D-by-P matrix of draws from the posterior distribution, where D is the number of draws and N is the number of parameters. We can obtain such a matrix using the <code>posterior_draws(x, shape = \"DxP\")</code>, and we can simply add a couple calls to our chain of operations:</p> <pre><code>avg_comparisons(mod, comparison = \"differenceavg\") |&gt;\n    posterior_draws(shape = \"DxP\") |&gt;\n    brms::hypothesis(\"b2 - b1 &gt; .2\")\n#&gt; Hypothesis Tests for class :\n#&gt;         Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n#&gt; 1 (b2-b1)-(.2) &gt; 0    -0.07      0.02    -0.11    -0.03          0         0     \n#&gt; ---\n#&gt; 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n#&gt; '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n#&gt; for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n#&gt; Posterior probabilities of point hypotheses assume equal prior probabilities.\n</code></pre>"},{"location":"vignettes/categorical/","title":"Categorical outcomes","text":"<p>Several packages in the <code>R</code> ecosystem allow users to estimate models for ordered or discrete choice, such as ordered probit or multinomial logit. This case study illustrates the use of <code>marginaleffects</code> with the <code>MASS</code>, <code>nnet</code>, and <code>mlogit</code> packages.</p> <p>We begin by loading two libraries:</p> <pre><code>library(marginaleffects)\nlibrary(tidyverse)\n</code></pre>"},{"location":"vignettes/categorical/#masspolr-function","title":"<code>MASS::polr</code> function","text":"<p>Consider a simple ordered logit model in which we predict the number of gears of a car based its miles per gallon and horsepower:</p> <pre><code>library(MASS)\nmod &lt;- polr(factor(gear) ~ mpg + hp, data = mtcars, Hess = TRUE)\n</code></pre> <p>Now, consider a car with 25 miles per gallon and 110 horsepower. The expected predicted probability for each outcome level (gear) for this car is:</p> <pre><code>predictions(mod, newdata = datagrid(mpg = 25, hp = 110))\n#&gt; \n#&gt;  Group mpg  hp Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;      3  25 110    0.203     0.0959 2.12   0.0339  4.9 0.0155  0.391\n#&gt;      4  25 110    0.578     0.1229 4.70   &lt;0.001 18.6 0.3373  0.819\n#&gt;      5  25 110    0.218     0.1007 2.17   0.0302  5.1 0.0209  0.416\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp \n#&gt; Type:  probs\n</code></pre> <p>Since the <code>gear</code> is categorical, we make one prediction for each level of the outcome.</p> <p>Now consider the marginal effects (aka slopes or partial derivatives) for the same car:</p> <pre><code>slopes(mod, variables = \"mpg\", newdata = datagrid(mpg = 25, hp = 110))\n#&gt; \n#&gt;  Group Term mpg  hp Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;      3  mpg  25 110 -0.06042     0.0169 -3.5812   &lt;0.001 11.5 -0.09349 -0.0274\n#&gt;      4  mpg  25 110 -0.00318     0.0335 -0.0949   0.9244  0.1 -0.06891  0.0625\n#&gt;      5  mpg  25 110  0.06361     0.0301  2.1135   0.0346  4.9  0.00462  0.1226\n#&gt; \n#&gt; Columns: rowid, term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, predicted_lo, predicted_hi, predicted, gear \n#&gt; Type:  probs\n</code></pre> <p>Again, <code>marginaleffects</code> produces one estimate of the slope for each outcome level. For a small step size \u03b5, the printed quantities are estimated as:</p> <p>$$\\frac{P(gear=3|mpg=25+\\varepsilon, hp=110)-P(gear=3|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}$$ $$\\frac{P(gear=4|mpg=25+\\varepsilon, hp=110)-P(gear=4|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}$$ $$\\frac{P(gear=5|mpg=25+\\varepsilon, hp=110)-P(gear=5|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}$$</p> <p>When we call <code>avg_slopes()</code>, <code>marginaleffects</code> will repeat the same computation for every row of the original dataset, and then report the average slope for each level of the outcome:</p> <pre><code>avg_slopes(mod)\n#&gt; \n#&gt;  Group Term Estimate Std. Error     z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;      3  hp  -0.00377   0.001514 -2.49  0.01285  6.3 -0.006735 -0.00080\n#&gt;      3  mpg -0.07014   0.015483 -4.53  &lt; 0.001 17.4 -0.100485 -0.03979\n#&gt;      4  hp   0.00201   0.000958  2.10  0.03554  4.8  0.000136  0.00389\n#&gt;      4  mpg  0.03748   0.013860  2.70  0.00685  7.2  0.010311  0.06464\n#&gt;      5  hp   0.00175   0.000833  2.11  0.03518  4.8  0.000122  0.00339\n#&gt;      5  mpg  0.03266   0.009570  3.41  &lt; 0.001 10.6  0.013905  0.05142\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre>"},{"location":"vignettes/categorical/#nnet-package","title":"<code>nnet</code> package","text":"<p>The <code>multinom</code> function of the <code>nnet</code> package allows users to fit log-linear models via neural networks. The <code>data</code> used for this function is a data frame with one observation per row, and the response variable is coded a factor. All the <code>marginaleffects</code> package function work seamlessly with this model. For example, we can estimate a model and compute average marginal effects as follows:</p> <pre><code>library(nnet)\n\nhead(mtcars)\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nmod &lt;- multinom(factor(gear) ~ hp + mpg, data = mtcars, trace = FALSE)\n\navg_slopes(mod, type = \"probs\")\n#&gt; \n#&gt;  Group Term  Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %    97.5 %\n#&gt;      3  hp  -3.36e-05    0.00225 -0.0149  0.98809  0.0 -0.00444  0.004372\n#&gt;      3  mpg -7.13e-02    0.02646 -2.6951  0.00704  7.2 -0.12315 -0.019448\n#&gt;      4  hp  -4.67e-03    0.00221 -2.1126  0.03463  4.9 -0.00900 -0.000337\n#&gt;      4  mpg  1.59e-02    0.02010  0.7917  0.42851  1.2 -0.02348  0.055316\n#&gt;      5  hp   4.70e-03    0.00130  3.6170  &lt; 0.001 11.7  0.00215  0.007247\n#&gt;      5  mpg  5.54e-02    0.01642  3.3732  &lt; 0.001 10.4  0.02320  0.087563\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre> <p>Notice that in such models, we get one marginal effect for each term, for each level of the response variable. For this reason, we should use <code>\"group\"</code> in the <code>condition</code> argument (or <code>facet_*()</code> function) when calling one of the plotting functions:</p> <pre><code>library(ggplot2)\n\nplot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\")\n</code></pre> <p></p> <pre><code>plot_predictions(mod, condition = \"mpg\", type = \"probs\") + facet_wrap(~group)\n</code></pre> <p></p> <pre><code>plot_comparisons(\n    mod,\n    variables = list(mpg = c(15, 30)),\n    condition = \"group\",\n    type = \"probs\")\n</code></pre> <p></p>"},{"location":"vignettes/categorical/#mlogit-package","title":"<code>mlogit</code> package","text":"<p>The <code>mlogit</code> package uses <code>data</code> in a slightly different structure, with one row per observation-choice combination. For example, this data on choice of travel mode includes 4 rows per individual, one for each mode of transportation:</p> <pre><code>library(\"AER\")\nlibrary(\"mlogit\")\nlibrary(\"tidyverse\")\ndata(\"TravelMode\", package = \"AER\")\n\nhead(TravelMode)\n#&gt;   individual  mode choice wait vcost travel gcost income size\n#&gt; 1          1   air     no   69    59    100    70     35    1\n#&gt; 2          1 train     no   34    31    372    71     35    1\n#&gt; 3          1   bus     no   35    25    417    70     35    1\n#&gt; 4          1   car    yes    0    10    180    30     35    1\n#&gt; 5          2   air     no   64    58     68    68     30    2\n#&gt; 6          2 train     no   44    31    354    84     30    2\n\nmod &lt;- mlogit(choice ~ wait + gcost | income + size, TravelMode)\n\navg_slopes(mod, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;  air   income  0.002786    0.00122  2.289  0.02209  5.5  0.000400  0.00517\n#&gt;  air   size   -0.126477    0.02892 -4.373  &lt; 0.001 16.3 -0.183162 -0.06979\n#&gt;  bus   income -0.000372    0.00110 -0.338  0.73557  0.4 -0.002531  0.00179\n#&gt;  bus   size    0.011349    0.02587  0.439  0.66084  0.6 -0.039347  0.06204\n#&gt;  car   income  0.003373    0.00137  2.455  0.01407  6.2  0.000681  0.00607\n#&gt;  car   size    0.045887    0.02476  1.853  0.06381  4.0 -0.002636  0.09441\n#&gt;  train income -0.005787    0.00132 -4.390  &lt; 0.001 16.4 -0.008371 -0.00320\n#&gt;  train size    0.069241    0.02478  2.794  0.00521  7.6  0.020666  0.11782\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Note that the <code>slopes</code> function will always return estimates of zero for regressors before the vertical bar in the formula. This is because <code>marginaleffects</code> increments all rows of the prediction dataset in the same way to compute slopes and contrast. Because <code>mlogit</code> data are in \u201clong\u201d format, this means that alternatives are incremented in the same way, which does not produce alternative-specific changes in the predictors.</p> <p>One strategy to circumvent this problem is to supply a data frame of numeric values to compare, with alternative specific changes. In this example, we test what happens to the probability of selecting each mode of transportation if we only increase the wait time of air travel:</p> <pre><code>altspec &lt;- data.frame(\n  low = TravelMode$wait,\n  high = ifelse(TravelMode$mode == \"air\", TravelMode$wait + 15, TravelMode$wait)\n)\n\navg_comparisons(mod, variables = list(wait = altspec))\n#&gt; \n#&gt;  Group Term Contrast Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 %  97.5 %\n#&gt;  air   wait   manual  -0.1321    0.01070 -12.35   &lt;0.001 114.0 -0.1531 -0.1111\n#&gt;  bus   wait   manual   0.0251    0.00460   5.45   &lt;0.001  24.2  0.0160  0.0341\n#&gt;  car   wait   manual   0.0701    0.00834   8.41   &lt;0.001  54.5  0.0538  0.0865\n#&gt;  train wait   manual   0.0369    0.00528   6.99   &lt;0.001  38.4  0.0266  0.0473\n#&gt; \n#&gt; Columns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can compute yet more kinds of marginal effects, we can construct customized data frames and feed them to the <code>newdata</code> argument of the <code>slopes</code> function.</p> <p>If we want to compute the slope of the response function (marginal effects) when each of the predictors is fixed to its global mean, we can do:</p> <pre><code>nd &lt;- TravelMode |&gt;\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"),\n              function(x) rep(mean(x), 4)))\nnd\n#&gt;       wait    gcost   income     size\n#&gt; 1 34.58929 110.8798 34.54762 1.742857\n#&gt; 2 34.58929 110.8798 34.54762 1.742857\n#&gt; 3 34.58929 110.8798 34.54762 1.742857\n#&gt; 4 34.58929 110.8798 34.54762 1.742857\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error     z Pr(&gt;|z|)   S     2.5 %    97.5 %\n#&gt;  air   income  6.66e-03   2.42e-03  2.75  0.00599 7.4  1.91e-03  1.14e-02\n#&gt;  air   size   -1.69e-01   5.87e-02 -2.88  0.00393 8.0 -2.85e-01 -5.43e-02\n#&gt;  bus   income -1.14e-03   9.43e-04 -1.21  0.22630 2.1 -2.99e-03  7.07e-04\n#&gt;  bus   size    4.67e-02   2.72e-02  1.72  0.08622 3.5 -6.65e-03  1.00e-01\n#&gt;  car   income  6.48e-06   2.02e-05  0.32  0.74885 0.4 -3.32e-05  4.62e-05\n#&gt;  car   size    1.36e-03   8.81e-04  1.54  0.12304 3.0 -3.68e-04  3.08e-03\n#&gt;  train income -5.52e-03   1.91e-03 -2.89  0.00384 8.0 -9.27e-03 -1.78e-03\n#&gt;  train size    1.21e-01   4.44e-02  2.73  0.00634 7.3  3.42e-02  2.08e-01\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>If we want to compute marginal effects with the <code>gcost</code> and <code>wait</code> fixed at their mean value, conditional on the choice of transportation mode:</p> <pre><code>nd &lt;- TravelMode |&gt;\n    group_by(mode) |&gt;\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"), mean))\nnd\n#&gt; # A tibble: 4 \u00d7 5\n#&gt;   mode   wait gcost income  size\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 air    61.0 103.    34.5  1.74\n#&gt; 2 train  35.7 130.    34.5  1.74\n#&gt; 3 bus    41.7 115.    34.5  1.74\n#&gt; 4 car     0    95.4   34.5  1.74\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %   97.5 %\n#&gt;  air   income  0.006015    0.00233  2.587  0.00968  6.7  0.001458  0.01057\n#&gt;  air   size   -0.232955    0.05661 -4.115  &lt; 0.001 14.7 -0.343900 -0.12201\n#&gt;  bus   income -0.000713    0.00146 -0.489  0.62478  0.7 -0.003568  0.00214\n#&gt;  bus   size    0.020447    0.03436  0.595  0.55185  0.9 -0.046906  0.08780\n#&gt;  car   income  0.005445    0.00229  2.382  0.01724  5.9  0.000964  0.00993\n#&gt;  car   size    0.067839    0.04123  1.645  0.09991  3.3 -0.012974  0.14865\n#&gt;  train income -0.010748    0.00256 -4.201  &lt; 0.001 15.2 -0.015762 -0.00573\n#&gt;  train size    0.144669    0.04773  3.031  0.00244  8.7  0.051127  0.23821\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can also explore more complex alternatives. Here, for example, only one alternative is affected by cost reduction:</p> <pre><code>nd &lt;- datagrid(mode = TravelMode$mode, newdata = TravelMode)\nnd &lt;- lapply(1:4, function(i) mutate(nd, gcost = ifelse(1:4 == i, 30, gcost)))\nnd &lt;- bind_rows(nd)\nnd\n#&gt;    individual choice wait vcost travel gcost income size  mode\n#&gt; 1           1     no   35    48    486    30     35    2   air\n#&gt; 2           1     no   35    48    486   111     35    2 train\n#&gt; 3           1     no   35    48    486   111     35    2   bus\n#&gt; 4           1     no   35    48    486   111     35    2   car\n#&gt; 5           1     no   35    48    486   111     35    2   air\n#&gt; 6           1     no   35    48    486    30     35    2 train\n#&gt; 7           1     no   35    48    486   111     35    2   bus\n#&gt; 8           1     no   35    48    486   111     35    2   car\n#&gt; 9           1     no   35    48    486   111     35    2   air\n#&gt; 10          1     no   35    48    486   111     35    2 train\n#&gt; 11          1     no   35    48    486    30     35    2   bus\n#&gt; 12          1     no   35    48    486   111     35    2   car\n#&gt; 13          1     no   35    48    486   111     35    2   air\n#&gt; 14          1     no   35    48    486   111     35    2 train\n#&gt; 15          1     no   35    48    486   111     35    2   bus\n#&gt; 16          1     no   35    48    486    30     35    2   car\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#&gt; \n#&gt;  Group   Term  Estimate Std. Error      z Pr(&gt;|z|)    S     2.5 %    97.5 %\n#&gt;  air   income  8.24e-03   2.46e-03  3.352   &lt;0.001 10.3  3.42e-03  0.013058\n#&gt;  air   size   -2.12e-01   6.02e-02 -3.526   &lt;0.001 11.2 -3.30e-01 -0.094353\n#&gt;  bus   income -1.33e-03   1.30e-03 -1.020    0.308  1.7 -3.88e-03  0.001223\n#&gt;  bus   size    6.06e-02   3.79e-02  1.600    0.110  3.2 -1.36e-02  0.134895\n#&gt;  car   income  2.66e-05   4.31e-05  0.617    0.537  0.9 -5.78e-05  0.000111\n#&gt;  car   size    2.38e-03   1.57e-03  1.512    0.131  2.9 -7.04e-04  0.005459\n#&gt;  train income -6.94e-03   1.86e-03 -3.735   &lt;0.001 12.4 -1.06e-02 -0.003298\n#&gt;  train size    1.49e-01   4.28e-02  3.488   &lt;0.001 11.0  6.55e-02  0.233374\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Important: The <code>newdata</code> argument for <code>mlogit</code> models must be a \u201cbalanced\u201d data frame, that is, it must have a number of rows that is a multiple of the number of choices.</p>"},{"location":"vignettes/comparisons/","title":"Comparisons","text":"<p>In this vignette, we introduce \u201ccomparisons\u201d, defined as:</p> <p>Compare the predictions made by a model for different regressor values (e.g., college graduates vs.\u00a0others): contrasts, differences, risk ratios, odds, etc.</p> <p>The <code>comparisons()</code> function is extremely flexible, and it allows users to estimate a vast array of quantities of interest. To describe those quantities, we will break the problem up in 4 steps:</p> <ol> <li>Quantity</li> <li>Grid</li> <li>Average</li> <li>Hypothesis</li> </ol> <p>These steps can be combined and mixed and matched to define and compute many different estimands.</p>"},{"location":"vignettes/comparisons/#simple-example-titanic","title":"Simple example: Titanic","text":"<p>Consider a logistic regression model estimated using the Titanic mortality data:</p> <pre><code>library(marginaleffects)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ndat$PClass[dat$PClass == \"*\"] &lt;- NA\nmod &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n</code></pre>"},{"location":"vignettes/comparisons/#step-1-quantity","title":"Step 1: Quantity","text":"<p>The question that interests us is:</p> <p>How does the probability of survival (outcome) change if a passenger travels in 1st class vs.\u00a03rd class?</p> <p>Since we are comparing two predicted outcomes, we will use <code>comparisons()</code>. To indicate that our focal variable is <code>PClass</code> and that we are interested in the comparison between 1st and 3rd class, we will use the <code>variables</code> argument:</p> <pre><code>comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n</code></pre>"},{"location":"vignettes/comparisons/#step-2-grid","title":"Step 2: Grid","text":"<p>In GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above.</p> <p>By default, <code>comparisons()</code> will compute estimates for every row of the original dataset that was used to fit a model. There are 1313 observations in the titanic dataset. Therefore, if we just execute the code in the previous section, we will obtain 1313 estimates of the difference between the probability of survival in 3rd and 1st class:</p> <pre><code>comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.496     0.0610 -8.13  &lt; 0.001 51.0 -0.616 -0.376\n#&gt;  PClass 3rd - 1st   -0.472     0.1247 -3.79  &lt; 0.001 12.7 -0.716 -0.228\n#&gt;  PClass 3rd - 1st   -0.353     0.0641 -5.51  &lt; 0.001 24.7 -0.478 -0.227\n#&gt;  PClass 3rd - 1st   -0.493     0.0583 -8.45  &lt; 0.001 55.0 -0.607 -0.379\n#&gt;  PClass 3rd - 1st   -0.445     0.1452 -3.07  0.00216  8.9 -0.730 -0.161\n#&gt; --- 746 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#&gt;  PClass 3rd - 1st   -0.377     0.0703 -5.36  &lt; 0.001 23.5 -0.515 -0.239\n#&gt;  PClass 3rd - 1st   -0.384     0.0726 -5.30  &lt; 0.001 23.0 -0.527 -0.242\n#&gt;  PClass 3rd - 1st   -0.412     0.0821 -5.02  &lt; 0.001 20.9 -0.573 -0.251\n#&gt;  PClass 3rd - 1st   -0.399     0.0773 -5.16  &lt; 0.001 22.0 -0.550 -0.247\n#&gt;  PClass 3rd - 1st   -0.361     0.0661 -5.47  &lt; 0.001 24.4 -0.490 -0.232\n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Survived, PClass, SexCode, Age \n#&gt; Type:  response\n</code></pre> <p>Notice that the contrast between 3rd and 1st is different from row to row. This reflects the fact that, in our model, moving from 1st to 3rd would have a different effect on the predicted probability of survival for different individuals.</p> <p>We can be more specific in our query. Instead of using the empirical distribution as our \u201cgrid\u201d, we can specify exactly where we want to evaluate the comparison in the predictor space, by using the <code>newdata</code> argument and the <code>datagrid()</code> function. For example, say I am interested in:</p> <p>The effect of moving from 1st to 3rd class on the probability of survival for a 50 year old man and a 50 year old woman.</p> <p>I can type:</p> <pre><code>cmp &lt;- comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\ncmp\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n</code></pre> <p>We now know that moving from 1st to 3rd changes by -0.184 the probability of survival for 50 year old men (<code>SexCode=0</code>), and by -0.511 the probability of survival for 50 year old women (<code>SexCode=1</code>).</p>"},{"location":"vignettes/comparisons/#step-3-averaging","title":"Step 3: Averaging","text":"<p>Again, by default <code>comparisons()</code> estimates quantities for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an \u201caverage contrast\u201d:</p> <pre><code>avg_comparisons(mod,                          # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.396     0.0425 -9.3   &lt;0.001 66.0 -0.479 -0.312\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Alternatively, we could also take the average, but just of the two estimates that we computed above for the 50 year old man and 50 year old woman.</p> <pre><code>avg_comparisons(mod,                           # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\n#&gt; \n#&gt;    Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  PClass 3rd - 1st   -0.348     0.0676 -5.15   &lt;0.001 21.8 -0.48 -0.215\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Notice that this is exactly the same as the average in the estimates from the previous section, which we had stored as <code>cmp</code>:</p> <pre><code>cmp$estimate\n#&gt; [1] -0.1844289 -0.5113098\n\nmean(cmp$estimate)\n#&gt; [1] -0.3478694\n</code></pre>"},{"location":"vignettes/comparisons/#hypothesis","title":"Hypothesis","text":"<p>Finally, imagine we are interested in this question:</p> <p>Does moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?</p> <p>To answer this, we use the <code>hypothesis</code> argument:</p> <pre><code>comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1), # Step 2: Grid\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>This result maps directly onto the estimates we had above. It is the difference in the contrast for 50-men and 50-women:</p> <pre><code>diff(cmp$estimate)\n#&gt; [1] -0.3268809\n</code></pre> <p>This result can be interpreted as a \u201cdifference-in-differences\u201d: Moving from 1st to 3rd has a much larger negative effect on the probability of survival for a 50 year old woman than for a 50 year old man. This difference is statistically significant.</p> <p>We can do a similar comparison, but instead of fixing a conditional grid, we can average over subgroups of the empirical distribution, using the <code>by</code> argument:</p> <pre><code>avg_comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  by = \"SexCode\",                              # Step 3: Average\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  b1=b2    0.162     0.0845 1.91   0.0558 4.2 -0.00402  0.327\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#manual-computation","title":"Manual computation","text":"<p>Now we show how to use the base <code>R</code> <code>predict()</code> function to compute some of the same quantities as above. This exercise may be clarifying for some users.</p> <pre><code>grid_50_1_3 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"3rd\")\ngrid_50_1_1 &lt;- data.frame(Age = 50, SexCode = 1, PClass = \"1st\")\ngrid_50_0_3 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"3rd\")\ngrid_50_0_1 &lt;- data.frame(Age = 50, SexCode = 0, PClass = \"1st\")\n\n\nyhat_50_1_3 &lt;- predict(mod, newdata = grid_50_1_3, type = \"response\")\nyhat_50_1_1 &lt;- predict(mod, newdata = grid_50_1_1, type = \"response\")\nyhat_50_0_3 &lt;- predict(mod, newdata = grid_50_0_3, type = \"response\")\nyhat_50_0_1 &lt;- predict(mod, newdata = grid_50_0_1, type = \"response\")\n\n## prediction on a grid\npredictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = \"3rd\"))\n#&gt; \n#&gt;  Age SexCode PClass Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   50       1    3rd    0.446    0.661 0.6 0.235  0.679\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, Survived, Age, SexCode, PClass \n#&gt; Type:  invlink(link)\nyhat_50_1_3\n#&gt;         1 \n#&gt; 0.4463379\n\n## contrast on a grid\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1))\n#&gt; \n#&gt;    Term  Contrast Age SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % PClass\n#&gt;  PClass 3rd - 1st  50       0   -0.184     0.0535 -3.45   &lt;0.001 10.8 -0.289 -0.0796    3rd\n#&gt;  PClass 3rd - 1st  50       1   -0.511     0.1242 -4.12   &lt;0.001 14.7 -0.755 -0.2679    3rd\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Age, SexCode, predicted_lo, predicted_hi, predicted, Survived, PClass \n#&gt; Type:  response\n\nyhat_50_0_3 - yhat_50_0_1\n#&gt;          1 \n#&gt; -0.1844289\nyhat_50_1_3 - yhat_50_1_1\n#&gt;          1 \n#&gt; -0.5113098\n\n## difference-in-differences \ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1),\n  hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\n(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)\n#&gt;         1 \n#&gt; 0.3268809\n\n## average of the empirical distribution of contrasts\navg_comparisons(mod, variables = list(PClass = c(\"1st\", \"3rd\")), by = \"SexCode\")\n#&gt; \n#&gt;    Term              Contrast SexCode Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  PClass mean(3rd) - mean(1st)       0   -0.334     0.0570 -5.86   &lt;0.001 27.7 -0.446 -0.222\n#&gt;  PClass mean(3rd) - mean(1st)       1   -0.496     0.0623 -7.95   &lt;0.001 49.0 -0.618 -0.374\n#&gt; \n#&gt; Columns: term, contrast, SexCode, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n\ngrid_empirical_1_3 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_1_1 &lt;- dat |&gt; subset(SexCode == 1) |&gt; transform(PClass = \"1st\")\ngrid_empirical_0_3 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"3rd\")\ngrid_empirical_0_1 &lt;- dat |&gt; subset(SexCode == 0) |&gt; transform(PClass = \"1st\")\nyhat_empirical_0_1 &lt;- predict(mod, newdata = grid_empirical_0_1, type = \"response\")\nyhat_empirical_0_3 &lt;- predict(mod, newdata = grid_empirical_0_3, type = \"response\")\nyhat_empirical_1_1 &lt;- predict(mod, newdata = grid_empirical_1_1, type = \"response\")\nyhat_empirical_1_3 &lt;- predict(mod, newdata = grid_empirical_1_3, type = \"response\")\nmean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)\n#&gt; [1] -0.3341426\nmean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)\n#&gt; [1] -0.4956673\n</code></pre>"},{"location":"vignettes/comparisons/#predictor-types","title":"Predictor types","text":""},{"location":"vignettes/comparisons/#logical-and-factor-predictors","title":"Logical and factor predictors","text":"<p>Consider a simple model with a logical and a factor variable:</p> <pre><code>library(marginaleffects)\n\ntmp &lt;- mtcars\ntmp$am &lt;- as.logical(tmp$am)\nmod &lt;- lm(mpg ~ am + factor(cyl), tmp)\n</code></pre> <p>The <code>comparisons</code> function automatically computes contrasts for each level of the categorical variables, relative to the baseline category (<code>FALSE</code> for logicals, and the reference level for factors), while holding all other values at their observed values. The <code>avg_comparisons()</code> does the same, but then marginalizes by taking the average of unit-level estimates:</p> <pre><code>cmp &lt;- avg_comparisons(mod)\ncmp\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 % 97.5 %\n#&gt;   am  TRUE - FALSE     2.56       1.30  1.97   0.0485  4.4   0.0167   5.10\n#&gt;   cyl 6 - 4           -6.16       1.54 -4.01   &lt;0.001 14.0  -9.1661  -3.15\n#&gt;   cyl 8 - 4          -10.07       1.45 -6.93   &lt;0.001 37.8 -12.9136  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The summary printed above says that moving from the reference category <code>4</code> to the level <code>6</code> on the <code>cyl</code> factor variable is associated with a change of -6.156 in the adjusted prediction. Similarly, the contrast from <code>FALSE</code> to <code>TRUE</code> on the <code>am</code> variable is equal to 2.560.</p> <p>We can obtain different contrasts by using the <code>comparisons()</code> function. For example:</p> <pre><code>avg_comparisons(mod, variables = list(cyl = \"sequential\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0 -9.17  -3.15\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01  &lt; 0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93  &lt; 0.001 37.8 -12.91  -7.22\n#&gt;   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4    -6.16       1.54 -4.01   &lt;0.001 14.0  -9.17  -3.15\n#&gt;   cyl    8 - 4   -10.07       1.45 -6.93   &lt;0.001 37.8 -12.91  -7.22\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>For comparison, this code produces the same results using the <code>emmeans</code> package:</p> <pre><code>library(emmeans)\nemm &lt;- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast    estimate   SE df t.ratio p.value\n#&gt;  cyl6 - cyl4    -6.16 1.54 28  -4.009  0.0012\n#&gt;  cyl8 - cyl4   -10.07 1.45 28  -6.933  &lt;.0001\n#&gt;  cyl8 - cyl6    -3.91 1.47 28  -2.660  0.0331\n#&gt; \n#&gt; Results are averaged over the levels of: am \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\nemm &lt;- emmeans(mod, specs = \"am\")\ncontrast(emm, method = \"revpairwise\")\n#&gt;  contrast     estimate  SE df t.ratio p.value\n#&gt;  TRUE - FALSE     2.56 1.3 28   1.973  0.0585\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n</code></pre> <p>Note that these commands also work on for other types of models, such as GLMs, on different scales:</p> <pre><code>mod_logit &lt;- glm(am ~ factor(gear), data = mtcars, family = binomial)\n\navg_comparisons(mod_logit)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  gear    4 - 3    0.667   1.36e-01 4.9e+00   &lt;0.001 20.0   0.4  0.933\n#&gt;  gear    5 - 3    1.000   9.95e-06 1.0e+05   &lt;0.001  Inf   1.0  1.000\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod_logit, type = \"link\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  gear    4 - 3     21.3       4578 0.00464    0.996 0.0  -8951   8994\n#&gt;  gear    5 - 3     41.1       9156 0.00449    0.996 0.0 -17904  17986\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n</code></pre>"},{"location":"vignettes/comparisons/#observation-wise-categorical-marginal-effect","title":"Observation-Wise Categorical Marginal Effect","text":"<p>For categorical predictors, Scholbeck et al.\u00a02023 recommend that analysts report what they call the \u201cobservation-wise categorical marginal effects.\u201d They describe the procedure as follows:</p> <p>Recall that the common definition of categorical MEs is based on first changing all observations\u2019 value of x<sub>j</sub> to each category and then computing the difference in predictions when changing it to the reference category. However, one is often interested in prediction changes if aspects of an actual observation change. We therefore propose an observation-wise categorical ME. We first select a single reference category c<sub>h</sub>. For each observation whose feature value x<sub>j</sub>\u2004\u2260\u2004c<sub>h</sub>, we predict once with the observed value x<sub>j</sub> and once where x<sub>j</sub> has been replaced by c<sub>h</sub>.</p> <p>To achieve this with <code>marginaleffects</code>, we proceed in three simple steps:</p> <ol> <li>Use the <code>factor()</code> function to set the reference level of the     categorical variable.</li> <li>Use the <code>newdata</code> argument to take the subset of data where the     observed x<sub>j</sub> is different from the reference level we     picked in 1.</li> <li>Apply the <code>avg_comparisons()</code> with the <code>\"revreference\"</code> option.</li> </ol> <pre><code>dat &lt;- transform(mtcars, cyl = factor(cyl, levels = c(6, 4, 8)))\n\nmod &lt;- glm(vs ~ mpg * factor(cyl), data = dat, family = binomial)\n\navg_comparisons(mod,\n  variables = list(cyl = \"revreference\"),\n  newdata = subset(dat, cyl != 6))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;   cyl    6 - 4   -0.323     0.2170 -1.49    0.137   2.9 -0.748  0.103\n#&gt;   cyl    6 - 8    0.561     0.0357 15.69   &lt;0.001 181.9  0.491  0.631\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#character-predictors","title":"Character predictors","text":"<p>All functions of the <code>marginaleffects</code> package attempt to treat character predictors as factor predictors. However, using factors instead of characters when modeling is strongly encouraged, because they are much safer and faster. This is because factors hold useful information about the full list of levels, which makes them easier to track and handle internally by <code>marginaleffects</code>. Users are strongly encouraged to convert their character variables to factor before fitting their models and using <code>slopes</code> functions.</p>"},{"location":"vignettes/comparisons/#numeric-predictors","title":"Numeric predictors","text":"<p>We can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the <code>hp</code> variable by 1 unit (default) or by 5 units about the original value:</p> <pre><code>mod &lt;- lm(mpg ~ hp, data = mtcars)\n\navg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp       +1  -0.0682     0.0101 -6.74   &lt;0.001 35.9 -0.0881 -0.0484\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = 5))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp       +5   -0.341     0.0506 -6.74   &lt;0.001 35.9 -0.44 -0.242\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Compare adjusted predictions for a change in the regressor between two arbitrary values:</p> <pre><code>avg_comparisons(mod, variables = list(hp = c(90, 110)))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp 110 - 90    -1.36      0.202 -6.74   &lt;0.001 35.9 -1.76 -0.968\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Compare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:</p> <pre><code>avg_comparisons(mod, variables = list(hp = \"iqr\"))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp  Q3 - Q1     -5.7      0.845 -6.74   &lt;0.001 35.9 -7.35  -4.04\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n#&gt; \n#&gt;  Term                Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   &lt;0.001 35.9 -6.04  -3.32\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"2sd\"))\n#&gt; \n#&gt;  Term            Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp (x + sd) - (x - sd)    -9.36       1.39 -6.74   &lt;0.001 35.9 -12.1  -6.64\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n#&gt; \n#&gt;  Term  Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    hp Max - Min    -19.3       2.86 -6.74   &lt;0.001 35.9 -24.9  -13.7\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#interactions-and-cross-contrasts","title":"Interactions and Cross-Contrasts","text":"<p>In some contexts we are interested in whether the \u201ceffect\u201d of a variable changes, as a function of another variable. A very simple strategy to tackle this question is to estimate a model with a multiplicative interaction like this one:</p> <pre><code>mod &lt;- lm(mpg ~ am * factor(cyl), data = mtcars)\n</code></pre> <p>Calling <code>avg_comparisons()</code> with the <code>by</code> argument shows that the estimated comparisons differ based on <code>cyl</code>:</p> <pre><code>avg_comparisons(mod, variables = \"am\", by = \"cyl\")\n#&gt; \n#&gt;  Term          Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;    am mean(1) - mean(0)   4     5.18       2.05 2.521   0.0117 6.4  1.15   9.20\n#&gt;    am mean(1) - mean(0)   6     1.44       2.32 0.623   0.5336 0.9 -3.10   5.98\n#&gt;    am mean(1) - mean(0)   8     0.35       2.32 0.151   0.8799 0.2 -4.19   4.89\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>However, using the <code>hypothesis</code> argument for pairwise contrasts between the above comparisons reveals that the heterogeneity is not statistically significant:</p> <pre><code>avg_comparisons(mod, variables = \"am\", by = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  4 - 6     3.73       3.09 1.206    0.228 2.1 -2.33   9.80\n#&gt;  4 - 8     4.82       3.09 1.559    0.119 3.1 -1.24  10.89\n#&gt;  6 - 8     1.09       3.28 0.333    0.739 0.4 -5.33   7.51\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>In other contexts, we are interested in a \u201ccross-contrast\u201d or \u201ccross-comparisons\u201d; we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the <code>variables</code> argument, and set the <code>cross=TRUE</code>:</p> <pre><code>avg_comparisons(mod, variables = c(\"cyl\", \"am\"), cross = TRUE)\n#&gt; \n#&gt;  Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 % C: am C: cyl\n#&gt;     -2.33       2.48 -0.942  0.34596 1.5  -7.19   2.52 1 - 0  6 - 4\n#&gt;     -7.50       2.77 -2.709  0.00674 7.2 -12.93  -2.07 1 - 0  8 - 4\n#&gt; \n#&gt; Columns: term, contrast_am, contrast_cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#quantities-of-interest","title":"Quantities of interest","text":"<p>This section compares 4 quantities:</p> <ol> <li>Unit-Level Contrasts</li> <li>Average Contrast</li> <li>Contrast at the Mean</li> <li>Contrast Between Marginal Means</li> </ol> <p>The ideas discussed in this section focus on contrasts, but they carry over directly to analogous types of marginal effects.</p>"},{"location":"vignettes/comparisons/#unit-level-contrasts","title":"Unit-level contrasts","text":"<p>In models with interactions or non-linear components (e.g., link function), the value of a contrast or marginal effect can depend on the value of all the predictors in the model. As a result, contrasts and marginal effects are fundamentally unit-level quantities. The effect of a 1 unit increase in X can be different for Mary or John. Every row of a dataset has a different contrast and marginal effect.</p> <p>The <code>mtcars</code> dataset has 32 rows, so the <code>comparisons()</code> function produces 32 contrast estimates:</p> <pre><code>library(marginaleffects)\nmod &lt;- glm(vs ~ factor(gear) + mpg, family = binomial, data = mtcars)\ncmp &lt;- comparisons(mod, variables = \"mpg\")\nnrow(cmp)\n#&gt; [1] 32\n</code></pre>"},{"location":"vignettes/comparisons/#average-contrasts","title":"Average contrasts","text":"<p>By default, the <code>slopes()</code> and <code>comparisons()</code> functions compute marginal effects and contrasts for every row of the original dataset. These unit-level estimates can be of great interest, as discussed in another vignette. Nevertheless, one may want to focus on one-number summaries: the <code>avg_*()</code> functions or the <code>by</code> argument compute the \u201cAverage Marginal Effect\u201d or \u201cAverage Contrast,\u201d by taking the mean of all the unit-level estimates.</p> <pre><code>avg_comparisons(mod, variables = \"mpg\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, variables = \"mpg\", by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0609     0.0128 4.78   &lt;0.001 19.1 0.0359 0.0859\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>which are equivalent to:</p> <pre><code>mean(cmp$estimate)\n#&gt; [1] 0.06091377\n</code></pre> <p>We could also show the full distribution of contrasts across our dataset with a histogram:</p> <pre><code>library(ggplot2)\n\ncmp &lt;- comparisons(mod, variables = \"gear\")\n\nggplot(cmp, aes(estimate)) +\n    geom_histogram(bins = 30) +\n    facet_wrap(~contrast, scale = \"free_x\") +\n    labs(x = \"Distribution of unit-level contrasts\")\n</code></pre> <p></p> <p>This graph displays the effect of a change of 1 unit in the <code>gear</code> variable, for each individual in the observed data.</p>"},{"location":"vignettes/comparisons/#contrasts-at-the-mean","title":"Contrasts at the mean","text":"<p>An alternative which used to be very common but has now fallen into a bit of disfavor is to compute \u201cContrasts at the mean.\u201d The idea is to create a \u201csynthetic\u201d or \u201chypothetical\u201d individual (row of the dataset) whose characteristics are completely average. Then, we compute and report the contrast for this specific hypothetical individual.</p> <p>This can be achieved by setting <code>newdata=\"mean\"</code> or to <code>newdata=datagrid()</code>, both of which fix variables to their means or modes:</p> <pre><code>comparisons(mod, variables = \"mpg\", newdata = \"mean\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % gear  mpg\n#&gt;   mpg       +1    0.155     0.0539 2.88  0.00399 8.0 0.0495  0.261    3 20.1\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, vs, gear, mpg \n#&gt; Type:  response\n</code></pre> <p>Contrasts at the mean can differ substantially from average contrasts.</p> <p>The advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.</p>"},{"location":"vignettes/comparisons/#contrasts-between-marginal-means","title":"Contrasts between marginal means","text":"<p>Yet another type of contrast is the \u201cContrast between marginal means.\u201d This type of contrast is closely related to the \u201cContrast at the mean\u201d, with a few wrinkles. It is the default approach used by the <code>emmeans</code> package for <code>R</code>.</p> <p>Roughly speaking, the procedure is as follows:</p> <ol> <li>Create a prediction grid with one cell for each combination of     categorical predictors in the model, and all numeric variables held     at their means.</li> <li>Make adjusted predictions in each cell of the prediction grid.</li> <li>Take the average of those predictions (marginal means) for each     combination of <code>btype</code> (focal variable) and <code>resp</code> (group <code>by</code>     variable).</li> <li>Compute pairwise differences (contrasts) in marginal means across     different levels of the focal variable <code>btype</code>.</li> </ol> <p>The contrast obtained through this approach has two critical characteristics:</p> <ol> <li>It is the contrast for a synthetic individual with perfectly average     qualities on every (numeric) predictor.</li> <li>It is a weighted average of unit-level contrasts, where weights     assume a perfectly balanced dataset across every categorical     predictor.</li> </ol> <p>With respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted as if the data were perfectly balanced?</p> <p>For example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.</p> <p>If the contrasts between marginal means is really the quantity of interest, it is easy to use the <code>comparisons()</code> to estimate contrasts between marginal means. The <code>newdata</code> determines the values of the predictors at which we want to compute contrasts. We can set <code>newdata=\"marginalmeans\"</code> to emulate the <code>emmeans</code> behavior. For example, here we compute contrasts in a model with an interaction:</p> <pre><code>dat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\nmod &lt;- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)\n\navg_comparisons(\n    mod,\n    newdata = \"marginalmeans\",\n    variables = c(\"species\", \"island\"))\n#&gt; \n#&gt;     Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  island  Dream - Biscoe       -0.434      0.451 -0.962    0.336   1.6 -1.318  0.450\n#&gt;  island  Torgersen - Biscoe    0.060      0.467  0.128    0.898   0.2 -0.856  0.976\n#&gt;  species Chinstrap - Adelie   10.563      0.418 25.272   &lt;0.001 465.7  9.744 11.382\n#&gt;  species Gentoo - Adelie       5.792      0.798  7.257   &lt;0.001  41.2  4.228  7.356\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Which is equivalent to this in <code>emmeans</code>:</p> <pre><code>emm &lt;- emmeans(\n    mod,\n    specs = c(\"species\", \"island\"))\ncontrast(emm, method = \"trt.vs.ctrl1\")\n#&gt;  contrast                            estimate    SE  df t.ratio p.value\n#&gt;  Chinstrap Biscoe - Adelie Biscoe      nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Biscoe - Adelie Biscoe          5.792 0.798 331   7.257  &lt;.0001\n#&gt;  Adelie Dream - Adelie Biscoe          -0.434 0.451 331  -0.962  0.7573\n#&gt;  Chinstrap Dream - Adelie Biscoe       nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Dream - Adelie Biscoe           5.358 1.035 331   5.177  &lt;.0001\n#&gt;  Adelie Torgersen - Adelie Biscoe       0.060 0.467 331   0.128  0.9987\n#&gt;  Chinstrap Torgersen - Adelie Biscoe   nonEst    NA  NA      NA      NA\n#&gt;  Gentoo Torgersen - Adelie Biscoe       5.852 1.070 331   5.468  &lt;.0001\n#&gt; \n#&gt; Results are averaged over the levels of: sex \n#&gt; P value adjustment: dunnettx method for 5 tests\n</code></pre> <p>The <code>emmeans</code> section of the Alternative Software vignette shows further examples.</p> <p>The excellent vignette of the <code>emmeans</code> package discuss the same issues in a slightly different (and more positive) way:</p> <p>The point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.</p> <p>All this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace.</p>"},{"location":"vignettes/comparisons/#conditional-contrasts","title":"Conditional contrasts","text":"<p>Consider a model with an interaction term. What happens to the dependent variable when the <code>hp</code> variable increases by 10 units?</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt, data = mtcars)\n\nplot_comparisons(\n    mod,\n    variables = list(hp = 10),\n    condition = \"wt\")\n</code></pre> <p></p>"},{"location":"vignettes/comparisons/#transformations","title":"Transformations","text":"<p>So far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:</p> <ul> <li><code>comparison</code></li> <li><code>transform</code></li> </ul> <p>Consider the case of a model with a single predictor x. To compute average contrasts, we proceed as follows:</p> <ol> <li>Compute adjusted predictions for each row of the dataset for the     observed values x: y\u0302<sub>x</sub></li> <li>Compute adjusted predictions for each row of the dataset for the     observed values x\u2005+\u20051: y\u0302<sub>x\u2005+\u20051</sub></li> <li><code>comparison</code>: Compute unit-level contrasts by taking the difference     between (or some other function of) adjusted predictions:     y\u0302<sub>x\u2005+\u20051</sub>\u2005\u2212\u2005y\u0302<sub>x</sub></li> <li>Compute the average contrast by taking the mean of unit-level     contrasts: $1/N \\sum_{i=1}^N \\hat{y}_{x+1} - \\hat{y}_x$</li> <li><code>transform</code>: Transform the average contrast or return them as-is.</li> </ol> <p>The <code>comparison</code> argument of the <code>comparisons()</code> function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with <code>hi</code> value of x, and predictions with a <code>lo</code> value of x: <code>function(hi, lo) hi-lo</code>.</p> <p>The <code>transform</code> argument of the <code>comparisons()</code> function applies a custom transformation to the unit-level contrasts.</p> <p>The <code>transform</code> argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without <code>transform</code>.</p>"},{"location":"vignettes/comparisons/#differences","title":"Differences","text":"<p>The default contrast calculate by the <code>comparisons()</code> function is a (untransformed) difference between two adjusted predictions. For instance, to estimate the effect of a change of 1 unit, we do:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- glm(vs ~ mpg, data = mtcars, family = binomial)\n\n## construct data\n\nmtcars_minus &lt;- mtcars_plus &lt;- mtcars\nmtcars_minus$mpg &lt;- mtcars_minus$mpg - 0.5\nmtcars_plus$mpg &lt;- mtcars_plus$mpg + 0.5\n\n## adjusted predictions\nyhat_minus &lt;- predict(mod, newdata = mtcars_minus, type = \"response\")\nyhat_plus &lt;- predict(mod, newdata = mtcars_plus, type = \"response\")\n\n## unit-level contrasts\ncon &lt;- yhat_plus - yhat_minus\n\n## average contrasts\nmean(con)\n#&gt; [1] 0.05540227\n</code></pre> <p>We can use the <code>avg_comparisons()</code> function , or the <code>by</code> argument to obtain the same results:</p> <pre><code>avg_comparisons(mod)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(mod, by = TRUE)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg       +1   0.0568    0.00835 6.81   &lt;0.001 36.5 0.0404 0.0732\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#difference-in-differences-in-differences","title":"Difference-in-Differences(-in-Differences)","text":"<p>Going back to our Titanic example:</p> <pre><code>dat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n</code></pre> <p>In this case, a contrast is a difference between predicted probabilities. We can compute that contrast for different types of individuals:</p> <pre><code>comparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Contrast PClass Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % SexCode  Age\n#&gt;  SexCode    1 - 0    1st    0.483     0.0631 7.65   &lt;0.001 45.5 0.359  0.606   0.381 30.4\n#&gt;  SexCode    1 - 0    3rd    0.335     0.0634 5.29   &lt;0.001 22.9 0.211  0.459   0.381 30.4\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, predicted_lo, predicted_hi, predicted, Survived, SexCode, Age \n#&gt; Type:  response\n</code></pre> <p>One we can notice above, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. Being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?</p> <p>To answer this question, we can compute a difference-in-difference using the <code>hypothesis</code> argument (see the Hypothesis vignette for details). For example, using <code>b1</code> and <code>b2</code> to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:</p> <pre><code>comparisons(\n  titanic,\n  hypothesis = \"b1 - b2 = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  b1-b2=0    0.148     0.0894 1.65   0.0987 3.3 -0.0276  0.323\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Now, let\u2019s say we consider more types of individuals:</p> <pre><code>comparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;     Term Contrast PClass   Age Estimate Std. Error      z Pr(&gt;|z|)     S   2.5 % 97.5 % SexCode\n#&gt;  SexCode    1 - 0    1st  0.17   0.1081      0.122  0.883   0.3774   1.4 -0.1319  0.348   0.381\n#&gt;  SexCode    1 - 0    1st 71.00   0.8795      0.057 15.437   &lt;0.001 176.2  0.7679  0.991   0.381\n#&gt;  SexCode    1 - 0    3rd  0.17   0.0805      0.157  0.513   0.6081   0.7 -0.2272  0.388   0.381\n#&gt;  SexCode    1 - 0    3rd 71.00   0.4265      0.203  2.101   0.0356   4.8  0.0287  0.824   0.381\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, PClass, Age, predicted_lo, predicted_hi, predicted, Survived, SexCode \n#&gt; Type:  response\n</code></pre> <p>With these results, we could compute a triple difference:</p> <pre><code>comparisons(\n  titanic,\n  hypothesis = \"(b1 - b3) - (b2 - b4) = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#&gt; \n#&gt;               Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b1-b3)-(b2-b4)=0   -0.425      0.359 -1.19    0.236 2.1 -1.13  0.278\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/comparisons/#ratios","title":"Ratios","text":"<p>Instead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, the <code>adjrr</code> function the <code>Stata</code> software package can compute \u201cadjusted risk ratios\u201d, which are ratios of adjusted predictions. To do this in <code>R</code>, we use the <code>comparison</code> argument:</p> <pre><code>avg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>This result is the average adjusted risk ratio for an increment of 1, that is, the adjusted predictions when the <code>mpg</code> are incremented by 0.5, divided by the adjusted predictions when <code>mpg</code> is decremented by 0.5.</p> <p>The <code>comparison</code> accepts different values for common types of contrasts: \u2018difference\u2019, \u2018ratio\u2019, \u2018lnratio\u2019, \u2018ratioavg\u2019, \u2018lnratioavg\u2019, \u2018lnoravg\u2019, \u2018differenceavg\u2019. These strings are shortcuts for functions that accept two vectors of adjusted predictions and returns a single vector of contrasts. For example, these two commands yield identical results:</p> <pre><code>avg_comparisons(mod, comparison = \"ratio\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(mod, comparison = function(hi, lo) hi / lo)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.27      0.124 10.2   &lt;0.001 79.3  1.03   1.52\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>This mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:</p> <pre><code>avg_comparisons(mod, comparison = function(hi, lo) sqrt(hi) / log(lo + 10))\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1    0.275     0.0252 10.9   &lt;0.001 89.4 0.225  0.324\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The same arguments work in the plotting function <code>plot_comparisons()</code> as well, which allows us to plot various custom contrasts. Here is a comparison of Adjusted Risk Ratio and Adjusted Risk Difference in a model of the probability of survival aboard the Titanic:</p> <pre><code>library(ggplot2)\nlibrary(patchwork)\ntitanic &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ntitanic &lt;- read.csv(titanic)\nmod_titanic &lt;- glm(\n    Survived ~ Sex * PClass + Age + I(Age^2),\n    family = binomial,\n    data = titanic)\n\navg_comparisons(mod_titanic)\n#&gt; \n#&gt;    Term      Contrast Estimate Std. Error     z Pr(&gt;|z|)     S    2.5 %  97.5 %\n#&gt;  Age    +1            -0.00639    0.00107  -6.0   &lt;0.001  28.9 -0.00848 -0.0043\n#&gt;  PClass 2nd - 1st     -0.20578    0.03954  -5.2   &lt;0.001  22.3 -0.28328 -0.1283\n#&gt;  PClass 3rd - 1st     -0.40428    0.03958 -10.2   &lt;0.001  78.9 -0.48187 -0.3267\n#&gt;  Sex    male - female -0.48468    0.03004 -16.1   &lt;0.001 192.2 -0.54355 -0.4258\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np1 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survival | Age + 1) / P(Survival | Age)\")\n\np2 &lt;- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\") +\n    ylab(\"Adjusted Risk Difference\\nP(Survival | Age + 1) - P(Survival | Age)\")\n\np1 + p2\n</code></pre> <p></p> <p>By default, the standard errors around contrasts are computed using the delta method on the scale determined by the <code>type</code> argument (e.g., \u201clink\u201d or \u201cresponse\u201d). Some analysts may prefer to proceed differently. For example, in <code>Stata</code>, the <code>adjrr</code> computes adjusted risk ratios (ARR) in two steps:</p> <ol> <li>Compute the natural log of the ratio between the mean of adjusted     predictions with x\u2005+\u20051 and the mean of adjusted predictions with     x.</li> <li>Exponentiate the estimate and confidence interval bounds.</li> </ol> <p>Step 1 is easy to achieve with the <code>comparison</code> argument described above. Step 2 can be achieved with the <code>transform</code> argument:</p> <pre><code>avg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Note that we can use the <code>lnratioavg</code> shortcut instead of defining the function ourselves.</p> <p>The order of operations in previous command was:</p> <ol> <li>Compute the custom unit-level log ratios</li> <li>Exponentiate them</li> <li>Take the average using the <code>avg_comparisons()</code></li> </ol> <p>There is a very subtle difference between the procedure above and this code:</p> <pre><code>avg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.26  0.00863 6.9  1.06   1.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Since the <code>exp</code> function is now passed to the <code>transform</code> argument of the <code>comparisons()</code> function, the exponentiation is now done only after unit-level contrasts have been averaged. This is what <code>Stata</code> appears to do under the hood, and the results are slightly different.</p> <pre><code>comparisons(\n    mod,\n    comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg       +1     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>Note that equivalent results can be obtained using shortcut strings in the <code>comparison</code> argument: \u201cratio\u201d, \u201clnratio\u201d, \u201clnratioavg\u201d.</p> <pre><code>comparisons(\n    mod,\n    comparison = \"lnratioavg\",\n    transform = exp)\n#&gt; \n#&gt;  Term Contrast Estimate Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   mpg mean(+1)     1.13   &lt;0.001 31.9  1.09   1.17\n#&gt; \n#&gt; Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>All the same arguments apply to the plotting functions of the <code>marginaleffects</code> package as well. For example we can plot the Adjusted Risk Ratio in a model with a quadratic term:</p> <pre><code>library(ggplot2)\ndat_titanic &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\nmod2 &lt;- glm(Survived  ~ Age, data = dat_titanic, family = binomial)\nplot_comparisons(\n    mod2,\n    variables = list(\"Age\" = 10),\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survived = 1 | Age + 10) / P(Survived = 1 | Age)\")\n</code></pre> <p></p>"},{"location":"vignettes/comparisons/#forward-backward-centered-and-custom-differences","title":"Forward, Backward, Centered, and Custom Differences","text":"<p>By default, the <code>comparisons()</code> function computes a \u201cforward\u201d difference. For example, if we ask <code>comparisons()</code> to estimate the effect of a 10-unit change in predictor <code>x</code> on outcome <code>y</code>, <code>comparisons()</code> will compare the predicted values with <code>x</code> and <code>x+10</code>.</p> <pre><code>dat &lt;- mtcars\ndat$new_hp &lt;- 49 * (mtcars$hp - min(mtcars$hp)) / (max(mtcars$hp) - min(mtcars$hp)) + 1\nmod &lt;- lm(mpg ~ log(new_hp), data = dat)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = 10))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp      +10     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:</p> <pre><code>forward_diff &lt;- \\(x) data.frame(x, x + 10)\nbackward_diff &lt;- \\(x) data.frame(x - 10, x)\ncenter_diff &lt;- \\(x) data.frame(x - 5, x + 5)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = forward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom     -3.8      0.435 -8.74   &lt;0.001 58.6 -4.65  -2.95\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = backward_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -6.51      0.744 -8.74   &lt;0.001 58.6 -7.97  -5.05\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = center_diff))\n#&gt; \n#&gt;    Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  new_hp   custom    -4.06      0.464 -8.74   &lt;0.001 58.6 -4.97  -3.15\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Notice that the last \u201ccentered\u201d difference gives the same results as the default <code>comparisons()</code> call.</p>"},{"location":"vignettes/comparisons/#lognormal-hurdle-model","title":"Lognormal hurdle model","text":"<p>With hurdle models, we can fit two separate models simultaneously:</p> <ol> <li>A model that predicts if the outcome is zero or not zero</li> <li>If the outcome is not zero, a model that predicts what the value of     the outcome is</li> </ol> <p>We can calculate predictions and marginal effects for each of these hurdle model processes, but doing so requires some variable transformation since the stages of these models use different link functions.</p> <p>The <code>hurdle_lognormal()</code> family in <code>brms</code> uses logistic regression (with a logit link) for the hurdle part of the model and lognormal regression (where the outcome is logged before getting used in the model) for the non-hurdled part. Let\u2019s look at an example of predicting GDP per capita (which is distributed exponentially) using life expectancy. We\u2019ll add some artificial zeros so that we can work with a hurdle stage of the model.</p> <pre><code>library(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(gapminder)\n\n## Build some 0s into the GDP column\nset.seed(1234)\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap0 = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero)\n\nmod &lt;- brm(\n  bf(gdpPercap0 ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = 4, cores = 4, seed = 1234)\n</code></pre> <p>We have two different sets of coefficients here for the two different processes. The hurdle part (<code>hu</code>) uses a logit link, and the non-hurdle part (<code>mu</code>) uses an identity link. However, that\u2019s a slight misnomer\u2014a true identity link would show the coefficients on a non-logged dollar value scale. Because we\u2019re using a <code>lognormal</code> family, GDP per capita is pre-logged, so the \u201coriginal\u201d identity scale is actually logged dollars.</p> <pre><code>summary(mod)\n</code></pre> <pre><code>#&gt;  Family: hurdle_lognormal \n#&gt;   Links: mu = identity; sigma = identity; hu = logit \n#&gt; Formula: gdpPercap0 ~ lifeExp \n#&gt;          hu ~ lifeExp\n#&gt;    Data: gapminder (Number of observations: 1680) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept        3.47      0.09     3.29     3.65 1.00     4757     3378\n#&gt; hu_Intercept     3.16      0.40     2.37     3.96 1.00     2773     2679\n#&gt; lifeExp          0.08      0.00     0.08     0.08 1.00     5112     3202\n#&gt; hu_lifeExp      -0.10      0.01    -0.12    -0.08 1.00     2385     2652\n#&gt; ...\n</code></pre> <p>We can get predictions for the <code>hu</code> part of the model on the link (logit) scale:</p> <pre><code>predictions(mod, dpar = \"hu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40   -0.817 -1.03 -0.604\n#&gt;       60   -2.805 -3.06 -2.555\n#&gt;       80   -4.790 -5.34 -4.275\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\n</code></pre> <p>\u2026or on the response (percentage point) scale:</p> <pre><code>predictions(mod, dpar = \"hu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate   2.5 % 97.5 %\n#&gt;       40  0.30630 0.26231 0.3534\n#&gt;       60  0.05703 0.04466 0.0721\n#&gt;       80  0.00824 0.00478 0.0137\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n</code></pre> <p>We can also get slopes for the <code>hu</code> part of the model on the link (logit) or response (percentage point) scales:</p> <pre><code>slopes(mod, dpar = \"hu\", type = \"link\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp Estimate  2.5 %  97.5 %\n#&gt;  lifeExp      40  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      60  -0.0993 -0.116 -0.0837\n#&gt;  lifeExp      80  -0.0993 -0.116 -0.0837\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  link\n\nslopes(mod, dpar = \"hu\", type = \"response\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;     Term lifeExp  Estimate    2.5 %    97.5 %\n#&gt;  lifeExp      40 -0.021080 -0.02592 -0.016590\n#&gt;  lifeExp      60 -0.005322 -0.00615 -0.004562\n#&gt;  lifeExp      80 -0.000812 -0.00115 -0.000543\n#&gt; \n#&gt; Columns: rowid, term, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n</code></pre> <p>Working with the <code>mu</code> part of the model is trickier. Switching between <code>type = \"link\"</code> and <code>type = \"response\"</code> doesn\u2019t change anything, since the outcome is pre-logged:</p> <pre><code>predictions(mod, dpar = \"mu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  link\npredictions(mod, dpar = \"mu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40     6.61  6.54   6.69\n#&gt;       60     8.18  8.15   8.22\n#&gt;       80     9.75  9.69   9.82\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n</code></pre> <p>For predictions, we need to exponentiate the results to scale them back up to dollar amounts. We can do this by post-processing the results (e.g.\u00a0with <code>dplyr::mutate(predicted = exp(predicted))</code>), or we can use the <code>transform</code> argument in <code>predictions()</code> to pass the results to <code>exp()</code> after getting calculated:</p> <pre><code>predictions(mod, dpar = \"mu\", \n            newdata = datagrid(lifeExp = seq(40, 80, 20)),\n            transform = exp)\n#&gt; \n#&gt;  lifeExp Estimate 2.5 % 97.5 %\n#&gt;       40      744   694    801\n#&gt;       60     3581  3449   3718\n#&gt;       80    17215 16110  18410\n#&gt; \n#&gt; Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp \n#&gt; Type:  response\n</code></pre> <p>We can pass <code>transform = exp</code> to <code>plot_predictions()</code> too:</p> <pre><code>plot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"link\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       title = \"Hurdle part (hu)\",\n       subtitle = \"Logit-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"response\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       subtitle = \"Percentage point-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       title = \"Non-hurdle part (mu)\",\n       subtitle = \"Log-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  transform = exp,\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       subtitle = \"Dollar-scale predictions\")\n</code></pre> <p></p> <p>For marginal effects, we need to transform the predictions before calculating the instantaneous slopes. We also can\u2019t use the <code>slopes()</code> function directly\u2014we need to use <code>comparisons()</code> and compute the numerical derivative ourselves (i.e.\u00a0predict <code>gdpPercap</code> at <code>lifeExp</code> of 40 and 40.001 and calculate the slope between those predictions). We can use the <code>comparison</code> argument to pass the pair of predicted values to <code>exp()</code> before calculating the slopes:</p> <pre><code>## step size of the numerical derivative\neps &lt;- 0.001\n\ncomparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  # rescale the elements of the slope\n  # (exp(40.001) - exp(40)) / exp(0.001)\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps\n)\n#&gt; \n#&gt;     Term Contrast lifeExp Estimate  2.5 % 97.5 %\n#&gt;  lifeExp   +0.001      40     58.4   55.8     61\n#&gt;  lifeExp   +0.001      60    280.9  266.6    296\n#&gt;  lifeExp   +0.001      80   1349.5 1222.6   1490\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, conf.low, conf.high, lifeExp, predicted_lo, predicted_hi, predicted, tmp_idx, gdpPercap0 \n#&gt; Type:  response\n</code></pre> <p>We can visually confirm that these are the instantaneous slopes at each of these levels of life expectancy:</p> <pre><code>predictions_data &lt;- predictions(\n  mod,\n  newdata = datagrid(lifeExp = seq(30, 80, 1)),\n  dpar = \"mu\",\n  transform = exp) |&gt;\n  select(lifeExp, prediction = estimate)\n\nslopes_data &lt;- comparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps) |&gt;\n  select(lifeExp, estimate) |&gt;\n  left_join(predictions_data, by = \"lifeExp\") |&gt;\n  # Point-slope formula: (y - y1) = m(x - x1)\n  mutate(intercept = estimate * (-lifeExp) + prediction)\n\nggplot(predictions_data, aes(x = lifeExp, y = prediction)) +\n  geom_line(size = 1) + \n  geom_abline(data = slopes_data, aes(slope = estimate, intercept = intercept), \n              size = 0.5, color = \"red\") +\n  geom_point(data = slopes_data) +\n  geom_label(data = slopes_data, aes(label = paste0(\"Slope: \", round(estimate, 1))),\n             nudge_x = -1, hjust = 1) +\n  theme_minimal()\n</code></pre> <p></p> <p>We now have this in the experiments section</p>"},{"location":"vignettes/comparisons/#visual-examples","title":"Visual examples","text":"<pre><code>okabeito &lt;- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\n\nset.seed(1024)\nn &lt;- 200\nd &lt;- data.frame(\n  y = rnorm(n),\n  cond = as.factor(sample(0:1, n, TRUE)),\n  episode = as.factor(sample(0:4, n, TRUE)))\n\nmodel1 &lt;- lm(y ~ cond * episode, data = d)\n\np &lt;- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point()\n</code></pre> <pre><code>## do episodes 1 and 2 differ when `cond=0`\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = \"black\") +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n</code></pre> <pre><code>comparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0))    # grid\n#&gt; \n#&gt;     Term Contrast cond Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 % episode\n#&gt;  episode    2 - 1    0    0.241      0.396 0.609    0.542 0.9 -0.535   1.02       0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, cond, predicted_lo, predicted_hi, predicted, y, episode \n#&gt; Type:  response\n\n## do cond=0 and cond=1 differ when episode = 1\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n</code></pre> <pre><code>comparisons(model1,\n  variables = \"cond\",              # comparison of interest\n  newdata = datagrid(episode = 1)) # grid\n#&gt; \n#&gt;  Term Contrast episode Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 % cond\n#&gt;  cond    1 - 0       1    0.546      0.347 1.57    0.115 3.1 -0.134   1.23    0\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, episode, predicted_lo, predicted_hi, predicted, y, cond \n#&gt; Type:  response\n\n## Is the difference between episode 1 and 2 larger in cond=0 or cond=1? \n## try this without the `hypothesis` argument to see what we are comparing more clearly\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  annotate(\"rect\", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = \"green\") +\n  annotate(\"rect\", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = \"orange\")  +\n  ggtitle(\"Is the green box taller than the orange box?\")\n</code></pre> <pre><code>comparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0:1),  # grid\n  hypothesis = \"b1 = b2\")          # hypothesis\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  b1=b2    0.413      0.508 0.812    0.417 1.3 -0.583   1.41\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/conformal/","title":"Conformal prediction","text":"<p>execute: cache: true</p> <p>This notebook shows how to estimate conformal prediction intervals with the <code>marginaleffects</code> package for <code>R</code>.</p>"},{"location":"vignettes/conformal/#confidence-vs-prediction-intervals","title":"Confidence vs.\u00a0prediction intervals","text":"<p>The <code>predictions()</code> function from the <code>marginaleffects()</code> package can compute confidence intervals for fitted values in over 80 model classes in <code>R</code>. These intervals quantify the uncertainty about the expected value of the response. A common misunderstanding is that these confidence intervals should be calibrated to cover a certain percentage of unseen data points. This is not the case. In fact, a 95% confidence interval reported by <code>predictions()</code> will typically cover a much smaller share of out-of-sample outcomes.</p> <p>Consider this simulation where Y<sub>train</sub> and Y<sub>test</sub> are drawn from a normal distribution with mean \u03c0 and standard deviation 1. We estimate a linear model with an intercept only, and compute a 90% confidence interval for the expected value of the response:</p> <pre><code>library(marginaleffects)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(nnet)\nlibrary(MASS)\n\nset.seed(1024)\n\nsimulation &lt;- function(...) {\n    Y_train &lt;- rnorm(25, mean = pi)\n    Y_test &lt;- rnorm(25, mean = pi)\n    m &lt;- lm(Y_train ~ 1)\n    p &lt;- predictions(m, conf_level = .90)\n    out &lt;- data.table(\n        `Test set coverage` = mean(Y_test &gt;= p$conf.low &amp; Y_test &lt;= p$conf.high),\n        `True mean coverage` = pi &gt;= p$conf.low[1] &amp; pi &lt;= p$conf.high[1]\n    )\n    return(out)\n}\nresults &lt;- rbindlist(lapply(1:1000, simulation))\n\ncolMeans(results)\n</code></pre> <pre><code> Test set coverage True mean coverage \n            0.2498             0.8770\n</code></pre> <p>We see that the confidence interval around predictions covers the true mean of \u03c0 about 90% of the time, whereas coverage of individual observations in the test set is much lower.</p> <p>If we care about out of sample predictions, that is, if we want our interval to cover a specific share of the actual outcome for unobserved individuals, we must compute \u201cprediction intervals\u201d instead of \u201cconfidence intervals.\u201d How do we do this? Conformal prediction is very flexible and powerful approach.</p>"},{"location":"vignettes/conformal/#conformal-prediction_1","title":"Conformal prediction","text":"<p>In their excellent tutorial, @AngBat2022 write that conformal prediction is</p> <p>\u201ca user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions.</p> <p>These are extraordinary claims which deserve to be underlined: In principle, conformal prediction should offer well-calibrated intervals, regardless of the prediction model we use, and even if that model is misspecified.</p> <p>The main caveats are:</p> <ol> <li>The conformal prediction algorithms implemented in <code>marginaleffects</code>     are designed for exchangeable data.[1] They do not offer coverage     guarantees in contexts where exchangeability is violated, such as in     time series data, when there is spatial dependence between     observations, or when there is distribution drift between the     training and test data.</li> <li>The conformal prediction algorithms implemented in <code>marginaleffects</code>     offer marginal coverage guarantees, that is, they guarantee that a     random test point will fall within the interval with a given     probability. Below, we show an example where the prediction interval     covers the right number of test points overall, but is not well     calibrated locally, in different strata of the predictors. Different     algorithms have recently been proposed to offer class-conditional     coverage guarantees (see @Din2023 for an example).</li> <li>The width of the conformal prediction interval will typically depend     on the quality of the prediction model and of the score function.</li> <li>The score functions implemented in <code>marginaleffects</code> simply take the     residual\u2014or difference between the observed outcome and predicted     value. This means that the <code>type</code> argument must ensure that     observations and predictions are on commensurable scales (usually     <code>type=\"response\"</code> or <code>type=\"prob\"</code>).</li> </ol>"},{"location":"vignettes/conformal/#data-and-models-linear-logit-multinomial-logit","title":"Data and models: Linear, Logit, Multinomial Logit","text":"<p>Download data, split it into training and testing sets, and estimate a few different models:</p> <pre><code># download data\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/military.csv\")\n\n# create a binary outcome variable\ndat &lt;- transform(dat, officer = as.numeric(grepl(\"officer\", grade)))\n\n# train/test split\nidx &lt;- sample(seq_len(nrow(dat)), 60000)\ntest &lt;- dat[idx[1:10000], ]\ntrain &lt;- dat[idx[10001:length(idx)], ]\n\n# linear regression\nm_lm &lt;- lm(rank ~ gender * race, data = train)\np_lm &lt;- predictions(m_lm, newdata = train)\n\n# logit regression\nm_glm &lt;- glm(officer ~ gender * race, data = train, family = binomial)\np_glm &lt;- predictions(m_glm, newdata = train)\n\n# multinomial logit regression\nm_mult &lt;- multinom(branch ~ gender * race, data = train, trace = FALSE)\np_mult &lt;- predictions(m_mult, newdata = train)\n</code></pre> <p>For LM and GLM models, <code>predictions()</code> returns a data frame with one prediction for each row of the original data. This data frame includes confidence intervals:</p> <pre><code>p_glm\n</code></pre> <pre><code> Estimate Pr(&gt;|z|)     S  2.5 % 97.5 %\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.1006   &lt;0.001 668.6 0.0885  0.114\n--- 49990 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.1775   &lt;0.001   Inf 0.1733  0.182\n   0.0859   &lt;0.001   Inf 0.0793  0.093\n   0.2080   &lt;0.001 837.6 0.1956  0.221\nColumns: rowid, estimate, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer \nType:  invlink(link)\n</code></pre> <p>For multinomial models, <code>predictions()</code> returns a data frame with one prediction for each row and for each outcome level. We can see the predicted probabilities of each outcome level for the first observation in the original data:</p> <pre><code>p_mult |&gt; subset(rowid == 1)\n</code></pre> <pre><code>        Group Estimate Std. Error    z Pr(&gt;|z|)     S CI low CI high\n air force       0.181    0.00479 37.8   &lt;0.001   Inf  0.171   0.190\n army            0.473    0.00621 76.2   &lt;0.001   Inf  0.461   0.485\n marine corps    0.101    0.00376 27.0   &lt;0.001 530.9  0.094   0.109\n navy            0.245    0.00535 45.7   &lt;0.001   Inf  0.234   0.255\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer\n</code></pre>"},{"location":"vignettes/conformal/#conformal-predictions-with-inferences","title":"Conformal predictions with <code>inferences()</code>","text":"<p>In the \u201cBootstrap and Simultation\u201d vignette, we saw that the <code>inferences()</code> function can be used to compute confidence intervals for any <code>marginaleffects</code> package estimates. The workflow is simple:</p> <ol> <li>Generate estimates with <code>predictions()</code>.</li> <li>Pass the resulting object to <code>inferences()</code>, along with arguments to     specify how to perform inference to obtain uncertainty estimates.</li> </ol> <p><code>inferences()</code> supports two strategies for conformal prediction: split or CV+ [@AngBat2022,@Din2023]. The former is faster but less efficient. In the rest of this vignette, we illustrate how to use this same workflow to compute conformal prediction intervals.</p>"},{"location":"vignettes/conformal/#cross-validation","title":"Cross-validation +","text":"<p>The <code>p_lm</code>, <code>p_glm</code>, and <code>p_mult</code> objects are <code>predictions</code> objects. They contain the point predictions and confidence intervals for each observation in the training set. Now, we use the <code>inferences()</code> function to compute predictions and prediction intervals for every observation in the test set:</p> <pre><code>p &lt;- predictions(m_lm, conf_level = .9) |&gt; \n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\np\n</code></pre> <pre><code> Estimate Std. Error   z Pr(&gt;|z|)   S 5.0 % 95.0 % Pred. 5.0 % Pred. 95.0 %\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     5.86     0.0283 207   &lt;0.001 Inf  5.82   5.91        3.00         8.73\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n--- 9990 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.51     0.0220 296   &lt;0.001 Inf  6.48   6.55        3.65         9.38\n     6.15     0.0100 612   &lt;0.001 Inf  6.13   6.17        3.29         9.01\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, rownames, grade, branch, gender, race, hisp, rank, officer, pred.low, pred.high \nType:  response\n</code></pre> <p>The prediction interval is expected to cover the (known) true value about 90% of the time:</p> <pre><code>mean(p$rank &lt;= p$pred.high &amp; p$rank &gt;= p$pred.low)\n</code></pre> <pre><code>[1] 0.9082\n</code></pre> <p>The coverage also seems adequate (about 80%) for the logit model:</p> <pre><code>p &lt;- predictions(m_glm, conf_level = .8) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\nmean(p$officer &lt;= p$pred.high &amp; p$officer &gt;= p$pred.low)\n</code></pre> <pre><code>[1] 0.7998\n</code></pre> <p>When the outcome is categorical, we use <code>conformal_score=\"softmax\"</code>. With this argument, <code>inferences()</code> generates \u201cconformal prediction sets,\u201d that is, sets of possible outcome classes with coverage guarantees. <code>inferences()</code> returns a list column of sets for each observation. On average, those sets should cover the true value about 70% of the time:</p> <pre><code>p &lt;- predictions(m_mult, conf_level = .7) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_score = \"softmax\",\n        conformal_test = test)\nhead(p)\n</code></pre> <pre><code>    branch                        Pred Set\n army                 air force, army     \n navy      air force, army     , navy     \n navy                 air force, army     \n army                           army, navy\n air force            air force, army     \n army                           army, navy\n\nColumns: rowid, branch, pred.set\n</code></pre> <p>For example, for the first observation in the dataset, the conformal prediction is {air force, army} and the true value is army. The conformal prediction set thus covers the true value. The coverage rate is:</p> <pre><code>mean(sapply(seq_len(nrow(p)), \\(i) p$branch[i] %in% p$pred.set[[i]]))\n</code></pre> <pre><code>[1] 0.6928\n</code></pre>"},{"location":"vignettes/conformal/#split-conformal-prediction","title":"Split conformal prediction","text":"<p>For split conformal prediction, we must first split the training set into a training and a calibration set (see @AngBat2022). Then, we pass the calibration set to the <code>inferences()</code> function:</p> <pre><code>calibration &lt;- train[1:1000,]\ntrain &lt;- train[1001:nrow(train),]\np &lt;- predictions(m_lm, conf_level = .9) |&gt;\n    inferences(\n        method = \"conformal_split\",\n        conformal_calibration = calibration,\n        conformal_test = test)\nmean(p$rank &lt;= p$pred.high &amp; p$rank &gt;= p$pred.low)\n</code></pre> <pre><code>[1] 0.9112\n</code></pre>"},{"location":"vignettes/conformal/#misspecification","title":"Misspecification","text":""},{"location":"vignettes/conformal/#polynomials","title":"Polynomials","text":"<p>As noted above, the conformal prediction interval should be valid even if the model is misspecified. To illustrate this, we generate data from a linear model with polynomials, but estimate a linear model without polynomials. Then, we plot the results and compute the coverage of the prediction interval:</p> <pre><code>N &lt;- 1000\nX &lt;- rnorm(N * 2)\ndat &lt;- data.frame(\n    X = X,\n    Y = X + X^2 + X^3 + rnorm(N * 2))\ntrain &lt;- dat[1:N,]\ntest &lt;- dat[(N + 1):nrow(dat),]\n\nm &lt;- lm(Y ~ X, data = train)\np &lt;- predictions(m) |&gt;\n    inferences(\n        R = 5,\n        method = \"conformal_cv+\",\n        conformal_test = test)\n\nmean(p$Y &lt;= p$pred.high &amp; p$Y &gt;= p$pred.low)\n</code></pre> <pre><code>[1] 0.953\n</code></pre> <pre><code>ggplot(p, aes(X, Y)) +\n    geom_point(alpha = .1) +\n    geom_ribbon(aes(X, ymin = pred.low, ymax = pred.high), alpha = .2, fill = \"#F0E442\") +\n    geom_ribbon(aes(X, ymin = conf.low, ymax = conf.high), alpha = .4, fill = \"#D55E00\") +\n    theme_bw() +\n    labs(\n        title = \"Confidence and prediction intervals for a misspecified linear model\",\n        subtitle = sprintf(\n            \"Confidence coverage (orange): %.2f%%; Prediction coverage (yellow): %.2f%%.\",\n            mean(p$Y &lt;= p$conf.high &amp; p$Y &gt;= p$conf.low),\n            mean(p$Y &lt;= p$pred.high &amp; p$Y &gt;= p$pred.low)))\n</code></pre> <p></p> <p>This example is interesting, because it shows that the prediction interval has adquate marginal coverage. However, the intervals are not necessarily well calibrated \u201clocally\u201d, in different strata of X. In the figure above, our model is misspecified, so we make more mistakes in the tails, where predictions are bad. In contrast, the interval catches more observations in the middle of the distribution, which ensures that the overall error rate is adequate.</p>"},{"location":"vignettes/conformal/#poisson-vs-negative-binomial","title":"Poisson vs Negative Binomial","text":"<p>Here is a second example of model misspecification. We generate data from a negative binomial model, but estimate a Poisson model. Nevertheless, the conformal prediction interval has good coverage:</p> <pre><code>n &lt;- 10000\nX &lt;- rnorm(n)\neta &lt;- -1 + 2*X\nmu &lt;- exp(eta)\nY &lt;- rnegbin(n, mu = mu, theta = 1)\ndat &lt;- data.frame(X = X, Y = Y)\ntrain &lt;- dat[1:5000,]\ntest &lt;- dat[5001:nrow(dat),]\n\nmod &lt;- glm(Y ~ X, data = train, family = poisson)\n\np &lt;- predictions(mod, conf_level = .9) |&gt;\n    inferences(\n        method = \"conformal_cv+\",\n        R = 10,\n        conformal_test = test)\n\nmean(p$Y &gt;= p$pred.low &amp; p$Y &lt;= p$pred.high)\n</code></pre> <pre><code>[1] 0.8968\n</code></pre> <p>[1] The usual \u201cindependent and identically distributed\u201d assumption is a special case of exchangeability.</p>"},{"location":"vignettes/core/","title":"Core","text":"<p>The following chapters are on the core aspects of the <code>marginaleffect</code> package.</p>"},{"location":"vignettes/elasticity/","title":"Elasticity","text":"<p>In some contexts, it is useful to interpret the results of a regression model in terms of elasticity or semi-elasticity. One strategy to achieve that is to estimate a log-log or a semilog model, where the left and/or right-hand side variables are logged. Another approach is to note that $\\frac{\\partial ln(x)}{\\partial x}=\\frac{1}{x}$, and to post-process the marginal effects to transform them into elasticities or semi-elasticities.</p> <p>For example, say we estimate a linear model of this form:</p> <p>y\u2004=\u2004\u03b2<sub>0</sub>\u2005+\u2005\u03b2<sub>1</sub>x<sub>1</sub>\u2005+\u2005\u03b2<sub>2</sub>x<sub>2</sub>\u2005+\u2005\u03b5</p> <p>Let y\u0302 be the adjusted prediction made by the model for some combination of covariates x<sub>1</sub> and x<sub>2</sub>. The slope with respect to x<sub>1</sub> (or \u201cmarginal effect\u201d) is:</p> <p>$$\\frac{\\partial \\hat{y}}{\\partial x_1}$$</p> <p>We can estimate the \u201ceyex\u201d, \u201ceydx\u201d, and \u201cdyex\u201d (semi-)elasticities with respect to x<sub>1</sub> as follows:</p> <p>$$\\eta_1=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{x_1}{\\hat{y}}$$ $$\\eta_2=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{1}{\\hat{y}}$$ $$\\eta_3=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot x_1$$</p> <p>with interpretations roughly as follows:</p> <ol> <li>A percentage point increase in x<sub>1</sub> is associated to a     \u03b7<sub>1</sub> percentage points increase in y.</li> <li>A unit increase in x<sub>1</sub> is associated to a     \u03b7<sub>2</sub> percentage points increase in y.</li> <li>A percentage point increase in x<sub>1</sub> is associated to a     \u03b7<sub>3</sub> units increase in y.</li> </ol> <p>For further intuition, consider the ratio of change in y to change in x: $\\frac{\\Delta y}{\\Delta x}$. We can turn this ratio into a ratio between relative changes by dividing both the numerator and the denominator: $\\frac{\\frac{\\Delta y}{y}}{\\frac{\\Delta x}{x}}$. This is of course linked to the expression for the \u03b7<sub>1</sub> elasticity above.</p> <p>With the <code>marginaleffects</code> package, these quantities are easy to compute:</p> <pre><code>library(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\navg_slopes(mod)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp  -0.0318    0.00903 -3.52   &lt;0.001 11.2 -0.0495 -0.0141\n#&gt;    wt  -3.8778    0.63276 -6.13   &lt;0.001 30.1 -5.1180 -2.6377\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"eyex\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;    hp    eY/eX   -0.285     0.0855 -3.34   &lt;0.001 10.2 -0.453 -0.118\n#&gt;    wt    eY/eX   -0.746     0.1418 -5.26   &lt;0.001 22.7 -1.024 -0.468\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"eydx\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S    2.5 %    97.5 %\n#&gt;    hp    eY/dX -0.00173   0.000502 -3.46   &lt;0.001 10.8 -0.00272 -0.000751\n#&gt;    wt    eY/dX -0.21165   0.037850 -5.59   &lt;0.001 25.4 -0.28583 -0.137462\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, slope = \"dyex\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;    hp    dY/eX    -4.66       1.32 -3.52   &lt;0.001 11.2  -7.26  -2.06\n#&gt;    wt    dY/eX   -12.48       2.04 -6.13   &lt;0.001 30.1 -16.47  -8.49\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/equivalence/","title":"Equivalence Tests","text":"<p>In many contexts, analysts are less interested in rejecting a null hypothesis, and more interested in testing whether an estimate is \u201cinferior\u201d, \u201csuperior\u201d, or \u201cequivalent\u201d to a given threshold or interval. For example, medical researchers may wish to determine if the estimated effect of a new treatment is larger than the effect of prior treatments, or larger than some threshold of \u201cclinical significance.\u201d Alternatively, researchers may wish to support a claim that an estimated parameter is \u201cequivalent to\u201d or \u201cnot meaningfully different from\u201d a null hypothesis.</p> <p>To answer these questions, we can use non-inferiority, non-superiority, or equivalence tests like the two-one-sided test (TOST). This article gives a primer and tutorial on TOST:</p> <p>Lakens D, Scheel AM, Isager PM. Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science. 2018;1(2):259-269. doi:10.1177/2515245918770963</p> <p>The <code>hypotheses()</code> function of the <code>marginaleffects</code> package includes an <code>equivalence</code> argument which allows users to apply these tests to any of the quantities generated by the package, as well as to arbitrary functions of a model\u2019s parameters. To illustrate, we begin by estimating a simple linear regression model:</p> <pre><code>library(marginaleffects)\nmod &lt;- lm(mpg ~ hp + factor(gear), data = mtcars)\n</code></pre> <p>The rest of this section considers several quantities estimated by <code>marginaleffects</code>.</p>"},{"location":"vignettes/equivalence/#predictions","title":"Predictions","text":"<p>Consider a single prediction, where all predictors are held at their median or mode:</p> <pre><code>p &lt;- predictions(mod, newdata = \"median\")\np\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp gear\n#&gt;      19.7          1 19.6   &lt;0.001 281.3  17.7   21.6 123    3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear \n#&gt; Type:  response\n</code></pre> <p>Now we specify an equivalence interval (or \u201cregion\u201d) for predictions between 17 and 18:</p> <pre><code>hypotheses(p, equivalence = c(17, 18))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)  hp gear\n#&gt;      19.7          1 19.6   &lt;0.001 281.3  17.7   21.6      0.951    0.00404     0.951 123    3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n</code></pre> <p>The results allow us to draw three conclusions:</p> <ol> <li>The p value for the non-inferiority test is 0.0040. This suggests     that we can reject the null hypothesis that the parameter is below     17.</li> <li>The p value for the non-superiority test is 0.9508. This suggests     that we cannot reject the null hypothesis that the parameter     (19.6589) is above 18.</li> <li>The p value for the equivalence test is 0.9508. This suggests that     we cannot reject the hypothesis that the parameter falls outside     the equivalence interval.</li> </ol>"},{"location":"vignettes/equivalence/#model-coefficients","title":"Model coefficients","text":"<p>The <code>hypotheses</code> function also allows users to conduct equivalence, non-inferiority, and non-superiority tests for model coefficients, and for arbitrary functions of model coefficients.</p> <p>Our estimate of the 4th coefficient in the model is:</p> <pre><code>coef(mod)[4]\n#&gt; factor(gear)5 \n#&gt;      6.574763\n</code></pre> <p>We can test if this parameter is likely to fall in the [5,7] interval by:</p> <pre><code>hypotheses(mod, equivalence = c(5, 7))[4, ]\n#&gt; \n#&gt;           Term Estimate Std. Error z Pr(&gt;|z|)    S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  factor(gear)5     6.57       1.64 4   &lt;0.001 14.0  3.36   9.79      0.398      0.169     0.398\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n</code></pre> <p>The p value is 0.3979, so we cannot reject the hypothesis that the factor(gear)5 parameter falls outside the [5,7] interval.</p>"},{"location":"vignettes/equivalence/#slopes","title":"Slopes","text":"<p>The same syntax can be used to conduct tests for all the quantities produced by the <code>marginaleffects</code> package. For example, imagine that, for substantive or theoretical reasons, an average slope between -0.1 and 0.1 is uninteresting. We can conduct an equivalence test to check if this is the case:</p> <pre><code>avg_slopes(mod, variables = \"hp\", equivalence = c(-.1, .1))\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;    hp  -0.0669      0.011 -6.05   &lt;0.001 29.4 -0.0885 -0.0452     &lt;0.001    0.00135   0.00135\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n</code></pre> <p>The p value is 0.0013, which suggests that we can reject the hypothesis that the parameter falls outside the region of \u201csubstantive equivalence\u201d that we have defined by the interval.</p>"},{"location":"vignettes/equivalence/#difference-between-comparisons-contrasts","title":"Difference between comparisons (contrasts)","text":"<p>Consider a model with a multiplicative interaction:</p> <pre><code>int &lt;- lm(mpg ~ hp * factor(gear), data = mtcars)\n</code></pre> <p>The average contrast for a change of 1 unit in <code>hp</code> differs based on the value of <code>gear</code>:</p> <pre><code>avg_comparisons(int, variables = \"hp\", by = \"gear\")\n#&gt; \n#&gt;  Term Contrast gear Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    hp mean(+1)    3  -0.0522     0.0146 -3.59   &lt;0.001 11.6 -0.0808 -0.0237\n#&gt;    hp mean(+1)    4  -0.1792     0.0303 -5.92   &lt;0.001 28.2 -0.2385 -0.1199\n#&gt;    hp mean(+1)    5  -0.0583     0.0126 -4.61   &lt;0.001 17.9 -0.0830 -0.0335\n#&gt; \n#&gt; Columns: term, contrast, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>Are these contrasts different from one another? Let\u2019s look at the pairwise differences between them:</p> <pre><code>avg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;  3 - 4  0.12695     0.0336  3.781   &lt;0.001 12.6  0.0611  0.1928\n#&gt;  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438\n#&gt;  4 - 5 -0.12092     0.0328 -3.688   &lt;0.001 12.1 -0.1852 -0.0567\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We consider that these pairwise comparisons are \u201cequivalent to zero\u201d when they fall in the [-.1, .1] interval:</p> <pre><code>avg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-.1, .1))\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)    S   2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  3 - 4  0.12695     0.0336  3.781   &lt;0.001 12.6  0.0611  0.1928      0.789     &lt;0.001     0.789\n#&gt;  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438     &lt;0.001     &lt;0.001    &lt;0.001\n#&gt;  4 - 5 -0.12092     0.0328 -3.688   &lt;0.001 12.1 -0.1852 -0.0567     &lt;0.001      0.738     0.738\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n#&gt; Type:  response\n</code></pre> <p>The <code>p (Equiv)</code> column shows that the difference between the average contrasts when <code>gear</code> is 3 and <code>gear</code> is 5 can be said to be equivalent to the specified interval. However, there are good reasons to think that the other two pairwise comparisons may fall outside the interval.</p>"},{"location":"vignettes/equivalence/#marginal-means-and-emmeans","title":"Marginal means and <code>emmeans</code>","text":"<p>This example shows the equivalence between results produced by the <code>emmeans</code> package and the <code>marginal_means()</code> function:</p> <pre><code>library(emmeans)\n\nmod &lt;- lm(log(conc) ~ source + factor(percent), data = pigs)\n\n## {emmeans}\nemmeans(mod, specs = \"source\") |&gt;\n    pairs() |&gt;\n    test(df = Inf,\n         null = 0,\n         delta = log(1.25),\n         side = \"equivalence\",\n         adjust = \"none\")\n#&gt;  contrast    estimate     SE  df z.ratio p.value\n#&gt;  fish - soy    -0.273 0.0529 Inf   0.937  0.8257\n#&gt;  fish - skim   -0.402 0.0542 Inf   3.308  0.9995\n#&gt;  soy - skim    -0.130 0.0530 Inf  -1.765  0.0388\n#&gt; \n#&gt; Results are averaged over the levels of: percent \n#&gt; Degrees-of-freedom method: user-specified \n#&gt; Results are given on the log (not the response) scale. \n#&gt; Statistics are tests of equivalence with a threshold of 0.22314 \n#&gt; P values are left-tailed\n\n## {marginaleffects}\nmarginal_means(\n    mod,\n    variables = \"source\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-log(1.25), log(1.25)))\n#&gt; \n#&gt;         Term   Mean Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 % p (NonSup) p (NonInf) p (Equiv)\n#&gt;  fish - soy  -0.273     0.0529 -5.15   &lt;0.001 21.9 -0.377 -0.1690     &lt;0.001     0.8257    0.8257\n#&gt;  fish - skim -0.402     0.0542 -7.43   &lt;0.001 43.0 -0.508 -0.2961     &lt;0.001     0.9995    0.9995\n#&gt;  soy - skim  -0.130     0.0530 -2.44   0.0146  6.1 -0.233 -0.0255     &lt;0.001     0.0388    0.0388\n#&gt; \n#&gt; Results averaged over levels of: percent, source \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, p.value.equiv, p.value.noninf, p.value.nonsup, statistic.noninf, statistic.nonsup \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/equivalence/#t-test","title":"t-test","text":"<p>Now we show that the results produced by <code>hypotheses()</code> are identical to the results produced by the <code>equivalence</code> package in the case of a simple t-test:</p> <pre><code>library(equivalence)\n\nset.seed(1024)\n\n## simulate data data\nN &lt;- 20\ndat &lt;- data.frame(\n    y = rnorm(N),\n    x = sample(c(rep(0, N / 2), rep(1, N / 2)), N))\n\n## fit model\nmod &lt;- lm(y ~ x, data = dat)\n\n## test with the {equivalence} package\ne &lt;- tost(\n    x = dat$y[dat$x == 0],\n    y = dat$y[dat$x == 1],\n    epsilon = 10)\ne\n#&gt; \n#&gt;  Welch Two Sample TOST\n#&gt; \n#&gt; data:  dat$y[dat$x == 0] and dat$y[dat$x == 1]\n#&gt; df = 17.607\n#&gt; sample estimates:\n#&gt;  mean of x  mean of y \n#&gt; -0.3788551 -0.2724594 \n#&gt; \n#&gt; Epsilon: 10 \n#&gt; 95 percent two one-sided confidence interval (TOST interval):\n#&gt;  -1.058539  0.845747\n#&gt; Null hypothesis of statistical difference is: rejected \n#&gt; TOST p-value: 4.248528e-13\n\n## test with {marginaleffects} package\nh &lt;- hypotheses(mod, equivalence = c(-10, 10), df = e$parameter)[2, ]\nh\n#&gt; \n#&gt;  Term Estimate Std. Error     t Pr(&gt;|t|)   S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)   Df\n#&gt;     x    0.106      0.548 0.194    0.848 0.2 -1.05   1.26     &lt;0.001     &lt;0.001    &lt;0.001 17.6\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, df, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\n# identical p values\nh$p.value.equiv |&gt; as.vector()\n#&gt; [1] 4.248528e-13\n\ne$tost.p.value |&gt; as.vector()\n#&gt; [1] 4.248528e-13\n</code></pre>"},{"location":"vignettes/extensions/","title":"Extensions","text":"<p>This vignette shows how to add support for new models and add new functionality for supported models.</p>"},{"location":"vignettes/extensions/#support-a-new-model-type","title":"Support a new model type","text":"<p>It is very easy to add support for new models in <code>marginaleffects</code>. All we need is to set a global option and define 4 very simple functions.</p> <p>If you add support for a class of models produced by a CRAN package, please consider submitting your code for inclusion in the package: https://github.com/vincentarelbundock/marginaleffects</p> <p>If you add support for a class of models produced by a package hosted elsewhere than CRAN, you can submit it for inclusion in the unsupported user-submitted library of extensions: Currently</p> <ul> <li><code>countreg</code>     package.     Thanks to Olivier Beaumais.</li> <li><code>censreg</code>     package.     Thanks to Oleg Komashko.</li> </ul> <p>The rest of this section illustrates how to add support for a very simple lm_manual model.</p>"},{"location":"vignettes/extensions/#fit-function","title":"Fit function","text":"<p>To begin, we define a function which fits a model. Normally, this function will be supplied by a modeling package published on CRAN. Here, we create a function called <code>lm_manual()</code>, which estimates a linear regression model using simple linear algebra operates:</p> <pre><code>lm_manual &lt;- function(f, data, ...) {\n    # design matrix\n    X &lt;- model.matrix(f, data = data)\n    # response matrix\n    Y &lt;- data[[as.character(f[2])]]\n    # coefficients\n    b &lt;- solve(crossprod(X)) %*% crossprod(X, Y)\n    Yhat &lt;- X %*% b\n    # variance-covariance matrix\n    e &lt;- Y - Yhat\n    df &lt;- nrow(X) - ncol(X)\n    s2 &lt;- sum(e^2) / df\n    V &lt;- s2 * solve(crossprod(X))\n    # model object\n    out &lt;- list(\n        d = data,\n        f = f,\n        X = X,\n        Y = Y,\n        V = V,\n        b = b)\n    # class name: lm_manual\n    class(out) &lt;- c(\"lm_manual\", \"list\")\n    return(out)\n}\n</code></pre> <p>Important: The custom fit function must assign a new class name to the object it returns. In the example above, the model is assigned to be of class <code>lm_manual</code> (see the penultimate line of code in the function).</p> <p>Our new function replicates the results of <code>lm()</code>:</p> <pre><code>model &lt;- lm_manual(mpg ~ hp + drat, data = mtcars)\nmodel$b\n#&gt;                    [,1]\n#&gt; (Intercept) 10.78986122\n#&gt; hp          -0.05178665\n#&gt; drat         4.69815776\n\nmodel_lm &lt;- lm(mpg ~ hp + drat, data = mtcars)\ncoef(model_lm)\n#&gt; (Intercept)          hp        drat \n#&gt; 10.78986122 -0.05178665  4.69815776\n</code></pre>"},{"location":"vignettes/extensions/#marginaleffects-extension","title":"<code>marginaleffects</code> extension","text":"<p>To extend support in <code>marginaleffects</code>, the first step is to tell the package that our new class is supported. We do this by defining a global option:</p> <pre><code>library(marginaleffects)\n\noptions(\"marginaleffects_model_classes\" = \"lm_manual\")\n</code></pre> <p>Then, we define 4 methods:</p> <ol> <li><code>get_coef()</code><ul> <li>Mandatory arguments: <code>model</code>, <code>...</code></li> <li>Returns: named vector of parameters (coefficients).</li> </ul> </li> <li><code>set_coef()</code><ul> <li>Mandatory arguments: <code>model</code>, <code>coefs</code> (named vector of     coefficients), <code>...</code></li> <li>Returns: A new model object in which the original coefficients     were replaced by the new vector.</li> <li>Example</li> </ul> </li> <li><code>get_vcov()</code><ul> <li>Mandatory arguments: <code>model</code>, <code>...</code>.</li> <li>Optional arguments: <code>vcov</code></li> <li>Returns: A named square variance-covariance matrix.</li> </ul> </li> <li><code>get_predict()</code><ul> <li>Mandatory arguments: <code>model</code>, <code>newdata</code> (data frame), <code>...</code></li> <li>Option arguments: <code>type</code> and other model-specific arguments.</li> <li>Returns: A data frame with two columns: a unique <code>rowid</code> and a     column of <code>estimate</code> values.</li> </ul> </li> </ol> <p>Note that each of these methods will be named with the suffix <code>.lm_manual</code> to indicate that they should be used whenever <code>marginaleffects</code> needs to process an object of class <code>lm_manual</code>.</p> <pre><code>get_coef.lm_manual &lt;- function(model, ...) {\n    b &lt;- model$b\n    b &lt;- setNames(as.vector(b), row.names(b))\n    return(b)\n}\n\nset_coef.lm_manual &lt;- function(model, coefs, ...) {\n    out &lt;- model\n    out$b &lt;- coefs\n    return(out)\n}\n\nget_vcov.lm_manual &lt;- function(model, ...) {\n    return(model$V)\n}\n\nget_predict.lm_manual &lt;- function(model, newdata, ...) {\n    newX &lt;- model.matrix(model$f, data = newdata)\n    Yhat &lt;- newX %*% model$b\n    out &lt;- data.frame(\n        rowid = seq_len(nrow(Yhat)),\n        estimate = as.vector(Yhat))\n    return(out)\n}\n</code></pre> <p>The methods we just defined work as expected:</p> <pre><code>get_coef(model)\n#&gt; (Intercept)          hp        drat \n#&gt; 10.78986122 -0.05178665  4.69815776\n\nget_vcov(model)\n#&gt;             (Intercept)            hp         drat\n#&gt; (Intercept) 25.78356135 -3.054007e-02 -5.836030687\n#&gt; hp          -0.03054007  8.635615e-05  0.004969385\n#&gt; drat        -5.83603069  4.969385e-03  1.419990359\n\nget_predict(model, newdata = head(mtcars))\n#&gt;   rowid estimate\n#&gt; 1     1 23.41614\n#&gt; 2     2 23.41614\n#&gt; 3     3 24.06161\n#&gt; 4     4 19.56366\n#&gt; 5     5 16.52639\n#&gt; 6     6 18.31918\n</code></pre> <p>Now we can use the <code>avg_slopes</code> function:</p> <pre><code>avg_slopes(model, newdata = mtcars, variables = c(\"hp\", \"drat\"))\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 %  97.5 %\n#&gt;  drat   4.6982    1.19166  3.94   &lt;0.001 13.6  2.36  7.0338\n#&gt;  hp    -0.0518    0.00929 -5.57   &lt;0.001 25.2 -0.07 -0.0336\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(model, newdata = mtcars) |&gt; head()\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  mpg cyl disp  hp drat   wt qsec vs am gear carb\n#&gt;      23.4      0.671 34.9   &lt;0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n#&gt;      23.4      0.671 34.9   &lt;0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n#&gt;      24.1      0.720 33.4   &lt;0.001 810.2  22.6   25.5 22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n#&gt;      19.6      0.999 19.6   &lt;0.001 281.4  17.6   21.5 21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\n#&gt;      16.5      0.735 22.5   &lt;0.001 369.1  15.1   18.0 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\n#&gt;      18.3      1.343 13.6   &lt;0.001 138.3  15.7   21.0 18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb \n#&gt; Type:  response\n</code></pre> <p>Note that, for custom model, we typically have to supply values for the <code>newdata</code> and <code>variables</code> arguments explicitly.</p>"},{"location":"vignettes/extensions/#modify-or-extend-supported-models","title":"Modify or extend supported models","text":"<p>Let\u2019s say you want to estimate a model using the <code>mclogit::mblogit</code> function. That package is already supported by <code>marginaleffects</code>, but you want to use a <code>type</code> (scale) of predictions that is not currently supported: a \u201ccentered link scale.\u201d</p> <p>To achieve this, we would need to override the <code>get_predict.mblogit()</code> method. However, it can be unsafe to reassign methods supplied by a package that we loaded with <code>library</code>. To be safe, we assign a new model class to our object (\u201ccustomclass\u201d) which will inherit from <code>mblogit</code>. Then, we define a <code>get_predict.customclass</code> method to make our new kinds of predictions.</p> <p>Load libraries, estimate a model:</p> <pre><code>library(mclogit)\nlibrary(data.table)\n\nmodel &lt;- mblogit(\n    factor(gear) ~ am + mpg,\n    data = mtcars,\n    trace = FALSE)\n</code></pre> <p>Tell <code>marginaleffects</code> that we are adding support for a new class model models, and assign a new inherited class name to a duplicate of the model object:</p> <pre><code>options(\"marginaleffects_model_classes\" = \"customclass\")\n\nmodel_custom &lt;- model\n\nclass(model_custom) &lt;- c(\"customclass\", class(model))\n</code></pre> <p>Define a new <code>get_predict.customclass</code> method. We use the default <code>predict()</code> function to obtain predictions. Since this is a multinomial model, <code>predict()</code> returns a matrix of predictions with one column per level of the response variable.</p> <p>Our new <code>get_predict.customclass</code> method takes this matrix of predictions, modifies it, and reshapes it to return a data frame with three columns: <code>rowid</code>, <code>group</code>, and <code>estimate</code>:</p> <pre><code>get_predict.customclass &lt;- function(model, newdata, ...) {\n    out &lt;- predict(model, newdata = newdata, type = \"link\")\n    out &lt;- cbind(0, out)\n    colnames(out)[1] &lt;- dimnames(model$D)[[1]][[1]]\n    out &lt;- out - rowMeans(out)\n    out &lt;- as.data.frame(out)\n    out$rowid &lt;- seq_len(nrow(out))\n    out &lt;- data.table(out)\n    out &lt;- melt(\n        out,\n        id.vars = \"rowid\",\n        value.name = \"estimate\",\n        variable.name = \"group\")\n}\n</code></pre> <p>Finally, we can call any <code>slopes</code> function and obtain results. Notice that our object of class <code>customclass</code> now produces different results than the default <code>mblogit</code> object:</p> <pre><code>avg_predictions(model)\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;      3    0.469     0.0444 10.56  &lt; 0.001 84.2 0.382  0.556\n#&gt;      4    0.375     0.0670  5.60  &lt; 0.001 25.5 0.244  0.506\n#&gt;      5    0.156     0.0501  3.12  0.00183  9.1 0.058  0.255\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_predictions(model_custom)\n#&gt; \n#&gt;  Group Estimate Std. Error         z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;      3    -1.42       2525 -0.000561    1.000 0.0 -4950   4947\n#&gt;      4     6.36       1779  0.003578    0.997 0.0 -3480   3493\n#&gt;      5    -4.95       3074 -0.001609    0.999 0.0 -6030   6020\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/faq/","title":"FAQ","text":""},{"location":"vignettes/faq/#stack-overflow-questions","title":"Stack Overflow questions","text":"<ul> <li><code>plot_predictions()</code> over a range of unobserved     values</li> <li>Plot the marginal effects from a <code>plm</code> package     model</li> <li>Models with demeaned, polynomials, or transformed     variables</li> <li><code>nlme::lme</code> problem with character     predictors</li> </ul>"},{"location":"vignettes/faq/#calling-marginaleffects-in-functions-loops-environments-or-after-re-assigning-variables","title":"Calling <code>marginaleffects</code> in functions, loops, environments, or after re-assigning variables","text":"<p>Functions from the <code>marginaleffects</code> package can sometimes fail when they are called inside a function, loop, or other environments. To see why, it is important to know that <code>marginaleffects</code> often needs to operate on the original data that was used to fit the model. To extract this original data, we use the <code>get_data()</code> function from the <code>insight</code> package.</p> <p>In most cases, <code>get_data()</code> can extract the data which is stored inside the model object created by the modeling package. However, some modeling packages do not save the original data in the model object (in order to save memory). In those cases, <code>get_data()</code> will parse the call to find the name of the data object, and will search for that data object in the global environment. When users fit models in a different environment (e.g., function calls), <code>get_data()</code> may not be able to retrieve the original data.</p> <p>A related problem can arise if users fit a model, but then assign a new value to the variable that used to store the dataset.</p> <p>Recommendations:</p> <ol> <li>Supply your dataset explicitly to the <code>newdata</code> argument of <code>slopes</code>     functions.</li> <li>Avoid assigning a new value to a variable that you use to store a     dataset for model fitting.</li> </ol>"},{"location":"vignettes/functions/","title":"Functions","text":""},{"location":"vignettes/functions/#functions","title":"Functions","text":"Goal Function Predictions <code>predictions()</code> <code>avg_predictions()</code> <code>plot_predictions()</code> Comparisons: Difference, Ratio, Odds, Lift, etc. <code>comparisons()</code> <code>avg_comparisons()</code> <code>plot_comparisons()</code> Slopes <code>slopes()</code> <code>avg_slopes()</code> <code>plot_slopes()</code> Marginal Means <code>marginal_means()</code> Grids <code>datagrid()</code> <code>datagridcf()</code> Hypothesis &amp; Equivalence <code>hypotheses()</code> Bayes, Bootstrap, Simulation <code>posterior_draws()</code> <code>inferences()</code>"},{"location":"vignettes/gam/","title":"GAM","text":""},{"location":"vignettes/gam/#estimate-a-generalized-additive-model","title":"Estimate a Generalized Additive Model","text":"<p>We will estimate a GAM model using the <code>mgcv</code> package and the <code>simdat</code> dataset distributed with the <code>itsadug</code> package:</p> <pre><code>library(marginaleffects)\nlibrary(itsadug)\nlibrary(mgcv)\n\nsimdat$Subject &lt;- as.factor(simdat$Subject)\n\ndim(simdat)\n#&gt; [1] 75600     6\nhead(simdat)\n#&gt;    Group      Time Trial Condition Subject         Y\n#&gt; 1 Adults   0.00000   -10        -1     a01 0.7554469\n#&gt; 2 Adults  20.20202   -10        -1     a01 2.7834759\n#&gt; 3 Adults  40.40404   -10        -1     a01 1.9696963\n#&gt; 4 Adults  60.60606   -10        -1     a01 0.6814298\n#&gt; 5 Adults  80.80808   -10        -1     a01 1.6939195\n#&gt; 6 Adults 101.01010   -10        -1     a01 2.3651969\n</code></pre> <p>Fit a model with a random effect and group-time smooths:</p> <pre><code>model &lt;- bam(Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\"),\n             data = simdat)\n\nsummary(model)\n#&gt; \n#&gt; Family: gaussian \n#&gt; Link function: identity \n#&gt; \n#&gt; Formula:\n#&gt; Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\")\n#&gt; \n#&gt; Parametric coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept)   2.0574     0.6903   2.980  0.00288 **\n#&gt; GroupAdults   3.1265     0.9763   3.202  0.00136 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Approximate significance of smooth terms:\n#&gt;                         edf Ref.df    F p-value    \n#&gt; s(Time):GroupChildren  8.26  8.850 3649  &lt;2e-16 ***\n#&gt; s(Time):GroupAdults    8.66  8.966 6730  &lt;2e-16 ***\n#&gt; s(Subject)            33.94 34.000  569  &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; R-sq.(adj) =  0.609   Deviance explained =   61%\n#&gt; fREML = 2.3795e+05  Scale est. = 31.601    n = 75600\n</code></pre>"},{"location":"vignettes/gam/#adjusted-predictions-predictions-and-plot_predictions","title":"Adjusted Predictions: <code>predictions()</code> and <code>plot_predictions()</code>","text":"<p>Compute adjusted predictions for each observed combination of regressor in the dataset used to fit the model. This gives us a dataset with the same number of rows as the original data, but new columns with predicted values and uncertainty estimates:</p> <pre><code>pred &lt;- predictions(model)\ndim(pred)\n#&gt; [1] 75600    12\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    -1.874      0.199 -9.41   &lt;0.001 67.4 -2.2643 -1.4834\n#&gt;    -1.346      0.182 -7.41   &lt;0.001 42.8 -1.7025 -0.9901\n#&gt;    -0.819      0.167 -4.90   &lt;0.001 20.0 -1.1467 -0.4916\n#&gt;    -0.293      0.156 -1.88   0.0605  4.0 -0.5988  0.0129\n#&gt;     0.231      0.149  1.55   0.1204  3.1 -0.0606  0.5232\n#&gt;     0.753      0.146  5.17   &lt;0.001 22.0  0.4675  1.0379\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject \n#&gt; Type:  response\n</code></pre> <p>We can easily plot adjusted predictions for different values of a regressor using the <code>plot_predictions()</code> function:</p> <pre><code>plot_predictions(model, condition = \"Time\")\n</code></pre> <p></p>"},{"location":"vignettes/gam/#marginal-effects-slopes-and-plot_slopes","title":"Marginal Effects: <code>slopes()</code> and <code>plot_slopes()</code>","text":"<p>Marginal effects are slopes of the prediction equation. They are an observation-level quantity. The <code>slopes()</code> function produces a dataset with the same number of rows as the original data, but with new columns for the slop and uncertainty estimates:</p> <pre><code>mfx &lt;- slopes(model, variables = \"Time\")\nhead(mfx)\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  Time   0.0261    0.00137 19.1   &lt;0.001 267.8 0.0234 0.0288\n#&gt;  Time   0.0261    0.00136 19.2   &lt;0.001 270.4 0.0234 0.0288\n#&gt;  Time   0.0261    0.00133 19.5   &lt;0.001 279.9 0.0235 0.0287\n#&gt;  Time   0.0260    0.00128 20.3   &lt;0.001 301.4 0.0235 0.0285\n#&gt;  Time   0.0259    0.00120 21.6   &lt;0.001 339.8 0.0235 0.0282\n#&gt;  Time   0.0257    0.00109 23.5   &lt;0.001 404.3 0.0236 0.0279\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, Y, Group, Time, Subject \n#&gt; Type:  response\n</code></pre> <p>We can plot marginal effects for different values of a regressor using the <code>plot_slopes()</code> function. This next plot shows the slope of the prediction equation, that is, the slope of the previous plot, at every value of the <code>Time</code> variable.</p> <pre><code>plot_slopes(model, variables = \"Time\", condition = \"Time\")\n</code></pre> <p></p> <p>The marginal effects in this plot can be interpreted as measuring the change in <code>Y</code> that is associated with a small increase in <code>Time</code>, for different baseline values of <code>Time</code>.</p>"},{"location":"vignettes/gam/#excluding-terms","title":"Excluding terms","text":"<p>The <code>predict()</code> method of the <code>mgcv</code> package allows users to \u201cexclude\u201d some smoothing terms, using the <code>exclude</code> argument. You can pass the same argument to any function in the <code>marginaleffects</code> package:</p> <pre><code>predictions(model, newdata = \"mean\", exclude = \"s(Subject)\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  Group Time Subject\n#&gt;      11.7      0.695 16.9   &lt;0.001 210.8  10.4   13.1 Adults 1000     a01\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject \n#&gt; Type:  response\n</code></pre> <p>See the documentation in <code>?mgcv:::predict.bam</code> for details.</p>"},{"location":"vignettes/gcomputation/","title":"G-Computation","text":"<p>This vignette has 3 goals:</p> <ol> <li>Give a concise introduction to the idea of \u201cParametric g-Formula\u201d</li> <li>Highlight the equivalence between one form of g-estimation and the     \u201cAverage Contrasts\u201d computed     by <code>marginaleffects</code></li> <li>Show how to obtain estimates, standard errors, and confidence     intervals via the Parametric g-Formula, using a single line of     <code>marginaleffects</code> code. This is convenient because, typically,     analysts have to construct counterfactual datasets manually and must     bootstrap their estimates.</li> </ol> <p>The \u201cParametric g-Formula\u201d is often used for causal inference in observational data.</p> <p>The explanations and illustrations that follow draw heavily on Chapter 13 of this excellent book (free copy available online):</p> <p>Hern\u00e1n MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman &amp; Hall/CRC.</p>"},{"location":"vignettes/gcomputation/#what-is-the-parametric-g-formula","title":"What is the parametric g-formula?","text":"<p>The parametric g-formula is a method of standardization which can be used to address confounding problems in causal inference with observational data. It relies on the same identification assumptions as Inverse Probability Weighting (IPW), but uses different modeling assumptions. Whereas IPW models the treatment equation, standardization models the mean outcome equation. As Hern\u00e1n and Robins note:</p> <p>\u201cBoth IP weighting and standardization are estimators of the g-formula, a general method for causal inference first described in 1986. \u2026 We say that standardization is a\u201dplug-in g-formula estimator\u201d because it simply replaces the conditional mean outcome in the g-formula by its estimates. When, like in Chapter 13, those estimates come from parametric models, we refer to the method as the parametric g-formula.\u201d</p>"},{"location":"vignettes/gcomputation/#how-does-it-work","title":"How does it work?","text":"<p>Imagine a causal model like this:</p> <p></p> <p>We want to estimate the effect of a binary treatment X on outcome Y, but there is a confounding variable W. We can use standardization with the parametric g-formula to handle this. Roughly speaking, the procedure is as follows:</p> <ol> <li>Use the observed data to fit a regression model with Y as outcome,     X as treatment, and W as control variable (with perhaps some     polynomials and/or interactions if there are multiple control     variables).</li> <li>Create a new dataset exactly identical to the original data, but     where X\u2004=\u20041 in every row.</li> <li>Create a new dataset exactly identical to the original data, but     where X\u2004=\u20040 in every row.</li> <li>Use the model from Step 1 to compute adjusted predictions in the two     counterfactual datasets from Steps 2 and 3.</li> <li>The quantity of interest is the difference between the means of     adjusted predictions in the two counterfactual datasets.</li> </ol> <p>This is equivalent to computing an \u201cAverage Contrast\u201d, in which the value of X moves from 0 to 1. Thanks to this equivalence, we can apply the parametric g-formula method using a single line of code in <code>marginaleffects</code>, and obtain delta method standard errors automatically.</p>"},{"location":"vignettes/gcomputation/#example-with-real-world-data","title":"Example with real-world data","text":"<p>Let\u2019s illustrate this method by replicating an example from Chapter 13 of Hern\u00e1n and Robins. The data come from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). The outcome is <code>wt82_71</code>, a measure of weight gain. The treatment is <code>qsmk</code>, a binary measure of smoking cessation. There are many confounders.</p> <p>Step 1 is to fit a regression model of the outcome on the treatment and control variables:</p> <pre><code>library(boot)\nlibrary(marginaleffects)\n\nf &lt;- wt82_71 ~ qsmk + sex + race + age + I(age * age) + factor(education) +\n     smokeintensity + I(smokeintensity * smokeintensity) + smokeyrs +\n     I(smokeyrs * smokeyrs) + factor(exercise) + factor(active) + wt71 +\n     I(wt71 * wt71) + I(qsmk * smokeintensity)\n\nurl &lt;- \"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/nhefs.csv\"\nnhefs &lt;- read.csv(url)\nnhefs &lt;- na.omit(nhefs[, all.vars(f)])\n\nfit &lt;- glm(f, data = nhefs)\n</code></pre> <p>Steps 2 and 3 require us to replicate the full dataset by setting the <code>qsmk</code> treatment to counterfactual values. We can do this automatically by calling <code>comparisons()</code>.</p>"},{"location":"vignettes/gcomputation/#tldr","title":"TLDR","text":"<p>These simple commands do everything we need to apply the parametric g-formula:</p> <pre><code>avg_comparisons(fit, variables = list(qsmk = 0:1))\n</code></pre> <pre><code> Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>The rest of the vignette walks through the process in a bit more detail and compares to replication code from Hern\u00e1n and Robins.</p>"},{"location":"vignettes/gcomputation/#adjusted-predictions","title":"Adjusted Predictions","text":"<p>We can compute average predictions in the original data, and average predictions in the two counterfactual datasets like this:</p> <pre><code>## average predicted outcome in the original data\np &lt;- predictions(fit)\nmean(p$estimate)\n</code></pre> <pre><code>[1] 2.6383\n</code></pre> <pre><code>## average predicted outcome in the two counterfactual datasets\np &lt;- predictions(fit, newdata = datagrid(qsmk = 0:1, grid_type = \"counterfactual\"))\naggregate(estimate ~ qsmk, data = p, FUN = mean)\n</code></pre> <pre><code>  qsmk estimate\n1    0 1.756213\n2    1 5.273587\n</code></pre> <p>In the <code>R</code> code that accompanies their book, Hern\u00e1n and Robins compute the same quantities manually, as follows:</p> <pre><code>## create a dataset with 3 copies of each subject\nnhefs$interv &lt;- -1 # 1st copy: equal to original one\n\ninterv0 &lt;- nhefs # 2nd copy: treatment set to 0, outcome to missing\ninterv0$interv &lt;- 0\ninterv0$qsmk &lt;- 0\ninterv0$wt82_71 &lt;- NA\n\ninterv1 &lt;- nhefs # 3rd copy: treatment set to 1, outcome to missing\ninterv1$interv &lt;- 1\ninterv1$qsmk &lt;- 1\ninterv1$wt82_71 &lt;- NA\n\nonesample &lt;- rbind(nhefs, interv0, interv1) # combining datasets\n\n## linear model to estimate mean outcome conditional on treatment and confounders\n## parameters are estimated using original observations only (nhefs)\n## parameter estimates are used to predict mean outcome for observations with \n## treatment set to 0 (interv=0) and to 1 (interv=1)\n\nstd &lt;- glm(f, data = onesample)\nonesample$predicted_meanY &lt;- predict(std, onesample)\n\n## estimate mean outcome in each of the groups interv=0, and interv=1\n## this mean outcome is a weighted average of the mean outcomes in each combination \n## of values of treatment and confounders, that is, the standardized outcome\nmean(onesample[which(onesample$interv == -1), ]$predicted_meanY)\n</code></pre> <pre><code>[1] 2.6383\n</code></pre> <pre><code>mean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n</code></pre> <pre><code>[1] 1.756213\n</code></pre> <pre><code>mean(onesample[which(onesample$interv == 1), ]$predicted_meanY)\n</code></pre> <pre><code>[1] 5.273587\n</code></pre> <p>It may be useful to note that the <code>datagrid()</code> function provided by <code>marginaleffects</code> can create counterfactual datasets automatically. This is equivalent to the <code>onesample</code> dataset:</p> <pre><code>nd &lt;- datagrid(\n    model = fit,\n    qsmk = c(0, 1),\n    grid_type = \"counterfactual\")\n</code></pre>"},{"location":"vignettes/gcomputation/#contrast","title":"Contrast","text":"<p>Now we want to compute the treatment effect with the parametric g-formula, which is the difference in average predicted outcomes in the two counterfactual datasets. This is equivalent to taking the average contrast with the <code>comparisons()</code> function. There are three important things to note in the command that follows:</p> <ul> <li>The <code>variables</code> argument is used to indicate that we want to     estimate a \u201ccontrast\u201d between adjusted predictions when <code>qsmk</code> is     equal to 1 or 0.</li> <li><code>comparisons()</code> automatically produces estimates of uncertainty.</li> </ul> <pre><code>avg_comparisons(std, variables = list(qsmk = 0:1))\n</code></pre> <pre><code> Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>Under the hood, <code>comparisons()</code> did exactly what we described in the g-formula steps above:</p> <p>We can obtain the same result by manually computing the quantities, using the replication code from Hern\u00e1n and Robins:</p> <pre><code>mean(onesample[which(onesample$interv == 1), ]$predicted_meanY) -\nmean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n</code></pre> <pre><code>[1] 3.517374\n</code></pre> <p>Although manual computation is simple, it does not provide uncertainty estimates. In contrast, <code>comparisons()</code> has already computed the standard error and confidence interval using the delta method.</p> <p>Instead of the delta method, most analysts will rely on bootstrapping. For example, the replication code from Hern\u00e1n and Robins does this:</p> <pre><code>## function to calculate difference in means\nstandardization &lt;- function(data, indices) {\n    # create a dataset with 3 copies of each subject\n    d &lt;- data[indices, ] # 1st copy: equal to original one`\n    d$interv &lt;- -1\n    d0 &lt;- d # 2nd copy: treatment set to 0, outcome to missing\n    d0$interv &lt;- 0\n    d0$qsmk &lt;- 0\n    d0$wt82_71 &lt;- NA\n    d1 &lt;- d # 3rd copy: treatment set to 1, outcome to missing\n    d1$interv &lt;- 1\n    d1$qsmk &lt;- 1\n    d1$wt82_71 &lt;- NA\n    d.onesample &lt;- rbind(d, d0, d1) # combining datasets\n\n    # linear model to estimate mean outcome conditional on treatment and confounders\n    # parameters are estimated using original observations only (interv= -1)\n    # parameter estimates are used to predict mean outcome for observations with set\n    # treatment (interv=0 and interv=1)\n    fit &lt;- glm(f, data = d.onesample)\n\n    d.onesample$predicted_meanY &lt;- predict(fit, d.onesample)\n\n    # estimate mean outcome in each of the groups interv=-1, interv=0, and interv=1\n    return(mean(d.onesample$predicted_meanY[d.onesample$interv == 1]) -\n           mean(d.onesample$predicted_meanY[d.onesample$interv == 0]))\n}\n\n## bootstrap\nresults &lt;- boot(data = nhefs, statistic = standardization, R = 1000)\n\n## generating confidence intervals\nse &lt;- sd(results$t[, 1])\nmeant0 &lt;- results$t0\nll &lt;- meant0 - qnorm(0.975) * se\nul &lt;- meant0 + qnorm(0.975) * se\n\nbootstrap &lt;- data.frame(\n    \" \" = \"Treatment - No Treatment\",\n    estimate = meant0,\n    std.error = se,\n    conf.low = ll,\n    conf.high = ul,\n    check.names = FALSE)\nbootstrap\n</code></pre> <pre><code>                           estimate std.error conf.low conf.high\n1 Treatment - No Treatment 3.517374 0.4721559 2.591966  4.442783\n</code></pre> <p>The results are close to those that we obtained with <code>comparisons()</code>, but the confidence interval differs slightly because of the difference between bootstrapping and the delta method.</p> <pre><code>avg_comparisons(fit, variables = list(qsmk = 0:1))\n</code></pre> <pre><code> Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   &lt;0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre>"},{"location":"vignettes/get_started/","title":"Get Started","text":""},{"location":"vignettes/get_started/#installation","title":"Installation","text":"<p>Install the latest CRAN release:</p> <pre><code>install.packages(\"marginaleffects\")\n</code></pre> <p>Install the development version:</p> <pre><code>install.packages(\n    c(\"marginaleffects\", \"insight\"),\n    repos = c(\"https://vincentarelbundock.r-universe.dev\", \"https://easystats.r-universe.dev\"))\n</code></pre> <p>Restart <code>R</code> completely before moving on.</p>"},{"location":"vignettes/get_started/#estimands-predictions-comparisons-and-slopes","title":"Estimands: Predictions, Comparisons, and Slopes","text":"<p>The <code>marginaleffects</code> package allows <code>R</code> users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons, and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, \u201cmarginal means\u201d, which is a special case of averaged predictions. <code>marginaleffects</code> can also average (or \u201cmarginalize\u201d) unit-level (or \u201cconditional\u201d) estimates of all those quantities, and conduct hypothesis tests on them.</p> <p>Predictions:</p> <p>The outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. <code>predictions()</code>, <code>avg_predictions()</code>, <code>plot_predictions()</code>.</p> <p>Comparisons:</p> <p>Compare the predictions made by a model for different regressor values (e.g., college graduates vs.\u00a0others): contrasts, differences, risk ratios, odds, etc. <code>comparisons()</code>, <code>avg_comparisons()</code>, <code>plot_comparisons()</code>.</p> <p>Slopes:</p> <p>Partial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. <code>slopes()</code>, <code>avg_slopes()</code>, <code>plot_slopes()</code>.</p> <p>Marginal Means:</p> <p>Predictions of a model, averaged across a \u201creference grid\u201d of categorical predictors. <code>marginalmeans()</code>.</p> <p>Hypothesis and Equivalence Tests:</p> <p>Hypothesis and equivalence tests can be conducted on linear or non-linear functions of model coefficients, or on any of the quantities computed by the <code>marginaleffects</code> packages (predictions, slopes, comparisons, marginal means, etc.). Uncertainy estimates can be obtained via the delta method (with or without robust standard errors), bootstrap, or simulation.</p> <p>Predictions, comparisons, and slopes are fundamentally unit-level (or \u201cconditional\u201d) quantities. Except in the simplest linear case, estimates will typically vary based on the values of all the regressors in a model. Each of the observations in a dataset is thus associated with its own prediction, comparison, and slope estimates. Below, we will see that it can be useful to marginalize (or \u201caverage over\u201d) unit-level estimates to report an \u201caverage prediction\u201d, \u201caverage comparison\u201d, or \u201caverage slope\u201d.</p> <p>One ambiguous aspect of the definitions above is that the word \u201cmarginal\u201d comes up in two different and opposite ways:</p> <ol> <li>In \u201cmarginal effects,\u201d we refer to the effect of a tiny (marginal)     change in the regressor on the outcome. This is a slope, or     derivative.</li> <li>In \u201cmarginal means,\u201d we refer to the process of marginalizing across     rows of a prediction grid. This is an average, or integral.</li> </ol> <p>On this website and in this package, we reserve the expression \u201cmarginal effect\u201d to mean a \u201cslope\u201d or \u201cpartial derivative\u201d.</p> <p>The <code>marginaleffects</code> package includes functions to estimate, average, plot, and summarize all of the estimands described above. The objects produced by <code>marginaleffects</code> are \u201ctidy\u201d: they produce simple data frames in \u201clong\u201d format. They are also \u201cstandards-compliant\u201d and work seamlessly with standard functions like <code>summary()</code>, <code>head()</code>, <code>tidy()</code>, and <code>glance()</code>, as well with external packages like <code>modelsummary</code> or <code>ggplot2</code>.</p> <p>We now apply <code>marginaleffects</code> functions to compute each of the estimands described above. First, we fit a linear regression model with multiplicative interactions:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp * wt * am, data = mtcars)\n</code></pre> <p>Then, we call the <code>predictions()</code> function. As noted above, predictions are unit-level estimates, so there is one specific prediction per observation. By default, the <code>predictions()</code> function makes one prediction per observation in the dataset that was used to fit the original model. Since <code>mtcars</code> has 32 rows, the <code>predictions()</code> outcome also has 32 rows:</p> <pre><code>pre &lt;- predictions(mod)\n\nnrow(mtcars)\n</code></pre> <pre><code>[1] 32\n</code></pre> <pre><code>nrow(pre)\n</code></pre> <pre><code>[1] 32\n</code></pre> <pre><code>pre\n</code></pre> <pre><code> Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     22.5      0.884 25.44   &lt;0.001 471.7  20.8   24.2\n     20.8      1.194 17.42   &lt;0.001 223.3  18.5   23.1\n     25.3      0.709 35.66   &lt;0.001 922.7  23.9   26.7\n     20.3      0.704 28.75   &lt;0.001 601.5  18.9   21.6\n     17.0      0.712 23.88   &lt;0.001 416.2  15.6   18.4\n--- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n     29.6      1.874 15.80   &lt;0.001 184.3  25.9   33.3\n     15.9      1.311 12.13   &lt;0.001 110.0  13.3   18.5\n     19.4      1.145 16.95   &lt;0.001 211.6  17.2   21.7\n     14.8      2.017  7.33   &lt;0.001  42.0  10.8   18.7\n     21.5      1.072 20.02   &lt;0.001 293.8  19.4   23.6\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response\n</code></pre> <p>Now, we use the <code>comparisons()</code> function to compute the difference in predicted outcome when each of the predictors is incremented by 1 unit (one predictor at a time, holding all others constant). Once again, comparisons are unit-level quantities. And since there are 3 predictors in the model and our data has 32 rows, we obtain 96 comparisons:</p> <pre><code>cmp &lt;- comparisons(mod)\n\nnrow(cmp)\n</code></pre> <pre><code>[1] 96\n</code></pre> <pre><code>cmp\n</code></pre> <pre><code> Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   am    1 - 0    0.325       1.68  0.193   0.8467 0.2  -2.97  3.622\n   am    1 - 0   -0.544       1.57 -0.347   0.7287 0.5  -3.62  2.530\n   am    1 - 0    1.201       2.35  0.511   0.6090 0.7  -3.40  5.802\n   am    1 - 0   -1.703       1.87 -0.912   0.3618 1.5  -5.36  1.957\n   am    1 - 0   -0.615       1.68 -0.366   0.7146 0.5  -3.91  2.680\n--- 86 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   wt    +1      -6.518       1.88 -3.462   &lt;0.001 10.9 -10.21 -2.828\n   wt    +1      -1.653       3.74 -0.442   0.6588  0.6  -8.99  5.683\n   wt    +1      -4.520       2.47 -1.830   0.0672  3.9  -9.36  0.321\n   wt    +1       0.635       4.89  0.130   0.8966  0.2  -8.95 10.216\n   wt    +1      -6.647       1.86 -3.572   &lt;0.001 11.5 -10.29 -2.999\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response\n</code></pre> <p>The <code>comparisons()</code> function allows customized queries. For example, what happens to the predicted outcome when the <code>hp</code> variable increases from 100 to 120?</p> <pre><code>comparisons(mod, variables = list(hp = c(120, 100)))\n</code></pre> <pre><code> Term  Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 %  97.5 %\n   hp 120 - 100   -0.738      0.370 -1.995  0.04607 4.4 -1.463 -0.0129\n   hp 120 - 100   -0.574      0.313 -1.836  0.06640 3.9 -1.186  0.0388\n   hp 120 - 100   -0.931      0.452 -2.062  0.03922 4.7 -1.817 -0.0460\n   hp 120 - 100   -0.845      0.266 -3.182  0.00146 9.4 -1.366 -0.3248\n   hp 120 - 100   -0.780      0.268 -2.909  0.00362 8.1 -1.306 -0.2547\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp 120 - 100   -1.451      0.705 -2.058  0.03958 4.7 -2.834 -0.0692\n   hp 120 - 100   -0.384      0.270 -1.422  0.15498 2.7 -0.912  0.1451\n   hp 120 - 100   -0.641      0.334 -1.918  0.05513 4.2 -1.297  0.0141\n   hp 120 - 100   -0.126      0.272 -0.463  0.64360 0.6 -0.659  0.4075\n   hp 120 - 100   -0.635      0.332 -1.911  0.05598 4.2 -1.286  0.0162\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response\n</code></pre> <p>What happens to the predicted outcome when the <code>wt</code> variable increases by 1 standard deviation about its mean?</p> <pre><code>comparisons(mod, variables = list(hp = \"sd\"))\n</code></pre> <pre><code> Term                Contrast Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 %  97.5 %\n   hp (x + sd/2) - (x - sd/2)   -2.530      1.269 -1.995  0.04607 4.4 -5.02 -0.0441\n   hp (x + sd/2) - (x - sd/2)   -1.967      1.072 -1.836  0.06640 3.9 -4.07  0.1332\n   hp (x + sd/2) - (x - sd/2)   -3.193      1.549 -2.062  0.03922 4.7 -6.23 -0.1578\n   hp (x + sd/2) - (x - sd/2)   -2.898      0.911 -3.182  0.00146 9.4 -4.68 -1.1133\n   hp (x + sd/2) - (x - sd/2)   -2.675      0.919 -2.909  0.00362 8.1 -4.48 -0.8731\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp (x + sd/2) - (x - sd/2)   -4.976      2.418 -2.058  0.03958 4.7 -9.71 -0.2373\n   hp (x + sd/2) - (x - sd/2)   -1.315      0.925 -1.422  0.15498 2.7 -3.13  0.4974\n   hp (x + sd/2) - (x - sd/2)   -2.199      1.147 -1.918  0.05513 4.2 -4.45  0.0483\n   hp (x + sd/2) - (x - sd/2)   -0.432      0.933 -0.463  0.64360 0.6 -2.26  1.3970\n   hp (x + sd/2) - (x - sd/2)   -2.177      1.139 -1.911  0.05598 4.2 -4.41  0.0556\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response\n</code></pre> <p>The <code>comparisons()</code> function also allows users to specify arbitrary functions of predictions, with the <code>comparison</code> argument. For example, what is the average ratio between predicted Miles per Gallon after an increase of 50 units in Horsepower?</p> <pre><code>comparisons(\n  mod,\n  variables = list(hp = 50),\n  comparison = \"ratioavg\")\n</code></pre> <pre><code> Term  Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n   hp mean(+50)    0.905     0.0319 28.4   &lt;0.001 586.8 0.843  0.968\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response\n</code></pre> <p>See the Comparisons vignette for detailed explanations and more options.</p> <p>The <code>slopes()</code> function allows us to compute the partial derivative of the outcome equation with respect to each of the predictors. Once again, we obtain a data frame with 96 rows:</p> <pre><code>mfx &lt;- slopes(mod)\n\nnrow(mfx)\n</code></pre> <pre><code>[1] 96\n</code></pre> <pre><code>mfx\n</code></pre> <pre><code> Term Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   am    1 - 0    0.325       1.68  0.193   0.8467 0.2  -2.97  3.622\n   am    1 - 0   -0.544       1.57 -0.347   0.7287 0.5  -3.62  2.530\n   am    1 - 0    1.201       2.35  0.511   0.6090 0.7  -3.40  5.802\n   am    1 - 0   -1.703       1.87 -0.912   0.3618 1.5  -5.36  1.957\n   am    1 - 0   -0.615       1.68 -0.366   0.7146 0.5  -3.91  2.680\n--- 86 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n   wt    dY/dX   -6.518       1.88 -3.462   &lt;0.001 10.9 -10.21 -2.828\n   wt    dY/dX   -1.653       3.74 -0.442   0.6588  0.6  -8.99  5.682\n   wt    dY/dX   -4.520       2.47 -1.830   0.0673  3.9  -9.36  0.322\n   wt    dY/dX    0.635       4.89  0.130   0.8966  0.2  -8.95 10.215\n   wt    dY/dX   -6.647       1.86 -3.572   &lt;0.001 11.5 -10.29 -3.000\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am \nType:  response\n</code></pre>"},{"location":"vignettes/get_started/#grid","title":"Grid","text":"<p>Predictions, comparisons, and slopes are typically \u201cconditional\u201d quantities which depend on the values of all the predictors in the model. By default, <code>marginaleffects</code> functions estimate quantities of interest for the empirical distribution of the data (i.e., for each row of the original dataset). However, users can specify the exact values of the predictors they want to investigate by using the <code>newdata</code> argument.</p> <p><code>newdata</code> accepts data frames, shortcut strings, or a call to the <code>datagrid()</code> function. For example, to compute the predicted outcome for a hypothetical car with all predictors equal to the sample mean or median, we can do:</p> <pre><code>predictions(mod, newdata = \"mean\")\n</code></pre> <pre><code> Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp   wt    am\n     18.4       0.68 27   &lt;0.001 531.7    17   19.7 147 3.22 0.406\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response\n</code></pre> <pre><code>predictions(mod, newdata = \"median\")\n</code></pre> <pre><code> Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp   wt am\n     19.4      0.646 30   &lt;0.001 653.2  18.1   20.6 123 3.33  0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \nType:  response\n</code></pre> <p>The <code>datagrid</code> function gives us a powerful way to define a grid of predictors. All the variables not mentioned explicitly in <code>datagrid()</code> are fixed to their mean or mode:</p> <pre><code>predictions(\n  mod,\n  newdata = datagrid(\n    am = c(0, 1),\n    wt = range))\n</code></pre> <pre><code> am   wt Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %  hp\n  0 1.51     23.3       2.71 8.60   &lt;0.001 56.7 17.96   28.6 147\n  0 5.42     12.8       2.98 4.30   &lt;0.001 15.8  6.96   18.6 147\n  1 1.51     27.1       2.85 9.52   &lt;0.001 69.0 21.56   32.7 147\n  1 5.42      5.9       5.81 1.01     0.31  1.7 -5.50   17.3 147\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt \nType:  response\n</code></pre> <p>The same mechanism is available in <code>comparisons()</code> and <code>slopes()</code>. To estimate the partial derivative of <code>mpg</code> with respect to <code>wt</code>, when <code>am</code> is equal to 0 and 1, while other predictors are held at their means:</p> <pre><code>slopes(\n  mod,\n  variables = \"wt\",\n  newdata = datagrid(am = 0:1))\n</code></pre> <pre><code> Term am Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n   wt  0    -2.68       1.42 -1.89   0.0593 4.1 -5.46  0.106\n   wt  1    -5.43       2.15 -2.52   0.0116 6.4 -9.65 -1.214\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, predicted_lo, predicted_hi, predicted, mpg, hp, wt \nType:  response\n</code></pre> <p>We can also plot how predictions, comparisons, or slopes change across different values of the predictors using three powerful plotting functions:</p> <ul> <li><code>plot_predictions</code>: Conditional Adjusted Predictions</li> <li><code>plot_comparisons</code>: Conditional Comparisons</li> <li><code>plot_slopes</code>: Conditional Marginal Effects</li> </ul> <p>For example, this plot shows the outcomes predicted by our model for different values of the <code>wt</code> and <code>am</code> variables:</p> <pre><code>plot_predictions(mod, condition = list(\"hp\", \"wt\" = \"threenum\", \"am\"))\n</code></pre> <p></p> <p>This plot shows how the derivative of <code>mpg</code> with respect to <code>am</code> varies as a function of <code>wt</code> and <code>hp</code>:</p> <pre><code>plot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"wt\" = \"minmax\"))\n</code></pre> <p></p> <p>See this vignette for more information: Plots, interactions, predictions, contrasts, and slopes</p>"},{"location":"vignettes/get_started/#averaging","title":"Averaging","text":"<p>Since predictions, comparisons, and slopes are conditional quantities, they can be a bit unwieldy. Often, it can be useful to report a one-number summary instead of one estimate per observation. Instead of presenting \u201cconditional\u201d estimates, some methodologists recommend reporting \u201cmarginal\u201d estimates, that is, an average of unit-level estimates.</p> <p>(This use of the word \u201cmarginal\u201d as \u201caveraging\u201d should not be confused with the term \u201cmarginal effect\u201d which, in the econometrics tradition, corresponds to a partial derivative, or the effect of a \u201csmall/marginal\u201d change.)</p> <p>To marginalize (average over) our unit-level estimates, we can use the <code>by</code> argument or the one of the convenience functions: <code>avg_predictions()</code>, <code>avg_comparisons()</code>, or <code>avg_slopes()</code>. For example, both of these commands give us the same result: the average predicted outcome in the <code>mtcars</code> dataset:</p> <pre><code>avg_predictions(mod)\n</code></pre> <pre><code> Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n     20.1       0.39 51.5   &lt;0.001 Inf  19.3   20.9\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>This is equivalent to manual computation by:</p> <pre><code>mean(predict(mod))\n</code></pre> <pre><code>[1] 20.09062\n</code></pre> <p>The main <code>marginaleffects</code> functions all include a <code>by</code> argument, which allows us to marginalize within sub-groups of the data. For example,</p> <pre><code>avg_comparisons(mod, by = \"am\")\n</code></pre> <pre><code> Term          Contrast am Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 %   97.5 %\n   am mean(1) - mean(0)  0  -1.3830     2.5250 -0.548  0.58388 0.8 -6.3319  3.56589\n   am mean(1) - mean(0)  1   1.9029     2.3086  0.824  0.40980 1.3 -2.6219  6.42773\n   hp mean(+1)           0  -0.0343     0.0159 -2.160  0.03079 5.0 -0.0654 -0.00317\n   hp mean(+1)           1  -0.0436     0.0213 -2.050  0.04039 4.6 -0.0854 -0.00191\n   wt mean(+1)           0  -2.4799     1.2316 -2.014  0.04406 4.5 -4.8939 -0.06595\n   wt mean(+1)           1  -6.0718     1.9762 -3.072  0.00212 8.9 -9.9451 -2.19846\n\nColumns: term, contrast, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response\n</code></pre> <p>Marginal Means are a special case of predictions, which are marginalized (or averaged) across a balanced grid of categorical predictors. To illustrate, we estimate a new model with categorical predictors:</p> <pre><code>dat &lt;- mtcars\ndat$am &lt;- as.logical(dat$am)\ndat$cyl &lt;- as.factor(dat$cyl)\nmod_cat &lt;- lm(mpg ~ am + cyl + hp, data = dat)\n</code></pre> <p>We can compute marginal means manually using the functions already described:</p> <pre><code>avg_predictions(\n  mod_cat,\n  newdata = datagrid(cyl = unique, am = unique),\n  by = \"am\")\n</code></pre> <pre><code>    am Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n FALSE     18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n  TRUE     22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>For convenience, the <code>marginaleffects</code> package for <code>R</code> also includes a <code>marginal_means()</code> function:</p> <pre><code>marginal_means(mod_cat, variables = \"am\")\n</code></pre> <pre><code> Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n   am FALSE 18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n   am  TRUE 22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n\nResults averaged over levels of: cyl, am \nColumns: term, value, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>The Marginal Means vignette offers more detail.</p>"},{"location":"vignettes/get_started/#hypothesis-and-equivalence-tests","title":"Hypothesis and equivalence tests","text":"<p>The <code>hypotheses()</code> function and the <code>hypothesis</code> argument can be used to conduct linear and non-linear hypothesis tests on model coefficients, or on any of the quantities computed by the functions introduced above.</p> <p>Consider this model:</p> <pre><code>mod &lt;- lm(mpg ~ qsec * drat, data = mtcars)\ncoef(mod)\n</code></pre> <pre><code>(Intercept)        qsec        drat   qsec:drat \n 12.3371987  -1.0241183  -3.4371461   0.5973153\n</code></pre> <p>Can we reject the null hypothesis that the <code>drat</code> coefficient is 2 times the size of the <code>qsec</code> coefficient?</p> <pre><code>hypotheses(mod, \"drat = 2 * qsec\")\n</code></pre> <pre><code>            Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n drat = 2 * qsec    -1.39       10.8 -0.129    0.897 0.2 -22.5   19.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>We can ask the same question but refer to parameters by position, with indices <code>b1</code>, <code>b2</code>, <code>b3</code>, etc.:</p> <pre><code>hypotheses(mod, \"b3 = 2 * b2\")\n</code></pre> <pre><code>        Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b3 = 2 * b2    -1.39       10.8 -0.129    0.897 0.2 -22.5   19.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>The main functions in <code>marginaleffects</code> all have a <code>hypothesis</code> argument, which means that we can do complex model testing. For example, consider two slope estimates:</p> <pre><code>slopes(\n  mod,\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n</code></pre> <pre><code> Term qsec Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n drat 14.5     5.22       3.79 1.38   0.1682 2.6 -2.206   12.7\n drat 22.9    10.24       5.16 1.98   0.0472 4.4  0.127   20.4\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, qsec, predicted_lo, predicted_hi, predicted, mpg, drat \nType:  response\n</code></pre> <p>Are these two slopes significantly different from one another? To test this, we can use the <code>hypothesis</code> argument:</p> <pre><code>slopes(\n  mod,\n  hypothesis = \"b1 = b2\",\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n</code></pre> <pre><code>  Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n b1=b2    -5.02       8.52 -0.589    0.556 0.8 -21.7   11.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>Alternatively, we can also refer to values with term names (when they are unique):</p> <pre><code>avg_slopes(mod)\n</code></pre> <pre><code> Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n drat     7.22      1.365 5.29  &lt; 0.001 23.0 4.549   9.90\n qsec     1.12      0.433 2.60  0.00944  6.7 0.275   1.97\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <pre><code>avg_slopes(mod, hypothesis = \"drat = qsec\")\n</code></pre> <pre><code>      Term Estimate Std. Error   z Pr(&gt;|z|)    S 2.5 % 97.5 %\n drat=qsec      6.1       1.45 4.2   &lt;0.001 15.2  3.25   8.95\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>Now, imagine that for theoretical (or substantive or clinical) reasons, we only care about slopes larger than 2. We can use the <code>equivalence</code> argument to conduct an equivalence test:</p> <pre><code>avg_slopes(mod, equivalence = c(-2, 2))\n</code></pre> <pre><code> Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % p (NonSup) p (NonInf) p (Equiv)\n drat     7.22      1.365 5.29  &lt; 0.001 23.0 4.549   9.90     0.9999     &lt;0.001    0.9999\n qsec     1.12      0.433 2.60  0.00944  6.7 0.275   1.97     0.0216     &lt;0.001    0.0216\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \nType:  response\n</code></pre> <p>See the Hypothesis Tests and Custom Contrasts vignette for background, details, and for instructions on how to conduct hypothesis tests in more complex situations.</p>"},{"location":"vignettes/get_started/#next-steps","title":"Next steps","text":"<p>There is much more you can do with <code>marginaleffects</code>. Click through he table of contents to read the many vignettes, starting with:</p> <ul> <li>Predictions</li> <li>Comparisons</li> <li>Slopes</li> <li>Hypothesis and equivalence tests</li> <li>Plots</li> </ul> <p>Then, move on to read the case studies and technical notes (see sidebar).</p>"},{"location":"vignettes/heterogeneity/","title":"Heterogeneity","text":"<p>author: \u201cVincent Arel-Bundock\u201d</p> <p>This short vignette illustrates how to use recursive partitioning to explore treatment effect heterogeneity. This exercise inspired by Scholbeck et al.\u00a02022 and their concept of \u201ccATE\u201d.</p> <p>As pointed out in other vignettes, most of the quantities estimated by the <code>marginaleffects</code> package are \u201cconditional\u201d, in the sense that they vary based on the values of all the predictors in our model. For instance, consider a Poisson regression that models the number of hourly bike rentals in Washington, DC:</p> <pre><code>library(marginaleffects)\nlibrary(partykit)\ndata(bikes, package = \"fmeffects\")\n\nmod &lt;- glm(\n    count ~ season * weekday + weather * temp,\n    data = bikes, family = quasipoisson)\n</code></pre> <p>We can use the <code>comparisons()</code> function to estimate how the predicted outcome changes for a 5 celsius increase in temperature:</p> <pre><code>cmp &lt;- comparisons(mod, variables = list(temp = 5))\ncmp\n</code></pre> <pre><code> Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n temp       +5     2.96      0.827 3.58  &lt; 0.001 11.5  1.343   4.58\n temp       +5     1.63      0.645 2.53  0.01155  6.4  0.365   2.90\n temp       +5    15.97      2.716 5.88  &lt; 0.001 27.9 10.645  21.29\n temp       +5    22.10      3.546 6.23  &lt; 0.001 31.0 15.145  29.05\n temp       +5    24.98      4.120 6.06  &lt; 0.001 29.5 16.909  33.06\n--- 717 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n temp       +5    25.61      4.373 5.86  &lt; 0.001 27.7 17.036  34.18\n temp       +5    20.31      3.463 5.86  &lt; 0.001 27.7 13.522  27.10\n temp       +5     2.47      0.794 3.10  0.00191  9.0  0.909   4.02\n temp       +5     1.80      0.621 2.91  0.00366  8.1  0.587   3.02\n temp       +5    16.31      2.813 5.80  &lt; 0.001 27.1 10.792  21.82\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, count, season, weekday, weather, temp \nType:  response\n</code></pre> <p>The output printed above includes 727 rows: 1 for each of the rows in the original <code>bikes</code> dataset. Indeed, since the \u201ceffect\u201d of a 5 unit increase depends on the values of covariates, different unit of observation will typically be associated with different contrasts.</p> <p>In such cases, a common strategy is to compute an average difference, as described in the G-Computation vignette:</p> <pre><code>avg_comparisons(mod, variables = list(temp = 5))\n</code></pre> <pre><code> Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n temp       +5       29       4.22 6.88   &lt;0.001 37.2  20.7   37.3\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>Alternatively, one may be interested in exploring heterogeneity in effect sizes in different subsets of the data. A convenient way to achieve this is to use the <code>ctree</code> function of the <code>partykit</code> package. This function allows us to use recursive partitioning (conditional inference trees) to find subspaces with reasonably homogenous estimates, and to report useful graphical and textual summaries.</p> <p>Imagine that we are particularly interested in how the effect of temperature on bike rentals varies based on day of the week and season:</p> <pre><code>tree &lt;- ctree(\n    estimate ~ weekday + season,\n    data = cmp,\n    control = ctree_control(maxdepth = 2)\n)\n</code></pre> <p>Now we can use the <code>plot()</code> function to draw the distributions of estimates for the effect of an increase of 5C on bike rentals, by week day and season:</p> <pre><code>plot(tree)\n</code></pre> <p></p> <p>To obtain conditional average estimates for each subspace, we first use the <code>predict()</code> function in order to place each observation in the dataset in its corresponding \u201cbucket\u201d or \u201cnode\u201d. Then, we use the <code>by</code> argument to indicate that <code>comparisons()</code> should compute average estimates for each of the nodes in the tree:</p> <pre><code>dat &lt;- transform(bikes, nodeid = predict(tree, type = \"node\"))\ncomparisons(mod,\n    variables = list(temp = 5),\n    newdata = dat,\n    by = \"nodeid\")\n</code></pre> <pre><code> Term Contrast nodeid Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n temp mean(+5)      3     6.42      1.024 6.26   &lt;0.001 31.3  4.41   8.42\n temp mean(+5)      4     2.27      0.536 4.23   &lt;0.001 15.4  1.22   3.32\n temp mean(+5)      6    44.11      6.416 6.87   &lt;0.001 37.2 31.53  56.68\n temp mean(+5)      7    21.43      3.250 6.59   &lt;0.001 34.4 15.06  27.80\n\nColumns: term, contrast, nodeid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response\n</code></pre> <p>The four <code>nodeid</code> values correspond to the terminal nodes in this tree:</p> <pre><code>print(tree)\n</code></pre> <pre><code>Model formula:\nestimate ~ weekday + season\n\nFitted party:\n[1] root\n|   [2] weekday in Sun, Sat\n|   |   [3] season in fall, summer, winter: 6.417 (n = 156, err = 341.3)\n|   |   [4] season in spring: 2.267 (n = 54, err = 24.4)\n|   [5] weekday in Mon, Tue, Wed, Thu, Fri\n|   |   [6] season in fall, summer, winter: 44.105 (n = 392, err = 31243.9)\n|   |   [7] season in spring: 21.431 (n = 125, err = 2434.0)\n\nNumber of inner nodes:    3\nNumber of terminal nodes: 4\n</code></pre>"},{"location":"vignettes/hypothesis/","title":"Hypothesis Tests","text":"<p>This vignette introduces the <code>hypotheses()</code> function, and the <code>hypothesis</code> argument of the <code>comparisons()</code>, <code>slopes()</code>, and <code>predictions()</code> function. These features allow users to conduct linear and non-linear hypothesis tests and to compute custom contrasts (linear combinations) between parameters.</p>"},{"location":"vignettes/hypothesis/#null-hypothesis","title":"Null hypothesis","text":"<p>The simplest way to modify a hypothesis test is to change the null hypothesis. By default, all functions in the <code>marginaleffects</code> package assume that the null is 0. This can be changed by changing the <code>hypothesis</code> argument.</p> <p>For example, consider a logistic regression model:</p> <pre><code>library(marginaleffects)\nmod &lt;- glm(am ~ hp + drat, data = mtcars, family = binomial)\n</code></pre> <p>We can compute the predicted outcome for a hypothetical unit where all regressors are fixed to their sample means:</p> <pre><code>predictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S  2.5 % 97.5 %  hp drat\n#&gt;     0.231    0.135 2.9 0.0584  0.592 147  3.6\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat \n#&gt; Type:  invlink(link)\n</code></pre> <p>The Z statistic and p value reported above assume that the null hypothesis equals zero. We can change the null with the <code>hypothesis</code> argument:</p> <pre><code>predictions(mod, newdata = \"mean\", hypothesis = .5)\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S  2.5 % 97.5 %  hp drat\n#&gt;     0.231   0.0343 4.9 0.0584  0.592 147  3.6\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat \n#&gt; Type:  invlink(link)\n</code></pre> <p>This can obviously be useful in other contexts. For instance, if we compute risk ratios (at the mean) associated with an increase of 1 unit in <code>hp</code>, it makes more sense to test the null hypothesis that the ratio of predictions is 1 rather than 0:</p> <pre><code>comparisons(\n    mod,\n    newdata = \"mean\",\n    variables = \"hp\",\n    comparison = \"ratio\",\n    hypothesis = 1) |&gt;\n    print(digits = 3)\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %  hp drat\n#&gt;    hp       +1     1.01    0.00791 1.05    0.293 1.8 0.993   1.02 147  3.6\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp, drat \n#&gt; Type:  response\n</code></pre> <p>Warning: Z statistics and p values are computed before applying functions in <code>transform</code>.</p>"},{"location":"vignettes/hypothesis/#hypothesis-tests-with-the-delta-method","title":"Hypothesis tests with the delta method","text":"<p>The <code>marginaleffects</code> package includes a powerful function called <code>hypotheses()</code>. This function emulates the behavior of the well-established <code>car::deltaMethod</code> and <code>car::linearHypothesis</code> functions, but it supports more models, requires fewer dependencies, and offers some convenience features like shortcuts for robust standard errors.</p> <p><code>hypotheses()</code> can be used to compute estimates and standard errors of arbitrary functions of model parameters. For example, it can be used to conduct tests of equality between coefficients, or to test the value of some linear or non-linear combination of quantities of interest. <code>hypotheses()</code> can also be used to conduct hypothesis tests on other functions of a model\u2019s parameter, such as adjusted predictions or marginal effects.</p> <p>Let\u2019s start by estimating a simple model:</p> <pre><code>library(marginaleffects)\nmod &lt;- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n</code></pre> <p>When the <code>FUN</code> and <code>hypothesis</code> arguments of <code>hypotheses()</code> equal <code>NULL</code> (the default), the function returns a data.frame of raw estimates:</p> <pre><code>hypotheses(mod)\n#&gt; \n#&gt;          Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 %    97.5 %\n#&gt;  (Intercept)   35.8460      2.041 17.56   &lt;0.001 227.0 31.8457 39.846319\n#&gt;  hp            -0.0231      0.012 -1.93   0.0531   4.2 -0.0465  0.000306\n#&gt;  wt            -3.1814      0.720 -4.42   &lt;0.001  16.6 -4.5918 -1.771012\n#&gt;  factor(cyl)6  -3.3590      1.402 -2.40   0.0166   5.9 -6.1062 -0.611803\n#&gt;  factor(cyl)8  -3.1859      2.170 -1.47   0.1422   2.8 -7.4399  1.068169\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Test of equality between coefficients:</p> <pre><code>hypotheses(mod, \"hp = wt\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  hp = wt     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Non-linear function of coefficients</p> <pre><code>hypotheses(mod, \"exp(hp + wt) = 0.1\")\n#&gt; \n#&gt;                Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;  exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 4.6 -0.117 -0.0022\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>The <code>vcov</code> argument behaves in the same was as in the <code>slopes()</code> function. It allows us to easily compute robust standard errors:</p> <pre><code>hypotheses(mod, \"hp = wt\", vcov = \"HC3\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  hp = wt     3.16      0.805 3.92   &lt;0.001 13.5  1.58   4.74\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>We can use shortcuts like <code>b1</code>, <code>b2</code>, <code>...</code> to identify the position of each parameter in the output of <code>FUN</code>. For example, <code>b2=b3</code> is equivalent to <code>hp=wt</code> because those term names appear in the 2nd and 3rd row when we call <code>hypotheses(mod)</code>.</p> <pre><code>hypotheses(mod, \"b2 = b3\")\n#&gt; \n#&gt;     Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b2 = b3     3.16       0.72 4.39   &lt;0.001 16.4  1.75   4.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <pre><code>hypotheses(mod, hypothesis = \"b* / b3 = 1\")\n#&gt; \n#&gt;         Term  Estimate Std. Error         z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  b1 / b3 = 1 -12.26735    2.07340   -5.9165   &lt;0.001 28.2 -16.33 -8.204\n#&gt;  b2 / b3 = 1  -0.99273    0.00413 -240.5539   &lt;0.001  Inf  -1.00 -0.985\n#&gt;  b3 / b3 = 1   0.00000         NA        NA       NA   NA     NA     NA\n#&gt;  b4 / b3 = 1   0.05583    0.58287    0.0958    0.924  0.1  -1.09  1.198\n#&gt;  b5 / b3 = 1   0.00141    0.82981    0.0017    0.999  0.0  -1.62  1.628\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Term names with special characters must be enclosed in backticks:</p> <pre><code>hypotheses(mod, \"`factor(cyl)6` = `factor(cyl)8`\")\n#&gt; \n#&gt;                             Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 0.1 -3.41   3.07\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre>"},{"location":"vignettes/hypothesis/#arbitrary-functions-fun","title":"Arbitrary functions: <code>FUN</code>","text":"<p>The <code>FUN</code> argument can be used to compute standard errors for arbitrary functions of model parameters. This user-supplied function must accept a single model object, and return a numeric vector or a data.frame with two columns named <code>term</code> and <code>estimate</code>.</p> <pre><code>mod &lt;- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf &lt;- function(x) {\n    out &lt;- x$coefficients[\"hp\"] + x$coefficients[\"mpg\"]\n    return(out)\n}\nhypotheses(mod, FUN = f)\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;     1     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>With labels:</p> <pre><code>f &lt;- function(x) {\n    out &lt;- data.frame(\n        term = \"Horsepower + Miles per Gallon\",\n        estimate = x$coefficients[\"hp\"] + x$coefficients[\"mpg\"]\n    )\n    return(out)\n}\nhypotheses(mod, FUN = f)\n#&gt; \n#&gt;                           Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Horsepower + Miles per Gallon     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Test of equality between two predictions (row 2 vs row 3):</p> <pre><code>f &lt;- function(x) predict(x, newdata = mtcars)\nhypotheses(mod, FUN = f, hypothesis = \"b2 = b3\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  b2 = b3    -1.33      0.616 -2.16   0.0305 5.0 -2.54 -0.125\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Note that we specified the <code>newdata</code> argument in the <code>f</code> function. This is because the <code>predict()</code> method associated with <code>lm</code> objects will automatically the original fitted values when <code>newdata</code> is <code>NULL</code>, instead of returning the slightly altered fitted values which we need to compute numerical derivatives in the delta method.</p> <p>We can also use numeric vectors to specify linear combinations of parameters. For example, there are 3 coefficients in the last model we estimated. To test the null hypothesis that the sum of the 2nd and 3rd coefficients is equal to 0, we can do:</p> <pre><code>hypotheses(mod, hypothesis = c(0, 1, 1))\n#&gt; \n#&gt;    Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom     1.31      0.593 2.22   0.0266 5.2 0.153   2.48\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>See below for more example of how to use string formulas, numeric vectors, or matrices to calculate custom contrasts, linear combinations, and linear or non-linear hypothesis tests.</p>"},{"location":"vignettes/hypothesis/#arbitrary-quantities-with-data-frames","title":"Arbitrary quantities with data frames","text":"<p><code>marginaleffects</code> can also compute uncertainty estimates for arbitrary quantities hosted in a data frame, as long as the user can supply a variance-covariance matrix. (Thanks to Kyle F Butts for this cool feature and example!)</p> <p>Say you run a monte-carlo simulation and you want to perform hypothesis of various quantities returned from each simulation. The quantities are correlated within each draw:</p> <pre><code># simulated means and medians\ndraw &lt;- function(i) { \n  x &lt;- rnorm(n = 10000, mean = 0, sd = 1)\n  out &lt;- data.frame(median = median(x), mean =  mean(x))\n  return(out)\n}\nsims &lt;- do.call(\"rbind\", lapply(1:25, draw))\n\n# average mean and average median \ncoeftable &lt;- data.frame(\n  term = c(\"median\", \"mean\"),\n  estimate = c(mean(sims$median), mean(sims$mean))\n)\n\n# variance-covariance\nvcov &lt;- cov(sims)\n\n# is the median equal to the mean?\nhypotheses(\n  coeftable,\n  vcov = vcov,\n  hypothesis = \"median = mean\"\n)\n#&gt; \n#&gt;           Term Estimate Std. Error     z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;  median = mean   0.0012    0.00937 0.128    0.898 0.2 -0.0172 0.0196\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre>"},{"location":"vignettes/hypothesis/#hypotheses-formulas","title":"<code>hypotheses</code> Formulas","text":"<p>Each of the 4 core functions of the package support a <code>hypothesis</code> argument which behaves similarly to the <code>hypotheses()</code> function. This argument allows users to specify custom hypothesis tests and contrasts, in order to test null hypotheses such as:</p> <ul> <li>The coefficients \u03b2<sub>1</sub> and \u03b2<sub>2</sub> are equal.</li> <li>The marginal effects of X<sub>1</sub> and X<sub>2</sub> equal.</li> <li>The marginal effect of X when W\u2004=\u20040 is equal to the marginal     effect of X when W\u2004=\u20041.</li> <li>A non-linear function of adjusted predictions is equal to 100.</li> <li>The marginal mean in the control group is equal to the average of     marginal means in the other 3 treatment arms.</li> <li>Cross-level contrasts: In a multinomial model, the effect of X on     the 1st outcome level is equal to the effect of X on the 2nd     outcome level.</li> </ul>"},{"location":"vignettes/hypothesis/#marginal-effects","title":"Marginal effects","text":"<p>For example, let\u2019s fit a model and compute some marginal effects at the mean:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ am + vs, data = mtcars)\n\nmfx &lt;- slopes(mod, newdata = \"mean\")\nmfx\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    am    1 - 0     6.07       1.27 4.76   &lt;0.001 19.0  3.57   8.57\n#&gt;    vs    1 - 0     6.93       1.26 5.49   &lt;0.001 24.6  4.46   9.40\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, am, vs \n#&gt; Type:  response\n</code></pre> <p>Is the marginal effect of <code>am</code> different from the marginal effect of <code>vs</code>? To answer this question we can run a linear hypothesis test using the <code>hypotheses</code> function:</p> <pre><code>hypotheses(mfx, hypothesis = \"am = vs\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Alternatively, we can specify the hypothesis directly in the original call:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ am + vs, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"am = vs\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The <code>hypotheses</code> string can include any valid <code>R</code> expression, so we can run some silly non-linear tests:</p> <pre><code>slopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"exp(am) - 2 * vs = -400\")\n#&gt; \n#&gt;               Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  exp(am)-2*vs=-400      817        550 1.49    0.137 2.9  -261   1896\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>But note that the p\u00a0values and confidence intervals are calculated using the delta method and are thus based on the assumption that the <code>hypotheses</code> expression is approximately normally distributed. For (very) non-linear functions of the parameters, this is not realistic, and we get p\u00a0values with incorrect error rates and confidence intervals with incorrect coverage probabilities. For such hypotheses, it\u2019s better to calculate the confidence intervals using the bootstrap (see <code>inferences</code> for details):</p> <pre><code>set.seed(1234)\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"exp(am) - 2 * vs = -400\") |&gt;\n  inferences(method = \"boot\")\n#&gt; \n#&gt;               Term Estimate Std. Error 2.5 % 97.5 %\n#&gt;  exp(am)-2*vs=-400      817       1854   414   6990\n#&gt; \n#&gt; Columns: term, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>While the confidence interval from the delta method is symmetric (equal to the estimate\u00a0\u00b1\u00a01.96 times the standard error), the more reliable confidence interval from the bootstrap is (here) highly skewed.</p>"},{"location":"vignettes/hypothesis/#adjusted-predictions","title":"Adjusted Predictions","text":"<p>Now consider the case of adjusted predictions:</p> <pre><code>p &lt;- predictions(\n    mod,\n    newdata = datagrid(am = 0:1, vs = 0:1))\np\n#&gt; \n#&gt;  am vs Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0  0     14.6      0.926 15.8   &lt;0.001 183.4  12.8   16.4\n#&gt;   0  1     21.5      1.130 19.0   &lt;0.001 266.3  19.3   23.7\n#&gt;   1  0     20.7      1.183 17.5   &lt;0.001 224.5  18.3   23.0\n#&gt;   1  1     27.6      1.130 24.4   &lt;0.001 435.0  25.4   29.8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, am, vs \n#&gt; Type:  response\n</code></pre> <p>Since there is no <code>term</code> column in the output of the <code>predictions</code> function, we must use parameter identifiers like <code>b1</code>, <code>b2</code>, etc. to determine which estimates we want to compare:</p> <pre><code>hypotheses(p, hypothesis = \"b1 = b2\")\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1=b2    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Or directly:</p> <pre><code>predictions(\n    mod,\n    hypothesis = \"b1 = b2\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;   Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1=b2    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate[1] - p$estimate[2]\n#&gt; [1] -6.929365\n</code></pre> <p>In the next section, we will see that we can get equivalent results by using a vector of contrast weights, which will be used to compute a linear combination of estimates:</p> <pre><code>predictions(\n    mod,\n    hypothesis = c(1, -1, 0, 0),\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;    Term Estimate Std. Error     z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  custom    -6.93       1.26 -5.49   &lt;0.001 24.6  -9.4  -4.46\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>There are many more possibilities:</p> <pre><code>predictions(\n    mod,\n    hypothesis = \"b1 + b2 = 30\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;      Term Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  b1+b2=30     6.12       1.64 3.74   &lt;0.001 12.4  2.91   9.32\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate[1] + p$estimate[2] - 30\n#&gt; [1] 6.118254\n\npredictions(\n    mod,\n    hypothesis = \"(b2 - b1) / (b3 - b2) = 0\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#&gt; \n#&gt;               Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (b2-b1)/(b3-b2)=0    -8.03         17 -0.473    0.636 0.7 -41.3   25.2\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/hypothesis/#average-contrasts-or-marginal-effects","title":"Average contrasts or marginal effects","text":"<p>The standard workflow with the <code>marginaleffects</code> package is to call a function like <code>predictions()</code>, <code>slopes()</code> or <code>comparisons()</code> to compute unit-level quantities; or one of their cousins <code>avg_predictions()</code>, <code>avg_comparisons()</code>, or <code>avg_slopes()</code> to aggregate the unit-level quantities into \u201cAverage Marginal Effects\u201d or \u201cAverage Contrasts.\u201d We can also use the <code>comparison</code> argument to emulate the behavior of the <code>avg_*()</code> functions.</p> <p>First, note that these three commands produce the same results:</p> <pre><code>comparisons(mod, variables = \"vs\")$estimate |&gt; mean()\n#&gt; [1] 6.929365\n\navg_comparisons(mod, variables = \"vs\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    vs    1 - 0     6.93       1.26 5.49   &lt;0.001 24.6  4.46    9.4\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\ncomparisons(\n    mod,\n    variables = \"vs\",\n    comparison = \"differenceavg\")\n#&gt; \n#&gt;  Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    vs mean(1) - mean(0)     6.93       1.26 5.49   &lt;0.001 24.6  4.46    9.4\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>See the transformations section of the Contrasts vignette for more details.</p> <p>With these results in hand, we can now conduct a linear hypothesis test between average marginal effects:</p> <pre><code>comparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    comparison = \"differenceavg\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Computing contrasts between average marginal effects requires a little care to obtain the right scale. In particular, we need to specify both the <code>variables</code> and the <code>comparison</code>:</p> <pre><code>comparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    variables = c(\"am\", \"vs\"),\n    comparison = \"dydxavg\")\n#&gt; \n#&gt;   Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/hypothesis/#generic-hypothesis-for-unsupported-s3-objects","title":"Generic Hypothesis for Unsupported S3 Objects","text":"<p><code>marginaleffects</code> provides a generic interface for hypothesis tests for linear models by providing (1) a data.frame containing point estimates (consiting of columns <code>term</code> containing the names and <code>estimate</code> containing the point estiamtes) and (2) a variance-covariance matrix of estimates.</p> <pre><code>coeftable &lt;- data.frame(term = names(mod$coefficients), estimate = as.numeric(mod$coefficients))\nvcov &lt;- vcov(mod)\n\nhypotheses(\n  coeftable, vcov = vcov, \n  hypothesis = \"am = vs\"\n)\n#&gt; \n#&gt;     Term Estimate Std. Error      z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  am = vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre>"},{"location":"vignettes/hypothesis/#hypotheses-vectors-and-matrices","title":"<code>hypotheses</code> Vectors and Matrices","text":"<p>The <code>marginal_means()</code> function computes estimated marginal means. The <code>hypothesis</code> argument of that function offers a powerful mechanism to estimate custom contrasts between marginal means, by way of linear combination.</p> <p>Consider a simple example:</p> <pre><code>library(marginaleffects)\nlibrary(emmeans)\nlibrary(nnet)\n\ndat &lt;- mtcars\ndat$carb &lt;- factor(dat$carb)\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\n\nmod &lt;- lm(mpg ~ carb + cyl, dat)\nmm &lt;- marginal_means(mod, variables = \"carb\")\nmm\n#&gt; \n#&gt;  Term Value Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  carb     1 21.7       1.44 15.06   &lt;0.001 167.8  18.8   24.5\n#&gt;  carb     2 21.3       1.23 17.29   &lt;0.001 220.0  18.9   23.8\n#&gt;  carb     3 21.4       2.19  9.77   &lt;0.001  72.5  17.1   25.7\n#&gt;  carb     4 18.9       1.21 15.59   &lt;0.001 179.7  16.5   21.3\n#&gt;  carb     6 19.8       3.55  5.56   &lt;0.001  25.2  12.8   26.7\n#&gt;  carb     8 20.1       3.51  5.73   &lt;0.001  26.6  13.2   27.0\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, value, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The contrast between marginal means for <code>carb==1</code> and <code>carb==2</code> is:</p> <pre><code>21.66232 - 21.34058 \n#&gt; [1] 0.32174\n</code></pre> <p>or</p> <pre><code>21.66232 + -(21.34058)\n#&gt; [1] 0.32174\n</code></pre> <p>or</p> <pre><code>sum(c(21.66232, 21.34058) * c(1, -1))\n#&gt; [1] 0.32174\n</code></pre> <p>or</p> <pre><code>c(21.66232, 21.34058) %*% c(1, -1)\n#&gt;         [,1]\n#&gt; [1,] 0.32174\n</code></pre> <p>The last two commands express the contrast of interest as a linear combination of marginal means.</p>"},{"location":"vignettes/hypothesis/#simple-contrast","title":"Simple contrast","text":"<p>In the <code>marginal_means()</code> function, we can supply a <code>hypothesis</code> argument to compute linear combinations of marginal means. This argument must be a numeric vector of the same length as the number of rows in the output of <code>marginal_means()</code>. For example, in the previous there were six rows, and the two marginal means we want to compare are at in the first two positions:</p> <pre><code>lc &lt;- c(1, -1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;    Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom 0.322       1.77 0.181    0.856 0.2 -3.15    3.8\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/hypothesis/#complex-contrast","title":"Complex contrast","text":"<p>Of course, we can also estimate more complex contrasts:</p> <pre><code>lc &lt;- c(0, -2, 1, 1, -1, 1)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;    Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom -2.02       6.32 -0.32    0.749 0.4 -14.4   10.4\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p><code>emmeans</code> produces similar results:</p> <pre><code>library(emmeans)\nem &lt;- emmeans(mod, \"carb\")\nlc &lt;- data.frame(custom_contrast = c(-2, 1, 1, 0, -1, 1))\ncontrast(em, method = lc)\n#&gt;  contrast        estimate   SE df t.ratio p.value\n#&gt;  custom_contrast   -0.211 6.93 24  -0.030  0.9760\n#&gt; \n#&gt; Results are averaged over the levels of: cyl\n</code></pre>"},{"location":"vignettes/hypothesis/#multiple-contrasts","title":"Multiple contrasts","text":"<p>Users can also compute multiple linear combinations simultaneously by supplying a numeric matrix to <code>hypotheses</code>. This matrix must have the same number of rows as the output of <code>slopes()</code>, and each column represents a distinct set of weights for different linear combinations. The column names of the matrix become labels in the output. For example:</p> <pre><code>lc &lt;- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    1, -1, 0, 0, 0, 0\n    ), ncol = 2)\ncolnames(lc) &lt;- c(\"Contrast A\", \"Contrast B\")\nlc\n#&gt;      Contrast A Contrast B\n#&gt; [1,]         -2          1\n#&gt; [2,]          1         -1\n#&gt; [3,]          1          0\n#&gt; [4,]          0          0\n#&gt; [5,]         -1          0\n#&gt; [6,]          1          0\n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#&gt; \n#&gt;        Term   Mean Std. Error       z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Contrast A -0.211       6.93 -0.0304    0.976 0.0 -13.79   13.4\n#&gt;  Contrast B  0.322       1.77  0.1814    0.856 0.2  -3.15    3.8\n#&gt; \n#&gt; Results averaged over levels of: cyl, carb \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/hypothesis/#contrasts-across-response-levels","title":"Contrasts across response levels","text":"<p>In models with multinomial outcomes, one may be interested in comparing outcomes or contrasts across response levels. For example, in this model there are 18 estimated marginal means, across 6 outcome levels (the <code>group</code> column):</p> <pre><code>library(nnet)\nmod &lt;- multinom(carb ~ mpg + cyl, data = dat, trace = FALSE)\nmm &lt;- marginal_means(mod, type = \"probs\")\nmm\n#&gt; \n#&gt;  Group Term Value     Mean Std. Error       z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      1  cyl     4 3.68e-01   2.60e-01 1.41647   0.1566 2.7 -1.41e-01 8.77e-01\n#&gt;      1  cyl     6 2.83e-01   1.96e-01 1.44488   0.1485 2.8 -1.01e-01 6.67e-01\n#&gt;      1  cyl     8 4.63e-04   9.59e-03 0.04824   0.9615 0.1 -1.83e-02 1.93e-02\n#&gt;      2  cyl     4 6.31e-01   2.60e-01 2.42772   0.0152 6.0  1.22e-01 1.14e+00\n#&gt;      2  cyl     6 1.85e-06   2.40e-06 0.77081   0.4408 1.2 -2.85e-06 6.55e-06\n#&gt;      2  cyl     8 6.65e-01   3.74e-01 1.77977   0.0751 3.7 -6.74e-02 1.40e+00\n#&gt;      3  cyl     4 6.85e-04   1.19e-02 0.05748   0.9542 0.1 -2.27e-02 2.40e-02\n#&gt;      3  cyl     6 1.12e-05   1.32e-03 0.00848   0.9932 0.0 -2.58e-03 2.60e-03\n#&gt;      3  cyl     8 3.10e-01   3.71e-01 0.83684   0.4027 1.3 -4.17e-01 1.04e+00\n#&gt;      4  cyl     4 2.12e-04   1.75e-02 0.01211   0.9903 0.0 -3.41e-02 3.45e-02\n#&gt;      4  cyl     6 5.56e-01   2.18e-01 2.55023   0.0108 6.5  1.29e-01 9.84e-01\n#&gt;      4  cyl     8 9.58e-03   2.28e-02 0.42007   0.6744 0.6 -3.51e-02 5.43e-02\n#&gt;      6  cyl     4 8.82e-06   8.39e-05 0.10506   0.9163 0.1 -1.56e-04 1.73e-04\n#&gt;      6  cyl     6 1.61e-01   1.54e-01 1.04698   0.2951 1.8 -1.40e-01 4.62e-01\n#&gt;      6  cyl     8 4.35e-09   9.89e-08 0.04393   0.9650 0.1 -1.90e-07 1.98e-07\n#&gt;      8  cyl     4 1.50e-04   7.97e-03 0.01878   0.9850 0.0 -1.55e-02 1.58e-02\n#&gt;      8  cyl     6 9.29e-06   7.98e-04 0.01164   0.9907 0.0 -1.56e-03 1.57e-03\n#&gt;      8  cyl     8 1.41e-02   4.66e-02 0.30323   0.7617 0.4 -7.72e-02 1.05e-01\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: group, term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre> <p>Let\u2019s contrast the marginal means in the first outcome level when <code>cyl</code> equals 4 and 6. These marginal means are located in rows 1 and 7 respectively:</p> <pre><code>lc &lt;- rep(0, nrow(mm))\nlc[1] &lt;- -1\nlc[7] &lt;- 1\nmarginal_means(\n    mod,\n    type = \"probs\",\n    hypothesis = lc)\n#&gt; \n#&gt;    Term   Mean Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  custom -0.367       0.26 -1.41    0.158 2.7 -0.877  0.143\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre> <p>This is indeed equal to the results we would have obtained manually:</p> <pre><code>2.828726e-01 - 3.678521e-01\n#&gt; [1] -0.0849795\n</code></pre> <p>Now let\u2019s say we want to calculate a \u201ccontrast in contrasts\u201d, that is, the outcome of a 3-step process:</p> <ol> <li>Contrast between <code>cyl=6</code> and <code>cyl=4</code> in the 1st outcome level</li> <li>Contrast between <code>cyl=6</code> and <code>cyl=4</code> in the 2nd outcome level</li> <li>Contrast between the contrasts defined in steps 1 and 2.</li> </ol> <p>We create the linear combination weights as follows:</p> <pre><code>lc &lt;- rep(0, nrow(mm))\nlc[c(1, 8)] &lt;- -1\nlc[c(7, 2)] &lt;- 1\n</code></pre> <p>To make sure that the weights are correct, we can display them side by side with the original <code>marginal_means()</code> output:</p> <pre><code>transform(mm[, 1:3], lc = lc)\n#&gt;    group term value lc\n#&gt; 1      1  cyl     4 -1\n#&gt; 2      1  cyl     6  1\n#&gt; 3      1  cyl     8  0\n#&gt; 4      2  cyl     4  0\n#&gt; 5      2  cyl     6  0\n#&gt; 6      2  cyl     8  0\n#&gt; 7      3  cyl     4  1\n#&gt; 8      3  cyl     6 -1\n#&gt; 9      3  cyl     8  0\n#&gt; 10     4  cyl     4  0\n#&gt; 11     4  cyl     6  0\n#&gt; 12     4  cyl     8  0\n#&gt; 13     6  cyl     4  0\n#&gt; 14     6  cyl     6  0\n#&gt; 15     6  cyl     8  0\n#&gt; 16     8  cyl     4  0\n#&gt; 17     8  cyl     6  0\n#&gt; 18     8  cyl     8  0\n</code></pre> <p>Compute the results:</p> <pre><code>marginal_means(mod, type = \"probs\", hypothesis = lc)\n#&gt; \n#&gt;    Term    Mean Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  custom -0.0843      0.321 -0.263    0.793 0.3 -0.714  0.545\n#&gt; \n#&gt; Results averaged over levels of: mpg, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre>"},{"location":"vignettes/hypothesis/#pairwise-contrasts-difference-in-differences","title":"Pairwise contrasts: Difference-in-Differences","text":"<p>Now we illustrate how to use the machinery described above to do pairwise comparisons between contrasts, a type of analysis often associated with a \u201cDifference-in-Differences\u201d research design.</p> <p>First, we simulate data with two treatment groups and pre/post periods:</p> <pre><code>library(data.table)\n\nN &lt;- 1000\ndid &lt;- data.table(\n    id = 1:N,\n    pre = rnorm(N),\n    trt = sample(0:1, N, replace = TRUE))\ndid$post &lt;- did$pre + did$trt * 0.3 + rnorm(N)\ndid &lt;- melt(\n    did,\n    value.name = \"y\",\n    variable.name = \"time\",\n    id.vars = c(\"id\", \"trt\"))\nhead(did)\n#&gt;    id trt time           y\n#&gt; 1:  1   1  pre -1.04356113\n#&gt; 2:  2   0  pre -0.99460367\n#&gt; 3:  3   0  pre -0.16962798\n#&gt; 4:  4   1  pre -0.01854487\n#&gt; 5:  5   0  pre -1.37156492\n#&gt; 6:  6   0  pre  0.33690893\n</code></pre> <p>Then, we estimate a linear model with a multiple interaction between the time and the treatment indicators. We also compute contrasts at the mean for each treatment level:</p> <pre><code>did_model &lt;- lm(y ~ time * trt, data = did)\n\ncomparisons(\n    did_model,\n    newdata = datagrid(trt = 0:1),\n    variables = \"time\")\n#&gt; \n#&gt;  Term   Contrast trt Estimate Std. Error      z Pr(&gt;|z|)    S  2.5 % 97.5 % time\n#&gt;  time post - pre   0   -0.035     0.0821 -0.426     0.67  0.6 -0.196  0.126  pre\n#&gt;  time post - pre   1    0.298     0.0792  3.757   &lt;0.001 12.5  0.142  0.453  pre\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, trt, predicted_lo, predicted_hi, predicted, y, time \n#&gt; Type:  response\n</code></pre> <p>Finally, we compute pairwise differences between contrasts. This is the Diff-in-Diff estimate:</p> <pre><code>comparisons(\n    did_model,\n    variables = \"time\",\n    newdata = datagrid(trt = 0:1),\n    hypothesis = \"pairwise\")\n#&gt; \n#&gt;           Term Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;  Row 1 - Row 2   -0.333      0.114 -2.92  0.00356 8.1 -0.556 -0.109\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/hypothesis/#joint-hypotheses-tests","title":"Joint hypotheses tests","text":"<p>The <code>hypotheses()</code> function can also test multiple hypotheses jointly. For example, consider this model:</p> <pre><code>model &lt;- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\ncoef(model)\n#&gt;        (Intercept)    as.factor(cyl)6    as.factor(cyl)8                 hp as.factor(cyl)6:hp as.factor(cyl)8:hp \n#&gt;        35.98302564       -15.30917451       -17.90295193        -0.11277589         0.10516262         0.09853177\n</code></pre> <p>We may want to test the null hypothesis that two of the coefficients are jointly (both) equal to zero.</p> <pre><code>hypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt; as.factor(cyl)6:hp = 0\n#&gt; as.factor(cyl)8:hp = 0\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  2.11    0.142    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n</code></pre> <p>The <code>joint</code> argument allows users to flexibly specify the parameters to be tested, using character vectors, integer indices, or Perl-compatible regular expressions. We can also specify the null hypothesis for each parameter individually using the <code>hypothesis</code> argument.</p> <p>Naturally, the <code>hypotheses</code> function also works with <code>marginaleffects</code> objects.</p> <pre><code># ## joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 0\n#&gt;  as.factor(cyl)8 = 0\n#&gt;  as.factor(cyl)6:hp = 0\n#&gt;  as.factor(cyl)8:hp = 0\n#&gt;  \n#&gt;    F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  5.7  0.00197    4   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 0\n#&gt;  as.factor(cyl)8 = 0\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  6.12  0.00665    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 1\n#&gt;  as.factor(cyl)8 = 1\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  6.84  0.00411    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  as.factor(cyl)6 = 1\n#&gt;  as.factor(cyl)8 = 2\n#&gt;  \n#&gt;     F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  7.47  0.00273    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: marginaleffects object\ncmp &lt;- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n#&gt; \n#&gt; \n#&gt; Joint hypothesis test:\n#&gt;  cyl 6 - 4 = 0\n#&gt;  cyl 8 - 4 = 0\n#&gt;  \n#&gt;    F Pr(&gt;|F|) Df 1 Df 2\n#&gt;  1.6    0.221    2   26\n#&gt; \n#&gt; Columns: statistic, p.value, df1, df2\n</code></pre> <p>We can also combine multiple calls to <code>hypotheses</code> to execute a joint test on linear combinations of coefficients:</p> <pre><code>## fit model\nmod &lt;- lm(mpg ~ factor(carb), mtcars)\n\n## hypothesis matrix for linear combinations\nH &lt;- matrix(0, nrow = length(coef(mod)), ncol = 2)\nH[2:3, 1] &lt;- H[4:6, 2] &lt;- 1\n\n## test individual linear combinations\nhyp &lt;- hypotheses(mod, hypothesis = H)\nhyp\n#&gt; \n#&gt;    Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  custom    -12.0       4.92 -2.44  0.01477 6.1 -21.6  -2.35\n#&gt;  custom    -25.5       9.03 -2.83  0.00466 7.7 -43.2  -7.85\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n## test joint hypotheses\n#hypotheses(hyp, joint = TRUE, hypothesis = c(-10, -20))\n</code></pre>"},{"location":"vignettes/hypothesis/#complex-aggregations","title":"Complex aggregations","text":"<p>The <code>FUN</code> argument of the <code>hypotheses()</code> function can be used to conduct complex aggregations of other estimates. For example, consider this ordered logit model fitted on a dataset of cars:</p> <pre><code>library(MASS)\nlibrary(dplyr)\n\ndat &lt;- transform(mtcars, gear = factor(gear))\nmod &lt;- polr(gear ~ factor(cyl) + hp, dat)\nsummary(mod)\n#&gt; Call:\n#&gt; polr(formula = gear ~ factor(cyl) + hp, data = dat)\n#&gt; \n#&gt; Coefficients:\n#&gt;                  Value Std. Error t value\n#&gt; factor(cyl)6  -3.87912    1.52625  -2.542\n#&gt; factor(cyl)8 -14.64228    4.46072  -3.282\n#&gt; hp             0.07269    0.02422   3.001\n#&gt; \n#&gt; Intercepts:\n#&gt;     Value    Std. Error t value \n#&gt; 3|4   3.6824   1.7945     2.0521\n#&gt; 4|5   7.3814   2.3473     3.1445\n#&gt; \n#&gt; Residual Deviance: 34.40969 \n#&gt; AIC: 44.40969\n</code></pre> <p>If we compute fitted values with the <code>predictions()</code> function, we obtain one predicted probability for each individual car and for each level of the response variable:</p> <pre><code>predictions(mod)\n#&gt; \n#&gt;  Group Estimate Std. Error      z Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.0440    0.04256   1.03  0.30081 1.7 -0.0394  0.127\n#&gt;      3   0.3931    0.19125   2.06  0.03982 4.7  0.0183  0.768\n#&gt;      3   0.9963    0.00721 138.17  &lt; 0.001 Inf  0.9822  1.010\n#&gt; --- 86 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;      5   0.6969    0.18931   3.68  &lt; 0.001 12.1  0.3258  1.068\n#&gt;      5   0.0555    0.06851   0.81  0.41775  1.3 -0.0788  0.190\n#&gt;      5   0.8115    0.20626   3.93  &lt; 0.001 13.5  0.4073  1.216\n#&gt;      5   0.9111    0.16818   5.42  &lt; 0.001 24.0  0.5815  1.241\n#&gt;      5   0.6322    0.19648   3.22  0.00129  9.6  0.2471  1.017\n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, cyl, hp \n#&gt; Type:  probs\n</code></pre> <p>There are three levels to the outcome: 3, 4, and 5. Imagine that, for each car in the dataset, we want to collapse categories of the output variable into two categories (\u201c3 &amp; 4\u201d and \u201c5\u201d) by taking sums of predicted probabilities. Then, we want to take the average of those predicted probabilities for each level of <code>cyl</code>. To do so, we define a custom function, and pass it to the <code>FUN</code> argument of the <code>hypotheses()</code> function:</p> <pre><code>aggregation_fun &lt;- function(model) {\n    predictions(model, vcov = FALSE) |&gt;\n        # label the new categories of outcome levels\n        mutate(group = ifelse(group %in% c(\"3\", \"4\"), \"3 &amp; 4\", \"5\")) |&gt;\n        # sum of probabilities at the individual level\n        summarize(estimate = sum(estimate), .by = c(\"rowid\", \"cyl\", \"group\")) |&gt;\n        # average probabilities for each value of `cyl`\n        summarize(estimate = mean(estimate), .by = c(\"cyl\", \"group\")) |&gt;\n        # the `FUN` argument requires a `term` column\n        rename(term = cyl)\n}\n\nhypotheses(mod, FUN = aggregation_fun)\n#&gt; \n#&gt;  Group Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3 &amp; 4    6   0.8390     0.0651 12.89   &lt;0.001 123.9 0.7115  0.967\n#&gt;  3 &amp; 4    4   0.7197     0.1099  6.55   &lt;0.001  34.0 0.5044  0.935\n#&gt;  3 &amp; 4    8   0.9283     0.0174 53.45   &lt;0.001   Inf 0.8943  0.962\n#&gt;  5        6   0.1610     0.0651  2.47   0.0134   6.2 0.0334  0.289\n#&gt;  5        4   0.2803     0.1099  2.55   0.0108   6.5 0.0649  0.496\n#&gt;  5        8   0.0717     0.0174  4.13   &lt;0.001  14.7 0.0377  0.106\n#&gt; \n#&gt; Columns: term, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>Note that this workflow will not work for bayesian models or with bootstrap. However, with those models it is trivial to do the same kind of aggregation by calling <code>posterior_draws()</code> and operating directly on draws from the posterior distribution. See the vignette on bayesian analysis for examples with the <code>posterior_draws()</code> function.</p>"},{"location":"vignettes/ipw/","title":"Ipw","text":"<p>title: Inverse Probability Weighting</p> <p>Inverse Probability Weighting (IPW) is a popular technique to remove confounding in statistical modeling. It essentially involves re-weighting your sample so that it represents the population you\u2019re interested in. Typically, we begin by estimating the predicted probability that each unit is treated. Then, we use these probabilities as weights in model fitting and in the computation of marginal effects, contrasts, risk differences, ratios, etc.</p> <p>This chapter introduces how to use <code>marginaleffects</code> for IPW. The presentation is very short. Readers who seek a more comprehensive understanding and application of these methods should refer to Noah Greifer\u2019s excellent and detailed work on the topic and to the <code>WeightIt</code> package vignettes and website.</p> <p>To illustrate, we use the Lalonde data.</p> <pre><code>library(marginaleffects)\ndata(\"lalonde\", package = \"MatchIt\")\nhead(lalonde)\n</code></pre> <pre><code>     treat age educ   race married nodegree re74 re75       re78\nNSW1     1  37   11  black       1        1    0    0  9930.0460\nNSW2     1  22    9 hispan       0        1    0    0  3595.8940\nNSW3     1  30   12  black       0        0    0    0 24909.4500\nNSW4     1  27   11  black       0        1    0    0  7506.1460\nNSW5     1  33    8  black       0        1    0    0   289.7899\nNSW6     1  22    9  black       0        1    0    0  4056.4940\n</code></pre> <p>To begin, we use a logistic regression model to estimate the probability that each unit will treated:</p> <pre><code>m &lt;- glm(treat ~ age + educ + race + re74, data = lalonde, family = binomial)\n</code></pre> <p>Then, we call <code>predictions()</code> to extract predicted probabilities. Note that we supply the original <code>lalonde</code> data explicity to the <code>newdata</code> argument. This ensures that all the original columns are carried over to the new dataset: <code>dat</code>. We also create a new column called <code>wts</code> that contains the inverse of the predicted probabilities:</p> <pre><code>dat &lt;- predictions(m, newdata = lalonde)\ndat$wts &lt;- ifelse(dat$treat == 1, 1 / dat$estimate, 1 / (1 - dat$estimate))\n</code></pre> <p>Now, we use linear regression to model the outcome of interest: personal income in 1978 (<code>re78</code>). Note that we use the predictions as weights in the model fitting process.</p> <pre><code>mod &lt;- lm(re78 ~ treat * (age + educ + race + re74), data = dat, weights = wts)\n</code></pre> <p>Finally, we call <code>avg_comparisons()</code> to compute the average treatment effect. Note that we use the <code>wts</code> argument to specify the weights to be used in the computation.</p> <pre><code>avg_comparisons(mod,\n    variables = \"treat\",\n    wts = \"wts\",\n    vcov = \"HC3\")\n</code></pre> <pre><code>  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat    1 - 0      973       1173 0.83    0.407 1.3 -1326   3272\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre> <p>By default, <code>avg_comparisons()</code> uses the Hajek estimator, that is, the weights are normalized to sum to 1 before computation. If a user wants to use the Horvitz-Thompson estimator\u2014where normalization accounts for sample size\u2014they can easily define a custom <code>comparison</code> function like this one:</p> <pre><code>ht &lt;- \\(hi, lo, w, newdata) {\n    (sum(hi * w) / nrow(newdata)) - (sum(lo * w) / nrow(newdata))\n}\n\ncomparisons(mod,\n    comparison = ht,\n    variables = \"treat\",\n    wts = \"wts\",\n    vcov = \"HC3\")\n</code></pre> <pre><code>  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat     1, 0     1851       2231 0.83    0.407 1.3 -2521   6222\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response\n</code></pre>"},{"location":"vignettes/links/","title":"Links","text":"<ul> <li>Equivalence Tests Using <code>marginaleffects</code>: Reproducing the Clark     and Golder (2006) Example from Rainey     (2014)     by Carlisle Rainey</li> <li>You are what you ATE: Choosing an effect size measure for binary     outcomes     by Cameron Patrick</li> <li>Causal inference with potential outcomes     bootcamp     by Solomon Kurz</li> <li>Marginal and conditional effects for GLMMs with     <code>marginaleffects</code>     by Andrew Heiss</li> <li>Marginalia: A guide to figuring out what the heck marginal effects,     marginal slopes, average marginal effects, marginal effects at the     mean, and all these other marginal things     are by     Andrew Heiss</li> <li>Matching     by Noah Greifer</li> <li>Double propensity score adjustment using     g-computation     by Noah Greifer</li> <li>Subgroup Analysis After Propensity Score Matching Using     R by Noah     Greifer</li> <li>Bayesian Model Averaged Marginal     Effects     by A. Jordan Nafa</li> </ul>"},{"location":"vignettes/lme4/","title":"Mixed Effects","text":"<p>This vignette replicates some of the analyses in this excellent blog post by Solomon Kurz: Use <code>emmeans()</code> to include 95% CIs around your <code>lme4</code>-based fitted lines</p> <p>Load libraries and fit two models of chick weights:</p> <pre><code>library(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\n## unconditional linear growth model\nfit1 &lt;- lmer(\n  weight ~ 1 + Time + (1 + Time | Chick),\n  data = ChickWeight)\n\n## conditional quadratic growth model\nfit2 &lt;- lmer(\n  weight ~ 1 + Time + I(Time^2) + Diet + Time:Diet + I(Time^2):Diet + (1 + Time + I(Time^2) | Chick),\n  data = ChickWeight)\n</code></pre>"},{"location":"vignettes/lme4/#unit-level-predictions","title":"Unit-level predictions","text":"<p>Predict weight of each chick over time:</p> <pre><code>pred1 &lt;- predictions(fit1,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np1 &lt;- ggplot(pred1, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Linear growth model\")\n\npred2 &lt;- predictions(fit2,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np2 &lt;- ggplot(pred2, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Quadratic growth model\")\n\np1 + p2\n</code></pre> <p></p> <p>Predictions for each chick, in the 4 counterfactual worlds with different values for the <code>Diet</code> variable:</p> <pre><code>pred &lt;- predictions(fit2)\n\nggplot(pred, aes(Time, estimate, level = Chick)) +\n    geom_line() +\n    ylab(\"Predicted Weight\") +\n    facet_wrap(~ Diet, labeller = label_both)\n</code></pre> <p></p>"},{"location":"vignettes/lme4/#population-level-predictions","title":"Population-level predictions","text":"<p>To make population-level predictions, we set the <code>Chick</code> variable to <code>NA</code>, and set <code>re.form=NA</code>. This last argument is offered by the <code>lme4::predict</code> function which is used behind the scenes to compute predictions:</p> <pre><code>pred &lt;- predictions(\n    fit2,\n    newdata = datagrid(Chick = NA,\n                       Diet = 1:4,\n                       Time = 0:21),\n    re.form = NA)\n\nggplot(pred, aes(x = Time, y = estimate, ymin = conf.low, ymax = conf.high)) +\n    geom_ribbon(alpha = .1, fill = \"red\") +\n    geom_line() +\n    facet_wrap(~ Diet, labeller = label_both) +\n    labs(title = \"Population-level trajectories\")\n</code></pre> <p></p>"},{"location":"vignettes/marginalmeans/","title":"Marginalmeans","text":""},{"location":"vignettes/marginalmeans/#marginal-means","title":"Marginal Means","text":"<p>In the context of this package, \u201cmarginal means\u201d refer to the values obtained by this three step process:</p> <ol> <li>Construct a \u201cgrid\u201d of predictor values with all combinations of     categorical variables, and where numeric variables are held at their     means.</li> <li>Calculate adjusted predictions for each cell in that grid.</li> <li>Take the average of those adjusted predictions across one dimension     of the grid to obtain the marginal means.</li> </ol> <p>For example, consider a model with a numeric, a factor, and a logical predictor:</p> <pre><code>library(marginaleffects)\n\ndat &lt;- mtcars\ndat$cyl &lt;- as.factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lm(mpg ~ hp + cyl + am, data = dat)\n</code></pre> <p>Using the <code>predictions</code> function, we set the <code>hp</code> variable at its mean and compute predictions for all combinations for <code>am</code> and <code>cyl</code>:</p> <pre><code>p &lt;- predictions(\n    mod,\n    newdata = datagrid(am = unique, cyl = unique))\n</code></pre> <p>For illustration purposes, it is useful to reshape the above results:</p>  am  cyl FALSE TRUE Marginal means by cyl 4 20.8 25.0 22.9 6 16.9 21.0 19.0 8 17.3 21.4 19.4 Marginal means by am 22.5 18.3 <p>The marginal means by <code>am</code> and <code>cyl</code> are obtained by taking the mean of the adjusted predictions across cells. The <code>marginal_means</code> function gives us the same results easily:</p> <pre><code>marginal_means(mod)\n#&gt; \n#&gt;  Term Value Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   cyl 4     22.9      1.357 16.9   &lt;0.001 209.7  20.2   25.5\n#&gt;   cyl 6     19.0      1.073 17.7   &lt;0.001 229.7  16.9   21.1\n#&gt;   cyl 8     19.4      1.377 14.1   &lt;0.001 146.6  16.7   22.1\n#&gt;   am  FALSE 18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n#&gt;   am  TRUE  22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n#&gt; \n#&gt; Results averaged over levels of: hp, cyl, am \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We could obtain the same results with the <code>by</code> argument of the <code>predictions()</code> function, which allows us to marginalize across some predictors:</p> <pre><code>predictions(\n    mod,\n    by = \"am\",\n    newdata = datagrid(am = unique, cyl = unique))\n#&gt; \n#&gt;     am Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  FALSE     18.3      0.785 23.3   &lt;0.001 397.4  16.8   19.9\n#&gt;   TRUE     22.5      0.834 26.9   &lt;0.001 528.6  20.8   24.1\n#&gt; \n#&gt; Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    by = \"cyl\",\n    newdata = datagrid(am = unique, cyl = unique))\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    4     22.9       1.36 16.9   &lt;0.001 209.7  20.2   25.5\n#&gt;    6     19.0       1.07 17.7   &lt;0.001 229.7  16.9   21.1\n#&gt;    8     19.4       1.38 14.1   &lt;0.001 146.6  16.7   22.1\n#&gt; \n#&gt; Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The same results can be achieved using the powerful <code>emmeans</code> package:</p> <pre><code>library(emmeans)\nemmeans(mod, specs = \"cyl\")\n#&gt;  cyl emmean   SE df lower.CL upper.CL\n#&gt;  4     22.9 1.36 27     20.1     25.7\n#&gt;  6     19.0 1.07 27     16.8     21.2\n#&gt;  8     19.4 1.38 27     16.5     22.2\n#&gt; \n#&gt; Results are averaged over the levels of: am \n#&gt; Confidence level used: 0.95\nemmeans(mod, specs = \"am\")\n#&gt;  am    emmean    SE df lower.CL upper.CL\n#&gt;  FALSE   18.3 0.785 27     16.7     19.9\n#&gt;   TRUE   22.5 0.834 27     20.8     24.2\n#&gt; \n#&gt; Results are averaged over the levels of: cyl \n#&gt; Confidence level used: 0.95\n</code></pre>"},{"location":"vignettes/marginalmeans/#marginal-means-vs-average-predictions","title":"Marginal Means vs.\u00a0Average Predictions","text":"<p>What should scientists report? Marginal means or average predictions?</p> <p>Many analysts ask this question, but unfortunately there isn\u2019t a single answer. As explained above, marginal means are a special case of predictions, made on a perfectly balanced grid of categorical predictors, with numeric predictors held at their means, and marginalized with respect to some focal variables. Whether the analyst prefers to report this specific type of marginal means or another kind of average prediction will depend on the characteristics of the sample and the population to which they want to generalize.</p> <p>After reading this vignette and the discussion of <code>emmeans</code> in the Alternative Software vignette, you may want to consult with a statistician to discuss your specific real-world problem and make an informed choice.</p>"},{"location":"vignettes/marginalmeans/#interactions","title":"Interactions","text":"<p>By default, the <code>marginal_means()</code> function calculates marginal means for each categorical predictor one after the other. We can also compute marginal means for combinations of categories by setting <code>cross=TRUE</code>:</p> <pre><code>library(lme4)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\ntitanic &lt;- glmer(\n    Survived ~ Sex * PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n</code></pre> <p>Regardless of the scale of the predictions (<code>type</code> argument), <code>marginal_means()</code> always computes standard errors using the Delta Method:</p> <pre><code>marginal_means(\n    titanic,\n    type = \"response\",\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  Sex    female 0.738     0.0207 35.68   &lt;0.001 923.8 0.698  0.779\n#&gt;  Sex    male   0.235     0.0203 11.62   &lt;0.001 101.3 0.196  0.275\n#&gt;  PClass 1st    0.708     0.0273 25.95   &lt;0.001 490.9 0.654  0.761\n#&gt;  PClass 2nd    0.511     0.0235 21.76   &lt;0.001 346.4 0.465  0.557\n#&gt;  PClass 3rd    0.242     0.0281  8.59   &lt;0.001  56.7 0.187  0.297\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>When the model is linear or on the link scale, it also produces confidence intervals:</p> <pre><code>marginal_means(\n    titanic,\n    type = \"link\",\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value    Mean Std. Error       z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  Sex    female  1.6407      0.206   7.984   &lt;0.001 49.3  1.238  2.043\n#&gt;  Sex    male   -1.3399      0.124 -10.828   &lt;0.001 88.3 -1.582 -1.097\n#&gt;  PClass 1st     1.6307      0.271   6.028   &lt;0.001 29.2  1.100  2.161\n#&gt;  PClass 2nd     0.0997      0.211   0.472    0.637  0.7 -0.314  0.513\n#&gt;  PClass 3rd    -1.2792      0.155  -8.255   &lt;0.001 52.6 -1.583 -0.975\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n</code></pre> <p>It is easy to transform those link-scale marginal means with arbitrary functions using the <code>transform</code> argument:</p> <pre><code>marginal_means(\n    titanic,\n    type = \"link\",\n    transform = insight::link_inverse(titanic),\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  Sex    female 0.838   &lt;0.001 49.3 0.775  0.885\n#&gt;  Sex    male   0.208   &lt;0.001 88.3 0.170  0.250\n#&gt;  PClass 1st    0.836   &lt;0.001 29.2 0.750  0.897\n#&gt;  PClass 2nd    0.525    0.637  0.7 0.422  0.626\n#&gt;  PClass 3rd    0.218   &lt;0.001 52.6 0.170  0.274\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n</code></pre> <p><code>marginal_means()</code> defaults to reporting EMMs for each category individually, without cross-margins:</p> <pre><code>titanic2 &lt;- glmer(\n    Survived ~ Sex + PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n\nmarginal_means(\n    titanic2,\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Term  Value  Mean Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;  Sex    female 0.741     0.0240 30.8   &lt;0.001 691.0 0.694  0.788\n#&gt;  Sex    male   0.253     0.0203 12.5   &lt;0.001 116.0 0.213  0.293\n#&gt;  PClass 1st    0.707     0.0289 24.5   &lt;0.001 436.5 0.650  0.763\n#&gt;  PClass 2nd    0.494     0.0287 17.2   &lt;0.001 217.5 0.437  0.550\n#&gt;  PClass 3rd    0.291     0.0268 10.9   &lt;0.001  88.8 0.238  0.344\n#&gt; \n#&gt; Results averaged over levels of: Sex, PClass \n#&gt; Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can force the cross:</p> <pre><code>marginal_means(\n    titanic2,\n    cross = TRUE,\n    variables = c(\"Sex\", \"PClass\"))\n#&gt; \n#&gt;    Mean Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  0.9288     0.0161 57.71   &lt;0.001   Inf 0.8973 0.9604\n#&gt;  0.7819     0.0357 21.93   &lt;0.001 351.8 0.7120 0.8518\n#&gt;  0.5118     0.0458 11.17   &lt;0.001  93.8 0.4220 0.6017\n#&gt;  0.4844     0.0468 10.35   &lt;0.001  81.0 0.3926 0.5761\n#&gt;  0.2051     0.0308  6.66   &lt;0.001  35.1 0.1448 0.2655\n#&gt;  0.0702     0.0135  5.18   &lt;0.001  22.1 0.0436 0.0967\n#&gt; \n#&gt; Columns: Sex, PClass, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/marginalmeans/#group-averages-with-the-by-argument","title":"Group averages with the <code>by</code> argument","text":"<p>We can collapse marginal means via averaging using the <code>by</code> argument:</p> <pre><code>dat &lt;- mtcars\ndat$am &lt;- factor(dat$am)\ndat$vs &lt;- factor(dat$vs)\ndat$cyl &lt;- factor(dat$cyl)\n\nmod &lt;- glm(gear ~ cyl + vs + am, data = dat, family = poisson)\n\nby &lt;- data.frame(\n    by = c(\"(4 &amp; 6)\", \"(4 &amp; 6)\", \"(8)\"),\n    cyl = c(4, 6, 8))\n\nmarginal_means(mod, by = by, variables = \"cyl\")\n#&gt; \n#&gt;       By Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;  (4 &amp; 6) 3.86   &lt;0.001 59.1  2.86   5.22\n#&gt;  (8)     3.59   &lt;0.001 18.5  2.11   6.13\n#&gt; \n#&gt; Results averaged over levels of: vs, am, cyl \n#&gt; Columns: by, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n</code></pre> <p>And we can use the <code>hypothesis</code> argument to compare those new collapsed subgroups:</p> <pre><code>marginal_means(mod, by = by, variables = \"cyl\", hypothesis = \"pairwise\")\n#&gt; \n#&gt;           Term  Mean Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  (4 &amp; 6) - (8) 0.271       1.39 0.195    0.845 0.2 -2.45      3\n#&gt; \n#&gt; Results averaged over levels of: vs, am, cyl \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/marginalmeans/#custom-contrasts-and-linear-combinations","title":"Custom Contrasts and Linear Combinations","text":"<p>See the vignette on Custom Contrasts and Combinations</p>"},{"location":"vignettes/marginalmeans/#tidy-summaries","title":"Tidy summaries","text":"<p>The <code>summary</code>, <code>tidy</code>, and <code>glance</code> functions are also available to summarize and manipulate the results:</p> <pre><code>mm &lt;- marginal_means(mod)\n\ntidy(mm)\n#&gt; # A tibble: 7 \u00d7 7\n#&gt;   term  value estimate  p.value s.value conf.low conf.high\n#&gt;   &lt;chr&gt; &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 cyl   4         3.83 2.34e- 8    25.4     2.39      6.13\n#&gt; 2 cyl   6         3.90 1.57e-12    39.2     2.67      5.68\n#&gt; 3 cyl   8         3.59 2.66e- 6    18.5     2.11      6.13\n#&gt; 4 vs    0         3.79 1.88e-12    39.0     2.62      5.50\n#&gt; 5 vs    1         3.75 5.82e-10    30.7     2.47      5.69\n#&gt; 6 am    0         3.27 3.96e-15    47.8     2.43      4.40\n#&gt; 7 am    1         4.34 6.83e-19    60.3     3.14      6.01\n\nglance(mm)\n#&gt; # A tibble: 1 \u00d7 7\n#&gt;     aic   bic r2.nagelkerke  rmse  nobs     F logLik   \n#&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;logLik&gt; \n#&gt; 1  113.  120.         0.672 0.437    32 0.737 -51.50168\n\nsummary(mm)\n#&gt; \n#&gt;  Term Value Mean Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;   cyl     4 3.83   &lt;0.001 25.4  2.39   6.13\n#&gt;   cyl     6 3.90   &lt;0.001 39.2  2.67   5.68\n#&gt;   cyl     8 3.59   &lt;0.001 18.5  2.11   6.13\n#&gt;   vs      0 3.79   &lt;0.001 39.0  2.62   5.50\n#&gt;   vs      1 3.75   &lt;0.001 30.7  2.47   5.69\n#&gt;   am      0 3.27   &lt;0.001 47.8  2.43   4.40\n#&gt;   am      1 4.34   &lt;0.001 60.3  3.14   6.01\n#&gt; \n#&gt; Results averaged over levels of: cyl, vs, am \n#&gt; Columns: term, value, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n</code></pre> <p>Thanks to those tidiers, we can also present the results in the style of a regression table using the <code>modelsummary</code> package.</p>"},{"location":"vignettes/marginalmeans/#case-study-multinomial-logit","title":"Case study: Multinomial Logit","text":"<p>To begin, we generate data and estimate a large model:</p> <pre><code>library(nnet)\nlibrary(marginaleffects)\n\nset.seed(1839)\nn &lt;- 1200\nx &lt;- factor(sample(letters[1:3], n, TRUE))\ny &lt;- vector(length = n)\ny[x == \"a\"] &lt;- sample(letters[4:6], sum(x == \"a\"), TRUE)\ny[x == \"b\"] &lt;- sample(letters[4:6], sum(x == \"b\"), TRUE, c(1 / 4, 2 / 4, 1 / 4))\ny[x == \"c\"] &lt;- sample(letters[4:6], sum(x == \"c\"), TRUE, c(1 / 5, 3 / 5, 2 / 5))\n\ndat &lt;- data.frame(x = x, y = factor(y))\ntmp &lt;- as.data.frame(replicate(20, factor(sample(letters[7:9], n, TRUE))))\ndat &lt;- cbind(dat, tmp)\nvoid &lt;- capture.output({\n    mod &lt;- multinom(y ~ ., dat)\n})\n</code></pre> <p>Try to compute marginal means, but realize that your grid won\u2019t fit in memory:</p> <pre><code>marginal_means(mod, type = \"probs\")\n#&gt; Error: You are trying to create a prediction grid with more than 1 billion rows, which is likely to exceed the memory and computational power available on your local machine. Presumably this is because you are considering many variables with many levels. All of the functions in the `marginaleffects` package include arguments to specify a restricted list of variables over which to create a prediction grid.\n</code></pre> <p>Use the <code>variables</code> and <code>variables_grid</code> arguments to compute marginal means over a more reasonably sized grid:</p> <pre><code>marginal_means(mod,\n              type = \"probs\",\n              variables = c(\"x\", \"V1\"),\n              variables_grid = paste0(\"V\", 2:3))\n</code></pre>"},{"location":"vignettes/marginalmeans/#plot-conditional-marginal-means","title":"Plot conditional marginal means","text":"<p>The <code>marginaleffects</code> package offers several functions to plot how some quantities vary as a function of others:</p> <ul> <li><code>plot_predictions</code>: Conditional adjusted predictions \u2013 how does the     predicted outcome change as a function of regressors?</li> <li><code>plot_comparisons</code>: Conditional comparisons \u2013 how do contrasts     change as a function of regressors?</li> <li><code>plot_slopes</code>: Conditional marginal effects \u2013 how does the slope     change as a function of regressors?</li> </ul> <p>There is no analogous function for marginal means. However, it is very easy to achieve a similar effect using the <code>predictions()</code> function, its <code>by</code> argument, and standard plotting functions. In the example below, we take these steps:</p> <ol> <li>Estimate a model with one continuous (<code>hp</code>) and one categorical     regressor (<code>cyl</code>).</li> <li>Create a perfectly \u201cbalanced\u201d data grid for each combination of <code>hp</code>     and <code>cyl</code>. This is specified by the user in the <code>datagrid()</code> call.</li> <li>Compute fitted values (aka \u201cadjusted predictions\u201d) for each cell of     the grid.</li> <li>Use the <code>by</code> argument to take the average of predicted values for     each value of <code>hp</code>, across margins of <code>cyl</code>.</li> <li>Compute standard errors around the averaged predicted values (i.e.,     marginal means).</li> <li>Create symmetric confidence intervals in the usual manner.</li> <li>Plot the results.</li> </ol> <pre><code>library(ggplot2)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\np &lt;- predictions(mod,\n    by = \"hp\",\n    newdata = datagrid(\n        model = mod,\n        hp = seq(100, 120, length.out = 10),\n        cyl = mtcars$cyl))\n\nggplot(p) +\n    geom_ribbon(aes(hp, ymin = conf.low, ymax = conf.high), alpha = .2) +\n    geom_line(aes(hp, estimate))\n</code></pre> <p></p>"},{"location":"vignettes/matching/","title":"Matching","text":"<p>author: \u201cVincent Arel-Bundock\u201d</p> <p>This chapter introduces how to use <code>marginaleffects</code> to estimate treatment effects after pre-processing a dataset to achieve better covariate balance. The presentation is very short. Readers who seek a more comprehensive understanding and application of these methods should refer to Noah Greifer\u2019s excellent and detailed work on the topic and to the <code>MatchIt</code> package vignettes and website</p> <p>The procedure we highlight can be broken down into three steps:</p> <ol> <li>Use <code>MatchIt</code> to pre-process the data and achieve better covariate     balance</li> <li>Fit a regression model to the outcome of interest</li> <li>Use <code>marginaleffects</code> and     G-Computation     to estimate a quantity of interest, such as the Average treatment     effect on the treated (ATT)</li> </ol> <p>To begin, we load libraries and the data from the classic Lalonde experiment:</p> <pre><code>library(\"MatchIt\")\nlibrary(\"marginaleffects\")\ndata(\"lalonde\", package = \"MatchIt\")\n\nhead(lalonde)\n</code></pre> <pre><code>     treat age educ   race married nodegree re74 re75       re78\nNSW1     1  37   11  black       1        1    0    0  9930.0460\nNSW2     1  22    9 hispan       0        1    0    0  3595.8940\nNSW3     1  30   12  black       0        0    0    0 24909.4500\nNSW4     1  27   11  black       0        1    0    0  7506.1460\nNSW5     1  33    8  black       0        1    0    0   289.7899\nNSW6     1  22    9  black       0        1    0    0  4056.4940\n</code></pre> <p>We are interested in the treatment effect of the <code>treat</code> variable on the <code>re78</code> outcome. The <code>treat</code> variable is a binary variable indicating whether the individual received job training. The <code>re78</code> variable is the individual\u2019s earnings in 1978.</p>"},{"location":"vignettes/matching/#matching_1","title":"Matching","text":"<p>The first step is to pre-process the dataset to achieve better covariate balance. To do this, we use the <code>MatchIt::matchit()</code> function and a 1-to-1 nearest neighbor matching with replacement on the Mahaloanobis distance. This function supports many other matching methods, see <code>?matchit</code>.</p> <pre><code>dat &lt;- matchit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75, \n    data = lalonde, distance = \"mahalanobis\",\n    replace = FALSE)\ndat &lt;- match.data(dat)\n</code></pre>"},{"location":"vignettes/matching/#fitting","title":"Fitting","text":"<p>Now, we estimate a linear regression model with interactions between the treatment and covariates. Note that we use the <code>weights</code> argument to use the weights supplied by our matching method:</p> <pre><code>fit &lt;- lm(\n    re78 ~ treat * (age + educ + race + married + nodegree),\n    data = dat,\n    weights = weights)\n</code></pre>"},{"location":"vignettes/matching/#quantity-of-interest","title":"Quantity of interest","text":"<p>Finally, we use the <code>avg_comparisons()</code> function of the <code>marginaleffects</code> package to estimate the ATT and its standard error. In effect, this function applies G-Computation to estimate the quantity of interest. We use the following arguments:</p> <ul> <li><code>variables=\"treat\"</code> indicates that we are interested in the effect     of the <code>treat</code> variable.</li> <li><code>newdata=subset(dat, treat == 1)</code> indicates that we want to estimate     the effect for the treated individuals only (i.e., the ATT).</li> <li><code>wts=\"weights\"</code> indicates that we want to use the weights supplied     by the matching method.</li> </ul> <pre><code>avg_comparisons(\n    fit,\n    variables = \"treat\",\n    newdata = subset(dat, treat == 1),\n    vcov = ~subclass,\n    wts = \"weights\")\n</code></pre> <pre><code>  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat    1 - 0     1221        850 1.44    0.151 2.7  -445   2888\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response\n</code></pre>"},{"location":"vignettes/matching/#learn-more","title":"Learn more","text":"<p>The <code>MatchIt</code> vignette titled \u201cEstimating Effects After Matching\u201d describes many more options, including different measures of uncertainty (bootstrap, clustering, etc.), different estimands (ATE, etc.), and different strategies for adjustment.</p>"},{"location":"vignettes/performance/","title":"Performance","text":""},{"location":"vignettes/performance/#what-to-do-when-marginaleffects-is-slow","title":"What to do when <code>marginaleffects</code> is slow?","text":"<p>Some options:</p> <ol> <li>Compute marginal effects and contrasts at the mean (or other     representative     value)     instead of all observed rows of the original dataset: Use the     <code>newdata</code> argument and the <code>datagrid()</code> function.</li> <li>Compute marginal effects for a subset of variables, paying special     attention to exclude factor variables which can be particularly     costly to process: Use the <code>variables</code> argument.</li> <li>Do not compute standard errors: Use the <code>vcov = FALSE</code> argument.</li> </ol> <p>This simulation illustrates how computation time varies for a model with 25 regressors and 100,000 observations:</p> <pre><code>library(marginaleffects)\n\n## simulate data and fit a large model\nN &lt;- 1e5\ndat &lt;- data.frame(matrix(rnorm(N * 26), ncol = 26))\nmod &lt;- lm(X1 ~ ., dat)\n\nresults &lt;- bench::mark(\n    # marginal effects at the mean; no standard error\n    slopes(mod, vcov = FALSE, newdata = \"mean\"),\n    # marginal effects at the mean\n    slopes(mod, newdata = \"mean\"),\n    # 1 variable; no standard error\n    slopes(mod, vcov = FALSE, variables = \"X3\"),\n    # 1 variable\n    slopes(mod, variables = \"X3\"),\n    # 26 variables; no standard error\n    slopes(mod, vcov = FALSE),\n    # 26 variables\n    slopes(mod),\n    iterations = 1, check = FALSE)\n\nresults[, c(1, 3, 5)]\n## &lt;bch:expr&gt;                                  &lt;bch:tm&gt; &lt;bch:byt&gt;\n## slopes(mod, vcov = FALSE, newdata = \"mean\") 230.09ms    1.24GB\n## slopes(mod, newdata = \"mean\")               329.14ms    1.25GB\n## slopes(mod, vcov = FALSE, variables = \"X3\")  198.7ms  496.24MB\n## slopes(mod, variables = \"X3\")                  1.27s    3.29GB\n## slopes(mod, vcov = FALSE)                      5.73s   11.05GB\n## slopes(mod)                                   21.68s   78.02GB\n</code></pre> <p>The benchmarks above were conducted using the development version of <code>marginaleffects</code> on 2023-02-03.</p>"},{"location":"vignettes/performance/#speed-comparison","title":"Speed comparison","text":"<p>The <code>slopes</code> function is relatively fast. This simulation was conducted using the development version of the package on 2022-04-04:</p> <pre><code>library(margins)\n\nN &lt;- 1e3\ndat &lt;- data.frame(\n    y = sample(0:1, N, replace = TRUE),\n    x1 = rnorm(N),\n    x2 = rnorm(N),\n    x3 = rnorm(N),\n    x4 = factor(sample(letters[1:5], N, replace = TRUE)))\nmod &lt;- glm(y ~ x1 + x2 + x3 + x4, data = dat, family = binomial)\n</code></pre> <p><code>marginaleffects</code> is about the same speed as <code>margins</code> when unit-level standard errors are not computed:</p> <pre><code>results &lt;- bench::mark(\n    slopes(mod, vcov = FALSE),\n    margins(mod, unit_ses = FALSE),\n    check = FALSE, relative = TRUE)\nresults[, c(1, 3, 5)]\n\n##   expression                        median mem_alloc\n##   &lt;bch:expr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;\n## 1 slopes(mod, vcov = FALSE)   1         1\n## 2 margins(mod, unit_ses = FALSE)       6.15      4.17\n</code></pre> <p><code>marginaleffects</code> can be 100x times faster than <code>margins</code> when unit-level standard errors are computed:</p> <pre><code>results &lt;- bench::mark(\n    slopes(mod, vcov = TRUE),\n    margins(mod, unit_ses = TRUE),\n    check = FALSE, relative = TRUE, iterations = 1)\nresults[, c(1, 3, 5)]\n\n## &lt;bch:expr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;\n## slopes(mod, vcov = TRUE)          1        1  \n## margins(mod, unit_ses = TRUE)   128.      18.6\n</code></pre> <p>Models estimated on larger datasets (&gt; 1000 observations) can be difficult to process using the <code>margins</code> package, because of memory and time constraints. In contrast, <code>marginaleffects</code> can work well on much larger datasets.</p> <p>In some cases, <code>marginaleffects</code> will be considerably slower than packages like <code>emmeans</code> or <code>modmarg</code>. This is because these packages make extensive use of hard-coded analytical derivatives, or reimplement their own fast prediction functions.</p>"},{"location":"vignettes/plot/","title":"Plots","text":"<p>The <code>marginaleffects</code> package includes three flexible functions to plot estimates and display interactions.</p> <ul> <li><code>plot_predictions()</code></li> <li><code>plot_comparisons()</code></li> <li><code>plot_slopes()</code></li> </ul> <p>Those functions can be used to plot two kinds of quantities:</p> <ul> <li>Conditional estimates:<ul> <li>Estimates computed on a substantively meaningful grid of     predictor values.</li> <li>This is analogous to using the <code>newdata</code> argument with the     <code>datagrid()</code> function in a <code>predictions()</code>, <code>comparisons()</code>, or     <code>slopes()</code> call.</li> </ul> </li> <li>Marginal estimates:<ul> <li>Estimates computed on the original data, but averaged by     subgroup.</li> <li>This is analogous to using the <code>newdata</code> argument with the     <code>datagrid()</code> function in a <code>predictions()</code>, <code>comparisons()</code>, or     <code>slopes()</code> call.</li> </ul> </li> </ul> <p>To begin, let\u2019s download data and fit a model:</p> <pre><code>## libraries\nlibrary(ggplot2)\nlibrary(patchwork) # combine plots with the + and / signs\nlibrary(marginaleffects)\n\n## visual theme\ntheme_set(theme_minimal())\nokabeito &lt;- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\noptions(ggplot2.discrete.colour = okabeito)\noptions(width = 1000)\n\n## download data\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nmod &lt;- lm(body_mass_g ~ flipper_length_mm * species * bill_length_mm + island, data = dat)\n</code></pre>"},{"location":"vignettes/plot/#predictions","title":"Predictions","text":""},{"location":"vignettes/plot/#conditional-predictions","title":"Conditional predictions","text":"<p>We call a prediction \u201cconditional\u201d when it is made on a grid of user-specified values. For example, we predict penguins\u2019 body mass for different values of flipper length and species:</p> <pre><code>pre &lt;- predictions(mod, newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\npre\n#&gt; \n#&gt;  flipper_length_mm   species Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;                172 Adelie        3859        204 18.9   &lt;0.001 263.0  3460   4259\n#&gt;                172 Chinstrap     3146        234 13.5   &lt;0.001 134.6  2688   3604\n#&gt;                172 Gentoo        2545        369  6.9   &lt;0.001  37.5  1822   3268\n#&gt;                231 Adelie        4764        362 13.2   &lt;0.001 128.9  4054   5474\n#&gt;                231 Chinstrap     4086        469  8.7   &lt;0.001  58.1  3166   5006\n#&gt;                231 Gentoo        5597        155 36.0   &lt;0.001 940.9  5292   5901\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, body_mass_g, bill_length_mm, island, flipper_length_mm, species \n#&gt; Type:  response\n</code></pre> <p>The <code>condition</code> argument of the <code>plot_predictions()</code> function can be used to build meaningful grids of predictor values somewhat more easily:</p> <pre><code>plot_predictions(mod, condition = c(\"flipper_length_mm\", \"species\"))\n</code></pre> <p></p> <p>Note that the values at each end of the x-axis correspond to the numerical results produced above. For example, the predicted outcome for a Gentoo with 231mm flippers is 5597.</p> <p>We can include a 3rd conditioning variable, specify what values we want to consider, supply <code>R</code> functions to compute summaries, and use one of several string shortcuts for common reference values (\u201cthreenum\u201d, \u201cminmax\u201d, \u201cquartile\u201d, etc.):</p> <pre><code>plot_predictions(\n    mod,\n    condition = list(\n        \"flipper_length_mm\" = 180:220,\n        \"bill_length_mm\" = \"threenum\",\n        \"species\" = unique))\n</code></pre> <p></p> <p>See <code>?plot_predictions</code> for more information.</p>"},{"location":"vignettes/plot/#marginal-predictions","title":"Marginal predictions","text":"<p>We call a prediction \u201cmarginal\u201d when it is the result of a two step process: (1) make predictions for each observed unit in the original dataset, and (2) average predictions across one or more categorical predictors. For example:</p> <pre><code>predictions(mod, by = \"species\")\n#&gt; \n#&gt;    species Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Adelie        3701       27.2 136.1   &lt;0.001 Inf  3647   3754\n#&gt;  Chinstrap     3733       40.5  92.2   &lt;0.001 Inf  3654   3812\n#&gt;  Gentoo        5076       30.1 168.5   &lt;0.001 Inf  5017   5135\n#&gt; \n#&gt; Columns: species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>We can plot those predictions by using the analogous command:</p> <pre><code>plot_predictions(mod, by = \"species\")\n</code></pre> <p></p> <p>We can also make predictions at the intersections of different variables:</p> <pre><code>predictions(mod, by = c(\"species\", \"island\"))\n#&gt; \n#&gt;    species    island Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  Adelie    Biscoe        3710       50.4  73.7   &lt;0.001 Inf  3611   3808\n#&gt;  Adelie    Dream         3688       44.6  82.6   &lt;0.001 Inf  3601   3776\n#&gt;  Adelie    Torgersen     3706       46.8  79.2   &lt;0.001 Inf  3615   3798\n#&gt;  Chinstrap Dream         3733       40.5  92.2   &lt;0.001 Inf  3654   3812\n#&gt;  Gentoo    Biscoe        5076       30.1 168.5   &lt;0.001 Inf  5017   5135\n#&gt; \n#&gt; Columns: species, island, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Note that certain species only live on certain islands. Visually:</p> <pre><code>plot_predictions(mod, by = c(\"species\", \"island\"))\n</code></pre> <p></p>"},{"location":"vignettes/plot/#comparisons","title":"Comparisons","text":""},{"location":"vignettes/plot/#conditional-comparisons","title":"Conditional comparisons","text":"<p>The syntax for conditional comparisons is the same as the syntax for conditional predictions, except that we now need to specify the variable(s) of interest using an additional argument:</p> <pre><code>comparisons(mod,\n  variables = \"flipper_length_mm\",\n  newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\n#&gt; \n#&gt;               Term Contrast flipper_length_mm   species Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 % bill_length_mm island\n#&gt;  flipper_length_mm       +1               172 Adelie        15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               172 Chinstrap     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               172 Gentoo        51.7       8.70 5.95   &lt;0.001 28.5 34.68   68.8           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Adelie        15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Chinstrap     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe\n#&gt;  flipper_length_mm       +1               231 Gentoo        51.7       8.70 5.95   &lt;0.001 28.5 34.68   68.8           43.9 Biscoe\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, flipper_length_mm, species, predicted_lo, predicted_hi, predicted, body_mass_g, bill_length_mm, island \n#&gt; Type:  response\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  condition = c(\"bill_length_mm\", \"species\"))\n</code></pre> <p></p> <p>We can specify custom comparisons, as we would using the <code>variables</code> argument of the <code>comparisons()</code> function. For example, see what happens to the predicted outcome when <code>flipper_length_mm</code> increases by 1 standard deviation or by 10mm:</p> <pre><code>plot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = \"sd\"),\n  condition = c(\"bill_length_mm\", \"species\")) +\n\nplot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = 10),\n  condition = c(\"bill_length_mm\", \"species\"))\n</code></pre> <p></p> <p>Notice that the vertical scale is different in the plots above, reflecting the fact that we are plotting the effect of a change of 1 standard deviation on the left vs 10 units on the right.</p> <p>Like the <code>comparisons()</code> function, <code>plot_comparisons()</code> is a very powerful tool because it allows us to compute and display custom comparisons such as differences, ratios, odds, lift, and arbitrary functions of predicted outcomes. For example, if we want to plot the ratio of predicted body mass for different species of penguins, we could do:</p> <pre><code>plot_comparisons(mod,\n  variables = \"species\",\n  condition = \"bill_length_mm\",\n  comparison = \"ratio\")\n</code></pre> <p></p> <p>The left panel shows that the ratio of Chinstrap body mass to Adelie body mass is approximately constant, at slightly above 0.8. The right panel shows that the ratio of Gentoo to Adelie body mass is depends on their bill length. For birds with short bills, Gentoos seem to have smaller body mass than Adelies. For birds with long bills, Gentoos seem heavier than Adelies, although the null ratio (1) is not outside the confidence interval.</p>"},{"location":"vignettes/plot/#marginal-comparisons","title":"Marginal comparisons","text":"<p>As above, we can also display marginal comparisons, by subgroups:</p> <pre><code>plot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = \"species\") +\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = c(\"species\", \"island\"))\n</code></pre> <p></p> <p>Multiple contrasts at once:</p> <pre><code>plot_comparisons(mod,\n  variables = c(\"flipper_length_mm\", \"bill_length_mm\"),\n  by = c(\"species\", \"island\"))\n</code></pre> <p></p>"},{"location":"vignettes/plot/#slopes","title":"Slopes","text":"<p>If you have read the sections above, the behavior of the <code>plot_slopes()</code> function should not surprise. Here we give two examples in which we compute display the elasticity of body mass with respect to bill length:</p> <pre><code>## conditional\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  condition = c(\"species\", \"island\"))\n</code></pre> <p></p> <pre><code>## marginal\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  by = c(\"species\", \"island\"))\n</code></pre> <p></p> <p>And here is an example of a marginal effects (aka \u201cslopes\u201d or \u201cpartial derivatives\u201d) plot for a model with multiplicative interactions between continuous variables:</p> <pre><code>mod2 &lt;- lm(mpg ~ wt * qsec * factor(gear), data = mtcars)\n\nplot_slopes(mod2, variables = \"qsec\", condition = c(\"wt\", \"gear\"))\n</code></pre> <p></p>"},{"location":"vignettes/plot/#uncertainty-estimates","title":"Uncertainty estimates","text":"<p>As with all the other functions in the package, the <code>plot_*()</code> functions have a <code>conf_level</code> argument and a <code>vcov</code> argument which can be used to control the size of confidence intervals and the types of standard errors used:</p> <pre><code>plot_slopes(mod,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## clustered standard errors\nplot_slopes(mod,\n  vcov = ~island,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## alpha level\nplot_slopes(mod,\n  conf_level = .8,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200))\n</code></pre> <p></p>"},{"location":"vignettes/plot/#customization","title":"Customization","text":"<p>A very useful feature of the plotting functions in this package is that they produce normal <code>ggplot2</code> objects. So we can customize them to our heart\u2019s content, using <code>ggplot2</code> itself, or one of the many packages designed to augment its functionalities:</p> <pre><code>library(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp * factor(cyl), data = mt)\n\nplot_predictions(mod, condition = c(\"hp\", \"cyl\"), points = .5, rug = TRUE, vcov = FALSE) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n</code></pre> <p></p> <p>All the plotting functions work with all the model supported by the <code>marginaleffects</code> package, so we can plot the output of a logistic regression model. This plot shows the probability of survival aboard the Titanic, for different ages and different ticket classes:</p> <pre><code>library(ggdist)\nlibrary(ggplot2)\n\ndat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\n\nmod &lt;- glm(Survived ~ Age * SexCode * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_dots(\n        alpha = .8,\n        scale = .3,\n        pch = 18,\n        data = dat, aes(\n        x = Age,\n        y = Survived,\n        side = ifelse(Survived == 1, \"bottom\", \"top\")))\n</code></pre> <p></p> <p>Thanks to Andrew Heiss who inspired this plot.</p> <p>Designing effective data visualizations requires a lot of customization to the specific context and data. The plotting functions in <code>marginaleffects</code> offer a powerful way to iterate quickly between plots and models, but they obviously cannot support all the features that users may want. Thankfully, it is very easy to use the <code>slopes</code> functions to generate datasets that can then be used in <code>ggplot2</code> or any other data visualization tool. Just use the <code>draw</code> argument:</p> <pre><code>p &lt;- plot_predictions(mod, condition = c(\"Age\", \"PClass\"), draw = FALSE)\nhead(p)\n#&gt;   rowid  estimate      p.value   s.value  conf.low conf.high  Survived   SexCode     Age PClass\n#&gt; 1     1 0.8679723 0.0013307148  9.553583 0.6754794 0.9540527 0.4140212 0.3809524 0.17000    1st\n#&gt; 2     2 0.8956789 0.0001333343 12.872665 0.7401973 0.9627887 0.4140212 0.3809524 0.17000    2nd\n#&gt; 3     3 0.4044513 0.2667759176  1.906300 0.2554245 0.5734603 0.4140212 0.3809524 0.17000    3rd\n#&gt; 4     4 0.8631027 0.0011563592  9.756195 0.6749549 0.9503543 0.4140212 0.3809524 1.61551    1st\n#&gt; 5     5 0.8813224 0.0001728858 12.497893 0.7228530 0.9548415 0.4140212 0.3809524 1.61551    2nd\n#&gt; 6     6 0.3934924 0.1899483119  2.396321 0.2535791 0.5533716 0.4140212 0.3809524 1.61551    3rd\n</code></pre> <p>This allows us to feed the data easily to other functions, such as those in the useful <code>ggdist</code> and <code>distributional</code> packages:</p> <pre><code>library(ggdist)\nlibrary(distributional)\nplot_slopes(mod, variables = \"SexCode\", condition = c(\"Age\", \"PClass\"), type = \"link\", draw = FALSE) |&gt;\n  ggplot() +\n  stat_lineribbon(aes(\n    x = Age,\n    ydist = dist_normal(mu = estimate, sigma = std.error),\n    fill = PClass),\n    alpha = 1 / 4)\n</code></pre> <p></p>"},{"location":"vignettes/plot/#fits-and-smooths","title":"Fits and smooths","text":"<p>We can compare the model predictors with fits and smoothers using the <code>geom_smooth()</code> function from the <code>ggplot2</code> package:</p> <pre><code>dat &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat &lt;- read.csv(dat)\nmod &lt;- glm(Survived ~ Age * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_smooth(data = dat, aes(Age, Survived), method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(data = dat, aes(Age, Survived), se = FALSE, color = \"black\")\n</code></pre> <p></p>"},{"location":"vignettes/plot/#groups-and-categorical-outcomes","title":"Groups and categorical outcomes","text":"<p>In some models, <code>marginaleffects</code> functions generate different estimates for different groups, such as categorical outcomes. For example,</p> <pre><code>library(MASS)\nmod &lt;- polr(factor(gear) ~ mpg + hp, data = mtcars)\n\npredictions(mod)\n#&gt; \n#&gt;  Group Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;      3   0.5316     0.1127 4.72   &lt;0.001 18.7  0.3107  0.753\n#&gt;      3   0.5316     0.1127 4.72   &lt;0.001 18.7  0.3107  0.753\n#&gt;      3   0.4492     0.1200 3.74   &lt;0.001 12.4  0.2140  0.684\n#&gt;      3   0.4944     0.1111 4.45   &lt;0.001 16.8  0.2765  0.712\n#&gt;      3   0.4213     0.1142 3.69   &lt;0.001 12.1  0.1973  0.645\n#&gt; --- 86 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;      5   0.6894     0.1957 3.52   &lt;0.001 11.2  0.3059  1.073\n#&gt;      5   0.1650     0.1290 1.28   0.2009  2.3 -0.0878  0.418\n#&gt;      5   0.1245     0.0698 1.78   0.0744  3.7 -0.0123  0.261\n#&gt;      5   0.3779     0.3243 1.17   0.2439  2.0 -0.2578  1.014\n#&gt;      5   0.0667     0.0458 1.46   0.1455  2.8 -0.0231  0.157\n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp \n#&gt; Type:  probs\n</code></pre> <p>We can plot those estimates in the same way as before, by specifying <code>group</code> as one of the conditional variable, or by adding that column to a <code>facet_wrap()</code> call:</p> <pre><code>plot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\", vcov = FALSE)\n</code></pre> <p></p> <pre><code>plot_predictions(mod, condition = \"mpg\", type = \"probs\", vcov = FALSE) +\n  facet_wrap(~ group)\n</code></pre> <p></p>"},{"location":"vignettes/plot/#plot-and-marginaleffects-objects","title":"<code>plot()</code> and <code>marginaleffects</code> objects","text":"<p>Some users may feel inclined to call <code>plot()</code> on a object produced by <code>marginaleffects</code> object. Doing so will generate an informative error like this one:</p> <pre><code>mod &lt;- lm(mpg ~ hp * wt * factor(cyl), data = mtcars)\np &lt;- predictions(mod)\nplot(p)\n#&gt; Error: Please use the `plot_predictions()` function.\n</code></pre> <p>The reason for this error is that the user query is underspecified. <code>marginaleffects</code> allows users to compute so many quantities of interest that it is not clear what the user wants when they simply call <code>plot()</code>. Adding several new arguments would compete with the main plotting functions, and risk sowing confusion. The <code>marginaleffects</code> developers thus decided to support one main path to plotting: <code>plot_predictions()</code>, <code>plot_comparisons()</code>, and <code>plot_slopes()</code>.</p> <p>That said, it may be useful to remind users that all <code>marginaleffects</code> output are standard \u201ctidy\u201d data frames. Although they get pretty-printed to the console, all the listed columns are accessible via standard <code>R</code> operators. For example:</p> <pre><code>p &lt;- avg_predictions(mod, by = \"cyl\")\np\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    4     26.7      0.695 38.4   &lt;0.001   Inf  25.3   28.0\n#&gt;    6     19.7      0.871 22.7   &lt;0.001 375.1  18.0   21.5\n#&gt;    8     15.1      0.616 24.5   &lt;0.001 438.2  13.9   16.3\n#&gt; \n#&gt; Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\np$estimate\n#&gt; [1] 26.66364 19.74286 15.10000\n\np$std.error\n#&gt; [1] 0.6951236 0.8713835 0.6161612\n\np$conf.low\n#&gt; [1] 25.30122 18.03498 13.89235\n</code></pre> <p>This allows us to plot all results very easily with standard plotting functions:</p> <pre><code>plot_predictions(mod, by = \"cyl\")\n</code></pre> <p></p> <pre><code>plot(p$cyl, p$estimate)\n</code></pre> <p></p> <pre><code>ggplot(p, aes(x = cyl, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange()\n</code></pre> <p></p>"},{"location":"vignettes/predictions/","title":"Predictions","text":""},{"location":"vignettes/predictions/#predictions","title":"Predictions","text":"<p>In the context of this package, an \u201cAdjusted Prediction\u201d is defined as:</p> <p>The outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \u201creference grid\u201d).</p> <p>Here, the word \u201cAdjusted\u201d simply means \u201cmodel-derived\u201d or \u201cmodel-based.\u201d</p>"},{"location":"vignettes/predictions/#prediction-type-or-scale","title":"Prediction type (or scale)","text":"<p>Using the <code>type</code> argument of the <code>predictions()</code> function we can specify the \u201cscale\u201d on which to make predictions. This refers to either the scale used to estimate the model (i.e., link scale) or to a more interpretable scale (e.g., response scale). For example, when fitting a linear regression model using the <code>lm()</code> function, the link scale and the response scale are identical. An \u201cAdjusted Prediction\u201d computed on either scale will be expressed as the mean value of the response variable at the given values of the predictor variables.</p> <p>On the other hand, when fitting a binary logistic regression model using the <code>glm()</code> function (which uses a binomial family and a logit link ), the link scale and the response scale will be different: an \u201cAdjusted Prediction\u201d computed on the link scale will be expressed as a log odds of a \u201csuccessful\u201d response at the given values of the predictor variables, whereas an \u201cAdjusted Prediction\u201d computed on the response scale will be expressed as a probability that the response variable equals 1.</p> <p>The default value of the <code>type</code> argument for most models is \u201cresponse\u201d, which means that the <code>predictions()</code> function will compute predicted probabilities (binomial family), Poisson means (poisson family), etc.</p>"},{"location":"vignettes/predictions/#prediction-grid","title":"Prediction grid","text":"<p>To compute adjusted predictions we must first specify the values of the predictors to consider: a \u201creference grid.\u201d For example, if our model is a linear model fitted with the lm() function which relates the response variable Happiness with the predictor variables Age, Gender and Income, the reference grid could be a <code>data.frame</code> with values for Age, Gender and Income: Age = 40, Gender = Male, Income = 60000.</p> <p>The \u201creference grid\u201d may or may not correspond to actual observations in the dataset used to fit the model; the example values given above could match the mean values of each variable, or they could represent a specific observed (or hypothetical) individual. The reference grid can include many different rows if we want to make predictions for different combinations of predictors. By default, the <code>predictions()</code> function uses the full original dataset as a reference grid, which means it will compute adjusted predictions for each of the individuals observed in the dataset that was used to fit the model.</p>"},{"location":"vignettes/predictions/#the-predictions-function","title":"The <code>predictions()</code> function","text":"<p>By default, <code>predictions</code> calculates the regression-adjusted predicted values for every observation in the original dataset:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\npred &lt;- predictions(mod)\n\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      26.4      0.962 27.5   &lt;0.001 549.0  24.5   28.3\n#&gt;      20.0      1.204 16.6   &lt;0.001 204.1  17.7   22.4\n#&gt;      15.9      0.992 16.0   &lt;0.001 190.0  14.0   17.9\n#&gt;      20.2      1.219 16.5   &lt;0.001 201.8  17.8   22.5\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n</code></pre> <p>In many cases, this is too limiting, and researchers will want to specify a grid of \u201ctypical\u201d values over which to compute adjusted predictions.</p>"},{"location":"vignettes/predictions/#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr","title":"Adjusted Predictions at User-Specified values (aka Adjusted Predictions at Representative values, APR)","text":"<p>There are two main ways to select the reference grid over which we want to compute adjusted predictions. The first is using the <code>variables</code> argument. The second is with the <code>newdata</code> argument and the <code>datagrid()</code> function</p>"},{"location":"vignettes/predictions/#variables-counterfactual-predictions","title":"<code>variables</code>: Counterfactual predictions","text":"<p>The <code>variables</code> argument is a handy way to create and make predictions on counterfactual datasets. For example, here the dataset that we used to fit the model has 32 rows. The counterfactual dataset with two distinct values of <code>hp</code> has 64 rows: each of the original rows appears twice, that is, once with each of the values that we specified in the <code>variables</code> argument:</p> <pre><code>p &lt;- predictions(mod, variables = list(hp = c(100, 120)))\nhead(p)\n#&gt; \n#&gt;  cyl  hp Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    4 100     26.2      0.986 26.63   &lt;0.001 516.6  24.3   28.2\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt;    8 100     17.7      1.881  9.42   &lt;0.001  67.6  14.0   21.4\n#&gt;    6 100     20.3      1.238 16.38   &lt;0.001 198.0  17.9   22.7\n#&gt; \n#&gt; Columns: rowid, rowidcf, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp \n#&gt; Type:  response\nnrow(p)\n#&gt; [1] 64\n</code></pre>"},{"location":"vignettes/predictions/#newdata-and-datagrid","title":"<code>newdata</code> and <code>datagrid</code>","text":"<p>A second strategy to construct grids of predictors for adjusted predictions is to combine the <code>newdata</code> argument and the <code>datagrid</code> function. Recall that this function creates a \u201ctypical\u201d dataset with all variables at their means or modes, except those we explicitly define:</p> <pre><code>datagrid(cyl = c(4, 6, 8), model = mod)\n#&gt;        mpg       hp cyl\n#&gt; 1 20.09062 146.6875   4\n#&gt; 2 20.09062 146.6875   6\n#&gt; 3 20.09062 146.6875   8\n</code></pre> <p>We can also use this <code>datagrid</code> function in a <code>predictions</code> call (omitting the <code>model</code> argument):</p> <pre><code>predictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp cyl\n#&gt;      16.6       1.28 13   &lt;0.001 125.6  14.1   19.1 147   8\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n\npredictions(mod, newdata = datagrid(cyl = c(4, 6, 8)))\n#&gt; \n#&gt;  cyl Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp\n#&gt;    4     25.1       1.37 18.4   &lt;0.001 247.5  22.4   27.8 147\n#&gt;    6     19.2       1.25 15.4   &lt;0.001 174.5  16.7   21.6 147\n#&gt;    8     16.6       1.28 13.0   &lt;0.001 125.6  14.1   19.1 147\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n#&gt; Type:  response\n</code></pre> <p>Users can change the summary function used to summarize each type of variables using the <code>FUN_numeric</code>, <code>FUN_factor</code>, and related arguments. For example:</p> <pre><code>m &lt;- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %  hp drat cyl am\n#&gt;      22.0       1.29 17.0   &lt;0.001 214.0  19.4   24.5 123  3.7   6  1\n#&gt;      18.2       1.27 14.3   &lt;0.001 151.9  15.7   20.7 123  3.7   6  0\n#&gt;      25.5       1.32 19.3   &lt;0.001 274.0  23.0   28.1 123  3.7   4  1\n#&gt;      21.8       1.54 14.1   &lt;0.001 148.3  18.8   24.8 123  3.7   4  0\n#&gt;      22.6       2.14 10.6   &lt;0.001  84.2  18.4   26.8 123  3.7   8  1\n#&gt;      18.9       1.73 10.9   &lt;0.001  89.0  15.5   22.3 123  3.7   8  0\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \n#&gt; Type:  response\n</code></pre> <p>The <code>data.frame</code> produced by <code>predictions</code> is \u201ctidy\u201d, which makes it easy to manipulate with other <code>R</code> packages and functions:</p> <pre><code>library(kableExtra)\nlibrary(tidyverse)\n\npredictions(\n    mod,\n    newdata = datagrid(cyl = mtcars$cyl, hp = c(100, 110))) |&gt;\n    select(hp, cyl, estimate) |&gt;\n    pivot_wider(values_from = estimate, names_from = cyl) |&gt;\n    kbl(caption = \"A table of Adjusted Predictions\") |&gt;\n    kable_styling() |&gt;\n    add_header_above(header = c(\" \" = 1, \"cyl\" = 3))\n</code></pre> A table of Adjusted Predictions  cyl  hp 4 6 8 100 26.24623 20.27858 17.72538 110 26.00585 20.03819 17.48500 <p>A table of Adjusted Predictions</p>"},{"location":"vignettes/predictions/#counterfactual-data-grid","title":"<code>counterfactual</code> data grid","text":"<p>An alternative approach to construct grids of predictors is to use <code>grid_type = \"counterfactual\"</code> argument value. This will duplicate the whole dataset, with the different values specified by the user.</p> <p>For example, the <code>mtcars</code> dataset has 32 rows. This command produces a new dataset with 64 rows, with each row of the original dataset duplicated with the two values of the <code>am</code> variable supplied (0 and 1):</p> <pre><code>mod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\nnd &lt;- datagrid(model = mod, am = 0:1, grid_type = \"counterfactual\")\n\ndim(nd)\n#&gt; [1] 64  4\n</code></pre> <p>Then, we can use this dataset and the <code>predictions</code> function to create interesting visualizations:</p> <pre><code>pred &lt;- predictions(mod, newdata = datagrid(am = 0:1, grid_type = \"counterfactual\")) |&gt;\n    select(am, estimate, rowidcf) |&gt;\n    pivot_wider(id_cols = rowidcf, \n                names_from = am,\n                values_from = estimate)\n\nggplot(pred, aes(x = `0`, y = `1`)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1) +\n    labs(x = \"Predicted Pr(vs=1), when am = 0\",\n         y = \"Predicted Pr(vs=1), when am = 1\")\n</code></pre> <p></p> <p>In this graph, each dot represents the predicted probability that <code>vs=1</code> for one observation of the dataset, in the counterfactual worlds where <code>am</code> is either 0 or 1.</p>"},{"location":"vignettes/predictions/#adjusted-prediction-at-the-mean-apm","title":"Adjusted Prediction at the Mean (APM)","text":"<p>Some analysts may want to calculate an \u201cAdjusted Prediction at the Mean,\u201d that is, the predicted outcome when all the regressors are held at their mean (or mode). To achieve this, we use the <code>datagrid</code> function. By default, this function produces a grid of data with regressors at their means or modes, so all we need to do to get the APM is:</p> <pre><code>predictions(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n</code></pre> <p>This is equivalent to calling:</p> <pre><code>predictions(mod, newdata = datagrid())\n#&gt; \n#&gt;  Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %  hp    am\n#&gt;    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#&gt; \n#&gt; Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am \n#&gt; Type:  invlink(link)\n</code></pre>"},{"location":"vignettes/predictions/#average-adjusted-predictions-aap","title":"Average Adjusted Predictions (AAP)","text":"<p>An \u201cAverage Adjusted Prediction\u201d is the outcome of a two step process:</p> <ol> <li>Create a new dataset with each of the original regressor values, but     fixing some regressors to values of interest.</li> <li>Take the average of the predicted values in this new dataset.</li> </ol> <p>We can obtain AAPs by applying the <code>avg_*()</code> functions or <code>by</code> argument:</p> <pre><code>modlin &lt;- lm(mpg ~ hp + factor(cyl), mtcars)\navg_predictions(modlin)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      20.1      0.556 36.1   &lt;0.001 946.7    19   21.2\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>This is equivalent to:</p> <pre><code>pred &lt;- predictions(modlin)\nmean(pred$estimate)\n#&gt; [1] 20.09062\n</code></pre> <p>Note that in GLM models with a non-linear link function, the default <code>type</code> is <code>invlink(link)</code>. This means that predictions are first made on the link scale, averaged, and then back transformed. Thus, the average prediction may not be exactly identical to the average of predictions:</p> <pre><code>mod &lt;- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\navg_predictions(mod)$estimate\n#&gt; [1] 0.06308965\n\n## Step 1: predict on the link scale\np &lt;- predictions(mod, type = \"link\")$estimate\n## Step 2: average\np &lt;- mean(p)\n## Step 3: backtransform\nmod$family$linkinv(p)\n#&gt; [1] 0.06308965\n</code></pre> <p>Users who want the average of individual-level predictions on the response scale can specify the <code>type</code> argument explicitly:</p> <pre><code>avg_predictions(mod, type = \"response\")\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;     0.437     0.0429 10.2   &lt;0.001 78.8 0.353  0.522\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/predictions/#average-adjusted-predictions-by-group","title":"Average Adjusted Predictions by Group","text":"<p>We can compute average adjusted predictions for different subsets of the data with the <code>by</code> argument.</p> <pre><code>predictions(mod, by = \"am\")\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S   2.5 % 97.5 %\n#&gt;   0   0.0591   0.1163 3.1 0.00198  0.665\n#&gt;   1   0.0694   0.0755 3.7 0.00424  0.566\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n</code></pre> <p>In the next example, we create a \u201ccounterfactual\u201d data grid where each observation of the dataset is repeated twice, with different values of the <code>am</code> variable, and all other variables held at the observed values. We also show the equivalent results using <code>dplyr</code>:</p> <pre><code>predictions(\n    mod,\n    type = \"response\",\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;   0    0.526     0.0330 15.93   &lt;0.001 187.3 0.461  0.591\n#&gt;   1    0.330     0.0646  5.11   &lt;0.001  21.6 0.204  0.457\n#&gt; \n#&gt; Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\npredictions(\n    mod,\n    type = \"response\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mean(estimate))\n#&gt; # A tibble: 2 \u00d7 2\n#&gt;      am   AAP\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     0 0.526\n#&gt; 2     1 0.330\n</code></pre> <p>Note that the two results are exactly identical when we specify <code>type=\"response\"</code> explicitly. However, they will differ slightly when we leave <code>type</code> unspecified, because <code>marginaleffects</code> will then automatically make predictions and average on the link scale, before backtransforming (<code>\"invlink(link)\"</code>):</p> <pre><code>predictions(\n    mod,\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#&gt; \n#&gt;  am Estimate Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;   0  0.24043   0.3922 1.4 2.22e-02  0.815\n#&gt;   1  0.00696   0.0359 4.8 6.81e-05  0.419\n#&gt; \n#&gt; Columns: am, estimate, p.value, s.value, conf.low, conf.high \n#&gt; Type:  invlink(link)\n\npredictions(\n    mod,\n    type = \"link\",\n    newdata = datagridcf(am = 0:1)) |&gt;\n    group_by(am) |&gt;\n    summarize(AAP = mod$family$linkinv(mean(estimate)))\n#&gt; # A tibble: 2 \u00d7 2\n#&gt;      am     AAP\n#&gt;   &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1     0 0.240  \n#&gt; 2     1 0.00696\n</code></pre>"},{"location":"vignettes/predictions/#multinomial-models","title":"Multinomial models","text":"<p>One place where this is particularly useful is in multinomial models with different response levels. For example, here we compute the average predicted outcome for each outcome level in a multinomial logit model. Note that response levels are identified by the \u201cgroup\u201d column.</p> <pre><code>library(nnet)\nnom &lt;- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n## first 5 raw predictions\npredictions(nom, type = \"probs\") |&gt; head()\n#&gt; \n#&gt;  Group Estimate Std. Error        z Pr(&gt;|z|)   S     2.5 %   97.5 %\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#&gt;      3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n#&gt;      3 4.04e-01   1.97e-01   2.0567   0.0397 4.7  1.90e-02 7.90e-01\n#&gt;      3 1.00e+00   1.25e-03 802.4784   &lt;0.001 Inf  9.98e-01 1.00e+00\n#&gt;      3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n#&gt; \n#&gt; Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \n#&gt; Type:  probs\n\n## average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n#&gt; \n#&gt;  Group Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;      3    0.469     0.0404 11.60   &lt;0.001 100.9 0.3895  0.548\n#&gt;      4    0.375     0.0614  6.11   &lt;0.001  29.9 0.2546  0.495\n#&gt;      5    0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre> <p>We can use custom aggregations by supplying a data frame to the <code>by</code> argument. All columns of this data frame must be present in the output of <code>predictions()</code>, and the data frame must also include a <code>by</code> column of labels. In this example, we \u201ccollapse\u201d response groups:</p> <pre><code>by &lt;- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n#&gt; \n#&gt;   By Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n#&gt;  3,4    0.422     0.0231 18.25   &lt;0.001 244.7 0.3766  0.467\n#&gt;  5      0.156     0.0462  3.38   &lt;0.001  10.4 0.0656  0.247\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n</code></pre> <p>This can be very useful in combination with the <code>hypothesis</code> argument. For example, here we compute the difference between average adjusted predictions for the 3 and 4 response levels, compared to the 5 response level:</p> <pre><code>predictions(nom, type = \"probs\", by = by, hypothesis = \"sequential\")\n#&gt; \n#&gt;     Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  5 - 3,4   -0.266     0.0694 -3.83   &lt;0.001 12.9 -0.402  -0.13\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre> <p>We can also use more complicated aggregations. Here, we compute the predicted probability of outcome levels for each value of <code>cyl</code>, by collapsing the \u201c3\u201d and \u201c4\u201d outcome levels:</p> <pre><code>nom &lt;- multinom(factor(gear) ~ mpg + factor(cyl), data = mtcars, trace = FALSE)\n\nby &lt;- expand.grid(\n    group = 3:5,\n    cyl = c(4, 6, 8),\n    stringsAsFactors = TRUE) |&gt;\n    # define labels\n    transform(by = ifelse(\n        group %in% 3:4,\n        sprintf(\"3/4 Gears &amp; %s Cylinders\", cyl),\n        sprintf(\"5 Gears &amp; %s Cylinders\", cyl)))\n\npredictions(nom, by = by)\n#&gt; \n#&gt;                       By Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  3/4 Gears &amp; 6 Cylinders    0.429     0.0661 6.49   &lt;0.001 33.4  0.2991  0.558\n#&gt;  3/4 Gears &amp; 4 Cylinders    0.409     0.0580 7.06   &lt;0.001 39.1  0.2956  0.523\n#&gt;  3/4 Gears &amp; 8 Cylinders    0.429     0.0458 9.35   &lt;0.001 66.6  0.3387  0.518\n#&gt;  5 Gears &amp; 6 Cylinders      0.143     0.1321 1.08    0.280  1.8 -0.1161  0.402\n#&gt;  5 Gears &amp; 4 Cylinders      0.182     0.1159 1.57    0.117  3.1 -0.0457  0.409\n#&gt;  5 Gears &amp; 8 Cylinders      0.143     0.0917 1.56    0.119  3.1 -0.0368  0.323\n#&gt; \n#&gt; Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n#&gt; Type:  probs\n</code></pre> <p>And we can then compare the different groups using the <code>hypothesis</code> argument:</p> <pre><code>predictions(nom, by = by, hypothesis = \"pairwise\")\n#&gt; \n#&gt;                                               Term  Estimate Std. Error         z Pr(&gt;|z|)   S    2.5 % 97.5 %\n#&gt;  3/4 Gears &amp; 6 Cylinders - 3/4 Gears &amp; 4 Cylinders  1.93e-02     0.0879  0.219839   0.8260 0.3 -0.15294  0.192\n#&gt;  3/4 Gears &amp; 6 Cylinders - 3/4 Gears &amp; 8 Cylinders  2.84e-05     0.0804  0.000353   0.9997 0.0 -0.15757  0.158\n#&gt;  3/4 Gears &amp; 6 Cylinders - 5 Gears &amp; 6 Cylinders    2.86e-01     0.1982  1.441540   0.1494 2.7 -0.10274  0.674\n#&gt;  3/4 Gears &amp; 6 Cylinders - 5 Gears &amp; 4 Cylinders    2.47e-01     0.1334  1.851411   0.0641 4.0 -0.01449  0.509\n#&gt;  3/4 Gears &amp; 6 Cylinders - 5 Gears &amp; 8 Cylinders    2.86e-01     0.1130  2.527636   0.0115 6.4  0.06415  0.507\n#&gt;  3/4 Gears &amp; 4 Cylinders - 3/4 Gears &amp; 8 Cylinders -1.93e-02     0.0739 -0.261054   0.7941 0.3 -0.16414  0.126\n#&gt;  3/4 Gears &amp; 4 Cylinders - 5 Gears &amp; 6 Cylinders    2.66e-01     0.1443  1.846189   0.0649 3.9 -0.01642  0.549\n#&gt;  3/4 Gears &amp; 4 Cylinders - 5 Gears &amp; 4 Cylinders    2.28e-01     0.1739  1.309481   0.1904 2.4 -0.11312  0.569\n#&gt;  3/4 Gears &amp; 4 Cylinders - 5 Gears &amp; 8 Cylinders    2.66e-01     0.1085  2.455119   0.0141 6.1  0.05371  0.479\n#&gt;  3/4 Gears &amp; 8 Cylinders - 5 Gears &amp; 6 Cylinders    2.86e-01     0.1399  2.042637   0.0411 4.6  0.01156  0.560\n#&gt;  3/4 Gears &amp; 8 Cylinders - 5 Gears &amp; 4 Cylinders    2.47e-01     0.1247  1.981366   0.0476 4.4  0.00267  0.491\n#&gt;  3/4 Gears &amp; 8 Cylinders - 5 Gears &amp; 8 Cylinders    2.86e-01     0.1375  2.076748   0.0378 4.7  0.01606  0.555\n#&gt;  5 Gears &amp; 6 Cylinders - 5 Gears &amp; 4 Cylinders     -3.86e-02     0.1758 -0.219838   0.8260 0.3 -0.38317  0.306\n#&gt;  5 Gears &amp; 6 Cylinders - 5 Gears &amp; 8 Cylinders     -5.68e-05     0.1608 -0.000353   0.9997 0.0 -0.31526  0.315\n#&gt;  5 Gears &amp; 4 Cylinders - 5 Gears &amp; 8 Cylinders      3.86e-02     0.1478  0.261054   0.7941 0.3 -0.25112  0.328\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  probs\n</code></pre>"},{"location":"vignettes/predictions/#bayesian-models","title":"Bayesian models","text":"<p>The same strategy works for Bayesian models:</p> <pre><code>library(brms)\nmod &lt;- brm(am ~ mpg * vs, data = mtcars, family = bernoulli)\n</code></pre> <pre><code>predictions(mod, by = \"vs\")\n#&gt; \n#&gt;  vs Estimate 2.5 % 97.5 %\n#&gt;   0    0.327 0.182  0.507\n#&gt;   1    0.499 0.366  0.672\n#&gt; \n#&gt; Columns: vs, estimate, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>The results above show the median of the posterior distribution of group-wise means. Note that we take the mean of predicted values for each MCMC draw before computing quantiles. This is equivalent to:</p> <pre><code>draws &lt;- posterior_epred(mod)\nquantile(rowMeans(draws[, mtcars$vs == 0]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.3271836 0.1824479 0.5072074\nquantile(rowMeans(draws[, mtcars$vs == 1]), probs = c(.5, .025, .975))\n#&gt;       50%      2.5%     97.5% \n#&gt; 0.4993250 0.3657956 0.6721267\n</code></pre>"},{"location":"vignettes/predictions/#conditional-adjusted-predictions-plot","title":"Conditional Adjusted Predictions (Plot)","text":"<p>First, we download the <code>ggplot2movies</code> dataset from the RDatasets archive. Then, we create a variable called <code>certified_fresh</code> for movies with a rating of at least 8. Finally, we discard some outliers and fit a logistic regression model:</p> <pre><code>library(tidyverse)\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\") |&gt;\n    mutate(style = case_when(Action == 1 ~ \"Action\",\n                             Comedy == 1 ~ \"Comedy\",\n                             Drama == 1 ~ \"Drama\",\n                             TRUE ~ \"Other\"),\n           style = factor(style),\n           certified_fresh = rating &gt;= 8) |&gt;\n    dplyr::filter(length &lt; 240)\n\nmod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n</code></pre> <p>We can plot adjusted predictions, conditional on the <code>length</code> variable using the <code>plot_predictions</code> function:</p> <pre><code>mod &lt;- glm(certified_fresh ~ length, data = dat, family = binomial)\n\nplot_predictions(mod, condition = \"length\")\n</code></pre> <p></p> <p>We can also introduce another condition which will display a categorical variable like <code>style</code> in different colors. This can be useful in models with interactions:</p> <pre><code>mod &lt;- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"length\", \"style\"))\n</code></pre> <p></p> <p>Since the output of <code>plot_predictions()</code> is a <code>ggplot2</code> object, it is very easy to customize. For example, we can add points for the actual observations of our dataset like so:</p> <pre><code>library(ggplot2)\nlibrary(ggrepel)\n\nmt &lt;- mtcars\nmt$label &lt;- row.names(mt)\n\nmod &lt;- lm(mpg ~ hp, data = mt)\n\nplot_predictions(mod, condition = \"hp\") +\n    geom_point(aes(x = hp, y = mpg), data = mt) +\n    geom_rug(aes(x = hp, y = mpg), data = mt) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp &gt; 250),\n                    nudge_y = 2) +\n    theme_classic()\n</code></pre> <p></p> <p>We can also use <code>plot_predictions()</code> in models with multinomial outcomes or grouped coefficients. For example, notice that when we call <code>draw=FALSE</code>, the result includes a <code>group</code> column:</p> <pre><code>library(MASS)\nlibrary(ggplot2)\n\nmod &lt;- nnet::multinom(factor(gear) ~ mpg, data = mtcars, trace = FALSE)\n\np &lt;- plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\",\n    draw = FALSE)\n\nhead(p)\n#&gt;   rowid group  estimate  std.error statistic       p.value  s.value  conf.low conf.high gear      mpg\n#&gt; 1     1     3 0.9714990 0.03873404  25.08127 7.962555e-139 458.7548 0.8955817  1.047416    3 10.40000\n#&gt; 2     2     3 0.9656724 0.04394086  21.97664 4.818284e-107 353.1778 0.8795499  1.051795    3 10.87959\n#&gt; 3     3     3 0.9586759 0.04964165  19.31193  4.263532e-83 273.6280 0.8613801  1.055972    3 11.35918\n#&gt; 4     4     3 0.9502914 0.05581338  17.02623  5.247849e-65 213.5336 0.8408992  1.059684    3 11.83878\n#&gt; 5     5     3 0.9402691 0.06240449  15.06733  2.656268e-51 168.0089 0.8179586  1.062580    3 12.31837\n#&gt; 6     6     3 0.9283274 0.06932768  13.39043  6.878233e-41 133.4170 0.7924476  1.064207    3 12.79796\n</code></pre> <p>Now we use the <code>group</code> column:</p> <pre><code>plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\") +\n    facet_wrap(~group)\n</code></pre> <p></p>"},{"location":"vignettes/predictions/#prediction-types","title":"Prediction types","text":"<p>The <code>predictions</code> function computes model-adjusted means on the scale of the output of the <code>predict(model)</code> function. By default, <code>predict</code> produces predictions on the <code>\"response\"</code> scale, so the adjusted predictions should be interpreted on that scale. However, users can pass a string to the <code>type</code> argument, and <code>predictions</code> will consider different outcomes.</p> <p>Typical values include <code>\"response\"</code> and <code>\"link\"</code>, but users should refer to the documentation of the <code>predict</code> of the package they used to fit the model to know what values are allowable. documentation.</p> <pre><code>mod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\npred &lt;- predictions(mod, type = \"response\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.461     0.1158 3.98  &lt; 0.001 13.8 0.2341  0.688\n#&gt;     0.598     0.1324 4.52  &lt; 0.001 17.3 0.3384  0.857\n#&gt;     0.492     0.1196 4.11  &lt; 0.001 14.6 0.2573  0.726\n#&gt;     0.297     0.1005 2.95  0.00314  8.3 0.0999  0.494\n#&gt;     0.260     0.0978 2.66  0.00788  7.0 0.0682  0.452\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  response\n\npred &lt;- predictions(mod, type = \"link\")\nhead(pred)\n#&gt; \n#&gt;  Estimate Std. Error       z Pr(&gt;|z|)   S  2.5 %  97.5 %\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#&gt;    0.3967      0.551  0.7204   0.4713 1.1 -0.683  1.4761\n#&gt;   -0.0331      0.479 -0.0692   0.9448 0.1 -0.971  0.9049\n#&gt;   -0.8621      0.482 -1.7903   0.0734 3.8 -1.806  0.0817\n#&gt;   -1.0463      0.509 -2.0575   0.0396 4.7 -2.043 -0.0496\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg \n#&gt; Type:  link\n</code></pre> <p>We can also plot predictions on different outcome scales:</p> <pre><code>plot_predictions(mod, condition = \"mpg\", type = \"response\")\n</code></pre> <p></p> <pre><code>plot_predictions(mod, condition = \"mpg\", type = \"link\")\n</code></pre> <p></p>"},{"location":"vignettes/python/","title":"Python","text":"<p>The Python programming language offers several powerful libraries for (bayesian) statistical analysis, such as <code>NumPyro</code> and <code>PyMC</code>. This vignette shows how to use the the full power of <code>marginaleffects</code> to analyze and interpret the results of models estimated by Markov Chain Monte Carlo using the <code>NumPyro</code> Python library.</p>"},{"location":"vignettes/python/#fitting-a-numpyro-model","title":"Fitting a <code>NumPyro</code> model","text":"<p>To begin, we load the <code>reticulate</code> package which allows us to interact with the Python interpreter from an <code>R</code> session. Then, we write a <code>NumPyro</code> model and we load it to memory using the <code>source_python()</code> function. The important functions to note in the Python code are:</p> <ul> <li><code>load_df()</code> downloads data on pulmonary fibrosis.</li> <li><code>model()</code> defines the <code>NumPyro</code> model.</li> <li><code>fit_mcmc_model()</code> fits the model using Markov Chain Monte Carlo.</li> <li><code>predict_mcmc()</code>: accepts a data frame and returns a matrix of draws     from the posterior distribution of adjusted predictions (fitted     values).</li> </ul> <pre><code>library(reticulate)\nlibrary(marginaleffects)\n\nmodel &lt;- '\n## Model code adapted from the NumPyro documtation under Apache License:\n## https://num.pyro.ai/en/latest/tutorials/bayesian_hierarchical_linear_regression.html\n\nimport pandas as pd\nimport numpy as np\nimport numpyro\nfrom numpyro.infer import SVI, Predictive, MCMC,NUTS, autoguide, TraceMeanField_ELBO\nimport numpyro.distributions as dist\nfrom numpyro.infer.initialization import init_to_median, init_to_uniform,init_to_sample\nfrom jax import random\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\n\ndef load_df():\n    train = pd.read_csv(\"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/osic_pulmonary_fibrosis.csv\")\n    return train\n\n\ndef model(data, predict = False):\n    FVC_obs = data[\"FVC\"].values  if predict == False else None\n    patient_encoder = LabelEncoder()\n    Age_obs = data[\"Age\"].values\n    patient_code = patient_encoder.fit_transform(data[\"Patient\"].values)\n    \u03bc_\u03b1 = numpyro.sample(\"\u03bc_\u03b1\", dist.Normal(0.0, 500.0))\n    \u03c3_\u03b1 = numpyro.sample(\"\u03c3_\u03b1\", dist.HalfNormal(100.0))\n\n    age = numpyro.sample(\"age\", dist.Normal(0.0, 500.0))\n\n    n_patients = len(np.unique(patient_code))\n\n    with numpyro.plate(\"plate_i\", n_patients):\n        \u03b1 = numpyro.sample(\"\u03b1\", dist.Normal(\u03bc_\u03b1, \u03c3_\u03b1))\n\n    \u03c3 = numpyro.sample(\"\u03c3\", dist.HalfNormal(100.0))\n    FVC_est = \u03b1[patient_code] + age * Age_obs\n\n    with numpyro.plate(\"data\", len(patient_code)):\n        numpyro.sample(\"obs\", dist.Normal(FVC_est, \u03c3), obs=FVC_obs)\n\n\ndef fit_mcmc_model(train_df, samples = 1000):\n    numpyro.set_host_device_count(4)\n    rng_key = random.PRNGKey(0)\n    mcmc = MCMC(\n        NUTS(model),\n        num_samples=samples,\n        num_warmup=1000,\n        progress_bar=True,\n        num_chains = 4\n        )\n\n    mcmc.run(rng_key, train_df)\n\n    posterior_draws = mcmc.get_samples()\n\n    with open(\"mcmc_posterior_draws.pickle\", \"wb\") as handle:\n        pickle.dump(posterior_draws, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef predict_mcmc(data):\n\n    with open(\"mcmc_posterior_draws.pickle\", \"rb\") as handle:\n        posterior_draws = pickle.load(handle)\n\n    predictive = Predictive(model = model,posterior_samples=posterior_draws)\n    samples = predictive(random.PRNGKey(1), data, predict = True)\n    y_pred = samples[\"obs\"]\n    # transpose so that each column is a draw and each row is an observation\n    y_pred = np.transpose(np.array(y_pred))\n\n    return y_pred \n'\n\n## save python script to temp file\ntmp &lt;- tempfile()\ncat(model, file = tmp)\n\n## load functions\nsource_python(tmp)\n\n## download data\ndf &lt;- load_df()\n\n## fit model\nfit_mcmc_model(df)\n</code></pre>"},{"location":"vignettes/python/#analyzing-the-results-in-marginaleffects","title":"Analyzing the results in <code>marginaleffects</code>","text":"<p>Each of the functions in the <code>marginaleffects</code> package requires that users supply a <code>model</code> object on which the function will operate. When estimating models outside <code>R</code>, we do not have such a model object. We thus begin by creating a \u201cfake\u201d model object: an empty data frame which we define to be of class \u201ccustom\u201d. Then, we set a global option to tell <code>marginaleffects</code> that this \u201ccustom\u201d class is supported.</p> <pre><code>mod &lt;- data.frame()\nclass(mod) &lt;- \"custom\"\n\noptions(\"marginaleffects_model_classes\" = \"custom\")\n</code></pre> <p>Next, we define a <code>get_predict</code> method for our new custom class. This method must accept three arguments: <code>model</code>, <code>newdata</code>, and <code>...</code>. The <code>get_predict</code> method must return a data frame with one row for each of the rows in <code>newdata</code>, two columns (<code>rowid</code> and <code>estimate</code>), and an attribute called <code>posterior_draws</code> which hosts a matrix of posterior draws with the same number of rows as <code>newdata</code>.</p> <p>The method below uses <code>reticulate</code> to call the <code>predict_mcmc()</code> function that we defined in the Python script above. The <code>predict_mcmc()</code> function accepts a data frame and returns a matrix with the same number of rows.</p> <pre><code>get_predict.custom &lt;- function(model, newdata, ...) {\n    pred &lt;- predict_mcmc(newdata)\n    out &lt;- data.frame(\n        rowid = seq_len(nrow(newdata)),\n        predicted = apply(pred, 1, stats::median)\n    )\n    attr(out, \"posterior_draws\") &lt;- pred\n    return(out)\n}\n</code></pre> <p>Now we can use most of the <code>marginaleffects</code> package functions to analyze our results. Since we use a \u201cfake\u201d model object, <code>marginaleffects</code> cannot retrieve the original data from the model object, and we always need to supply a <code>newdata</code> argument:</p> <pre><code>## predictions on the original dataset\npredictions(mod, newdata = df) |&gt; head()\n\n## predictions for user-defined predictor values\npredictions(mod, newdata = datagrid(newdata = df, Age = c(60, 70)))\n\npredictions(mod, newdata = datagrid(newdata = df, Age = range))\n\n## average predictions by group\npredictions(mod, newdata = df, by = \"Sex\")\n\n## contrasts (average)\navg_comparisons(mod, variables = \"Age\", newdata = df)\n\navg_comparisons(mod, variables = list(\"Age\" = \"sd\"), newdata = df)\n\n## slope (elasticity)\navg_slopes(mod, variables = \"Age\", slope = \"eyex\", newdata = df)\n</code></pre>"},{"location":"vignettes/references/","title":"References","text":""},{"location":"vignettes/references/#references","title":"References","text":"<p>Predictions</p> <ul> <li><code>predictions</code></li> <li><code>avg_predictions</code></li> <li><code>plot_predictions</code></li> </ul> <p>Comparisons: Contrasts, Diffrences, Ratios, Odds, etc.</p> <ul> <li><code>comparisons</code></li> <li><code>avg_comparisons</code></li> <li><code>plot_comparisons</code></li> </ul> <p>Slopes, marginal effects, partial derivatives: * <code>slopes</code> * <code>avg_slopes</code> * <code>plot_slopes</code></p> <p>Marginal means:</p> <ul> <li><code>marginal_means</code></li> </ul> <p>Other:</p> <ul> <li><code>hypotheses</code></li> <li><code>inferences</code></li> <li><code>posterior_draws</code></li> </ul>"},{"location":"vignettes/slopes/","title":"Slopes","text":""},{"location":"vignettes/slopes/#definition","title":"Definition","text":"<p>Slopes are defined as:</p> <p>Partial derivatives of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends.</p> <p>This vignette follows the econometrics tradition by referring to \u201cslopes\u201d and \u201cmarginal effects\u201d interchangeably. In this context, the word \u201cmarginal\u201d refers to the idea of a \u201csmall change,\u201d in the calculus sense.</p> <p>A marginal effect measures the association between a change in a regressor x, and a change in the response y. Put differently, differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor x.</p> <p>Marginal effects are extremely useful, because they are intuitive and easy to interpret. They are often the main quantity of interest in an empirical analysis.</p> <p>In scientific practice, the \u201cMarginal Effect\u201d falls in the same toolbox as the \u201cContrast.\u201d Both try to answer a counterfactual question: What would happen to y if x were different? They allow us to model the \u201ceffect\u201d of a change/difference in the regressor x on the response y.[1]</p> <p>To illustrate the concept, consider this quadratic function:</p> <p>y\u2004=\u2004\u2005\u2212\u2005x<sup>2</sup></p> <p>From the definition above, we know that the marginal effect is the partial derivative of y with respect to x:</p> <p>$$\\frac{\\partial y}{\\partial x} = -2x$$</p> <p>To get intuition about how to interpret this quantity, consider the response of y to x. It looks like this:</p> <p></p> <p>When x increases, y starts to increase. But then, as x increases further, y creeps back down in negative territory.</p> <p>A marginal effect is the slope of this response function at a certain value of x. The next plot adds three tangent lines, highlighting the slopes of the response function for three values of x. The slopes of these tangents tell us three things:</p> <ol> <li>When x\u2004\\&lt;\u20040, the slope is positive: an increase in x is     associated with an increase in y: The marginal effect is positive.</li> <li>When x\u2004=\u20040, the slope is null: a (small) change in x is     associated with no change in y. The marginal effect is null.</li> <li>When x\u2004&gt;\u20040, the slope is negative: an increase in x is     associated with a decrease in y. The marginal effect is negative.</li> </ol> <p></p> <p>Below, we show how to reach the same conclusions in an estimation context, with simulated data and the <code>slopes</code> function.</p>"},{"location":"vignettes/slopes/#slopes-function","title":"<code>slopes</code> function","text":"<p>The marginal effect is a unit-level measure of association between changes in a regressor and changes in the response. Except in the simplest linear models, the value of the marginal effect will be different from individual to individual, because it will depend on the values of the other covariates for each individual.</p> <p>The <code>slopes</code> function thus produces distinct estimates of the marginal effect for each row of the data used to fit the model. The output of <code>marginaleffects</code> is a simple <code>data.frame</code>, which can be inspected with all the usual <code>R</code> commands.</p> <p>To show this, we load the library, download the Palmer Penguins, and estimate a GLM model:</p> <pre><code>library(marginaleffects)\n\ndat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin &lt;- ifelse(dat$body_mass_g &gt; median(dat$body_mass_g, na.rm = TRUE), 1, 0)\n\nmod &lt;- glm(large_penguin ~ bill_length_mm + flipper_length_mm + species,\n           data = dat, family = binomial)\n</code></pre> <pre><code>mfx &lt;- slopes(mod)\nhead(mfx)\n#&gt; \n#&gt;            Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 % 97.5 %\n#&gt;  bill_length_mm    dY/dX   0.0176    0.00837 2.11   0.0352  4.8 0.00122 0.0340\n#&gt;  bill_length_mm    dY/dX   0.0358    0.01235 2.90   0.0037  8.1 0.01164 0.0601\n#&gt;  bill_length_mm    dY/dX   0.0844    0.02109 4.00   &lt;0.001 14.0 0.04310 0.1258\n#&gt;  bill_length_mm    dY/dX   0.0347    0.00642 5.41   &lt;0.001 23.9 0.02213 0.0473\n#&gt;  bill_length_mm    dY/dX   0.0509    0.01351 3.77   &lt;0.001 12.6 0.02440 0.0774\n#&gt;  bill_length_mm    dY/dX   0.0165    0.00777 2.12   0.0337  4.9 0.00128 0.0317\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm, flipper_length_mm, species \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/slopes/#the-marginal-effects-zoo","title":"The Marginal Effects Zoo","text":"<p>A dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. There are ways to make this information easier to digest, by computing various quantities of interest. In a characteristically excellent blog post, Professor Andrew Heiss introduces many such quantities:</p> <ul> <li>Average Marginal Effects</li> <li>Group-Average Marginal Effects</li> <li>Marginal Effects at User-Specified Values (or Representative Values)</li> <li>Marginal Effects at the Mean</li> <li>Counterfactual Marginal Effects</li> <li>Conditional Marginal Effects</li> </ul> <p>The rest of this vignette defines each of those quantities and explains how to use the <code>slopes()</code> and <code>plot_slopes()</code> functions to compute them. The main differences between these quantities pertain to (a) the regressor values at which we estimate marginal effects, and (b) the way in which unit-level marginal effects are aggregated.</p> <p>Heiss drew this exceedingly helpful graph which summarizes the information in the rest of this vignette:</p> <p></p>"},{"location":"vignettes/slopes/#average-marginal-effect-ame","title":"Average Marginal Effect (AME)","text":"<p>A dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. Many analysts like to report the \u201cAverage Marginal Effect\u201d, that is, the average of all the observation-specific marginal effects. These are easy to compute based on the full <code>data.frame</code> shown above, but the <code>avg_slopes()</code> function is convenient:</p> <pre><code>avg_slopes(mod)\n#&gt; \n#&gt;               Term           Contrast Estimate Std. Error      z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;  bill_length_mm    dY/dX                0.0276    0.00578  4.774   &lt;0.001 19.1  0.01625  0.0389\n#&gt;  flipper_length_mm dY/dX                0.0106    0.00235  4.511   &lt;0.001 17.2  0.00598  0.0152\n#&gt;  species           Chinstrap - Adelie  -0.4148    0.05654 -7.336   &lt;0.001 42.0 -0.52561 -0.3040\n#&gt;  species           Gentoo - Adelie      0.0617    0.10688  0.577    0.564  0.8 -0.14779  0.2712\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Note that since marginal effects are derivatives, they are only properly defined for continuous numeric variables. When the model also includes categorical regressors, the <code>summary</code> function will try to display relevant (regression-adjusted) contrasts between different categories, as shown above.</p> <p>You can also extract average marginal effects using <code>tidy</code> and <code>glance</code> methods which conform to the <code>broom</code> package specification:</p> <pre><code>tidy(mfx)\n#&gt; # A tibble: 4 \u00d7 8\n#&gt;   term              contrast                       estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;             &lt;chr&gt;                             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 bill_length_mm    mean(dY/dX)                      0.0276   0.00578     4.77  1.81e- 6  0.0163     0.0389\n#&gt; 2 flipper_length_mm mean(dY/dX)                      0.0106   0.00235     4.51  6.45e- 6  0.00598    0.0152\n#&gt; 3 species           mean(Chinstrap) - mean(Adelie)  -0.415    0.0565     -7.34  2.20e-13 -0.526     -0.304 \n#&gt; 4 species           mean(Gentoo) - mean(Adelie)      0.0617   0.107       0.577 5.64e- 1 -0.148      0.271\n\nglance(mfx)\n#&gt; # A tibble: 1 \u00d7 7\n#&gt;     aic   bic r2.tjur  rmse  nobs     F logLik   \n#&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;logLik&gt; \n#&gt; 1  180.  199.   0.695 0.276   342  15.7 -84.92257\n</code></pre>"},{"location":"vignettes/slopes/#group-average-marginal-effect-g-ame","title":"Group-Average Marginal Effect (G-AME)","text":"<p>We can also use the <code>by</code> argument the average marginal effects within different subgroups of the observed data, based on values of the regressors. For example, to compute the average marginal effects of Bill Length for each Species, we do:</p> <pre><code>avg_slopes(\n  mod,\n  by = \"species\",\n  variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term    Contrast   species Estimate Std. Error    z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;  bill_length_mm mean(dY/dX) Adelie     0.04353    0.00878 4.96   &lt;0.001 20.4  0.0263 0.06074\n#&gt;  bill_length_mm mean(dY/dX) Chinstrap  0.03679    0.00976 3.77   &lt;0.001 12.6  0.0177 0.05591\n#&gt;  bill_length_mm mean(dY/dX) Gentoo     0.00287    0.00284 1.01    0.312  1.7 -0.0027 0.00845\n#&gt; \n#&gt; Columns: term, contrast, species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>This is equivalent to manually taking the mean of the observation-level marginal effect for each species sub-group:</p> <pre><code>aggregate(\n  mfx$estimate,\n  by = list(mfx$species, mfx$term),\n  FUN = mean)\n#&gt;     Group.1           Group.2            x\n#&gt; 1    Adelie    bill_length_mm  0.043532570\n#&gt; 2 Chinstrap    bill_length_mm  0.036793674\n#&gt; 3    Gentoo    bill_length_mm  0.002872923\n#&gt; 4    Adelie flipper_length_mm  0.016708310\n#&gt; 5 Chinstrap flipper_length_mm  0.014121843\n#&gt; 6    Gentoo flipper_length_mm  0.001102661\n#&gt; 7    Adelie           species -0.054519623\n#&gt; 8 Chinstrap           species -0.313337522\n#&gt; 9    Gentoo           species -0.250726004\n</code></pre> <p>Note that <code>marginaleffects</code> follows <code>Stata</code> and the <code>margins</code> package in computing standard errors using the group-wise averaged Jacobian.</p>"},{"location":"vignettes/slopes/#marginal-effect-at-user-specified-values","title":"Marginal Effect at User-Specified Values","text":"<p>Sometimes, we are not interested in all the unit-specific marginal effects, but would rather look at the estimated marginal effects for certain \u201ctypical\u201d individuals, or for user-specified values of the regressors. The <code>datagrid</code> function helps us build a data grid full of \u201ctypical\u201d rows. For example, to generate artificial Adelies and Gentoos with 180mm flippers:</p> <pre><code>datagrid(flipper_length_mm = 180,\n         species = c(\"Adelie\", \"Gentoo\"),\n         model = mod)\n#&gt;   large_penguin bill_length_mm flipper_length_mm species\n#&gt; 1     0.4853801       43.92193               180  Adelie\n#&gt; 2     0.4853801       43.92193               180  Gentoo\n</code></pre> <p>The same command can be used (omitting the <code>model</code> argument) to <code>marginaleffects</code>\u2019s <code>newdata</code> argument to compute marginal effects for those (fictional) individuals:</p> <pre><code>slopes(\n  mod,\n  newdata = datagrid(\n    flipper_length_mm = 180,\n    species = c(\"Adelie\", \"Gentoo\")))\n#&gt; \n#&gt;               Term           Contrast flipper_length_mm species Estimate Std. Error      z Pr(&gt;|z|)    S    2.5 %   97.5 %\n#&gt;  bill_length_mm    dY/dX                            180  Adelie   0.0607    0.03322  1.827   0.0678  3.9 -0.00443  0.12578\n#&gt;  bill_length_mm    dY/dX                            180  Gentoo   0.0847    0.03925  2.157   0.0310  5.0  0.00773  0.16159\n#&gt;  flipper_length_mm dY/dX                            180  Adelie   0.0233    0.00551  4.230   &lt;0.001 15.4  0.01250  0.03408\n#&gt;  flipper_length_mm dY/dX                            180  Gentoo   0.0325    0.00851  3.819   &lt;0.001 12.9  0.01582  0.04917\n#&gt;  species           Chinstrap - Adelie               180  Adelie  -0.2111    0.10668 -1.978   0.0479  4.4 -0.42013 -0.00197\n#&gt;  species           Chinstrap - Adelie               180  Gentoo  -0.2111    0.10668 -1.978   0.0479  4.4 -0.42013 -0.00197\n#&gt;  species           Gentoo - Adelie                  180  Adelie   0.1591    0.30225  0.526   0.5986  0.7 -0.43328  0.75152\n#&gt;  species           Gentoo - Adelie                  180  Gentoo   0.1591    0.30225  0.526   0.5986  0.7 -0.43328  0.75152\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, flipper_length_mm, species, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm \n#&gt; Type:  response\n</code></pre> <p>When variables are omitted from the <code>datagrid</code> call, they will automatically be set at their mean or mode (depending on variable type).</p>"},{"location":"vignettes/slopes/#marginal-effect-at-the-mean-mem","title":"Marginal Effect at the Mean (MEM)","text":"<p>The \u201cMarginal Effect at the Mean\u201d is a marginal effect calculated for a hypothetical observation where each regressor is set at its mean or mode. By default, the <code>datagrid</code> function that we used in the previous section sets all regressors to their means or modes. To calculate the MEM, we can set the <code>newdata</code> argument, which determines the values of predictors at which we want to compute marginal effects:</p> <pre><code>slopes(mod, newdata = \"mean\")\n#&gt; \n#&gt;               Term           Contrast Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %  97.5 %\n#&gt;  bill_length_mm    dY/dX                0.0503    0.01244   4.038   &lt;0.001 14.2  0.02586  0.0746\n#&gt;  flipper_length_mm dY/dX                0.0193    0.00553   3.489   &lt;0.001 11.0  0.00845  0.0301\n#&gt;  species           Chinstrap - Adelie  -0.8070    0.07690 -10.494   &lt;0.001 83.2 -0.95776 -0.6563\n#&gt;  species           Gentoo - Adelie      0.0829    0.11469   0.722     0.47  1.1 -0.14193  0.3076\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, large_penguin, bill_length_mm, flipper_length_mm, species \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/slopes/#counterfactual-marginal-effects","title":"Counterfactual Marginal Effects","text":"<p>The <code>datagrid</code> function allowed us look at completely fictional individuals. Setting the <code>grid_type</code> argument of this function to <code>\"counterfactual\"</code> lets us compute the marginal effects for the actual observations in our dataset, but with a few manipulated values. For example, this code will create a <code>data.frame</code> twice as long as the original <code>dat</code>, where each observation is repeated with different values of the <code>flipper_length_mm</code> variable:</p> <pre><code>nd &lt;- datagrid(flipper_length_mm = c(160, 180),\n               model = mod,\n               grid_type = \"counterfactual\")\n</code></pre> <p>We see that the rows 1, 2, and 3 of the original dataset have been replicated twice, with different values of the <code>flipper_length_mm</code> variable:</p> <pre><code>nd[nd$rowid %in% 1:3,]\n#&gt;     rowidcf large_penguin bill_length_mm species flipper_length_mm\n#&gt; 1         1             0           39.1  Adelie               160\n#&gt; 2         2             0           39.5  Adelie               160\n#&gt; 3         3             0           40.3  Adelie               160\n#&gt; 343       1             0           39.1  Adelie               180\n#&gt; 344       2             0           39.5  Adelie               180\n#&gt; 345       3             0           40.3  Adelie               180\n</code></pre> <p>We can use the observation-level marginal effects to compute average (or median, or anything else) marginal effects over the counterfactual individuals:</p> <pre><code>library(dplyr)\n\nslopes(mod, newdata = nd) |&gt;\n    group_by(term) |&gt;\n    summarize(estimate = median(estimate))\n#&gt; # A tibble: 3 \u00d7 2\n#&gt;   term               estimate\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;\n#&gt; 1 bill_length_mm    0.00985  \n#&gt; 2 flipper_length_mm 0.00378  \n#&gt; 3 species           0.0000226\n</code></pre>"},{"location":"vignettes/slopes/#conditional-marginal-effects-plot","title":"Conditional Marginal Effects (Plot)","text":"<p>The <code>plot_slopes</code> function can be used to draw \u201cConditional Marginal Effects.\u201d This is useful when a model includes interaction terms and we want to plot how the marginal effect of a variable changes as the value of a \u201ccondition\u201d (or \u201cmoderator\u201d) variable changes:</p> <pre><code>mod &lt;- lm(mpg ~ hp * wt + drat, data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"wt\")\n</code></pre> <p></p> <p>The marginal effects in the plot above were computed with values of all regressors \u2013 except the <code>variables</code> and the <code>condition</code> \u2013 held at their means or modes, depending on variable type.</p> <p>Since <code>plot_slopes()</code> produces a <code>ggplot2</code> object, it is easy to customize. For example:</p> <pre><code>plot_slopes(mod, variables = \"hp\", condition = \"wt\") +\n    geom_rug(aes(x = wt), data = mtcars) +\n    theme_classic()\n</code></pre> <p></p>"},{"location":"vignettes/slopes/#example-quadratic","title":"Example: Quadratic","text":"<p>In the \u201cDefinition\u201d section of this vignette, we considered how marginal effects can be computed analytically in a simple quadratic equation context. We can now use the <code>slopes</code> function to replicate our analysis of the quadratic function in a regression application.</p> <p>Say you estimate a linear regression model with a quadratic term:</p> <p>Y\u2004=\u2004\u03b2<sub>0</sub>\u2005+\u2005\u03b2<sub>1</sub>X<sup>2</sup>\u2005+\u2005\u03b5</p> <p>and obtain estimates of \u03b2<sub>0</sub>\u2004=\u20041 and \u03b2<sub>1</sub>\u2004=\u20042. Taking the partial derivative with respect to X and plugging in our estimates gives us the marginal effect of X on Y:</p> <p>\u2202Y/\u2202X\u2004=\u2004\u03b2<sub>0</sub>\u2005+\u20052\u2005\u22c5\u2005\u03b2<sub>1</sub>X \u2202Y/\u2202X\u2004=\u20041\u2005+\u20054X</p> <p>This result suggests that the effect of a change in X on Y depends on the level of X. When X is large and positive, an increase in X is associated to a large increase in Y. When X is small and positive, an increase in X is associated to a small increase in Y. When X is a large negative value, an increase in X is associated with a decrease in Y.</p> <p><code>marginaleffects</code> arrives at the same conclusion in simulated data:</p> <pre><code>library(tidyverse)\nN &lt;- 1e5\nquad &lt;- data.frame(x = rnorm(N))\nquad$y &lt;- 1 + 1 * quad$x + 2 * quad$x^2 + rnorm(N)\nmod &lt;- lm(y ~ x + I(x^2), quad)\n\nslopes(mod, newdata = datagrid(x = -2:2))  |&gt;\n    mutate(truth = 1 + 4 * x) |&gt;\n    select(estimate, truth)\n#&gt; \n#&gt;  Estimate\n#&gt;        -7\n#&gt;        -3\n#&gt;         1\n#&gt;         5\n#&gt;         9\n#&gt; \n#&gt; Columns: estimate, truth\n</code></pre> <p>We can plot conditional adjusted predictions with <code>plot_predictions</code> function:</p> <pre><code>plot_predictions(mod, condition = \"x\")\n</code></pre> <p></p> <p>We can plot conditional marginal effects with the <code>plot_slopes</code> function (see section below):</p> <pre><code>plot_slopes(mod, variables = \"x\", condition = \"x\")\n</code></pre> <p></p> <p>Again, the conclusion is the same. When x\u2004\\&lt;\u20040, an increase in x is associated with an decrease in y. When x\u2004&gt;\u20041/4, the marginal effect is positive, which suggests that an increase in x is associated with an increase in y.</p>"},{"location":"vignettes/slopes/#slopes-vs-predictions-a-visual-interpretation","title":"Slopes vs Predictions: A Visual Interpretation","text":"<p>Often, analysts will plot predicted values of the outcome with a best fit line:</p> <pre><code>library(ggplot2)\n\nmod &lt;- lm(mpg ~ hp * qsec, data = mtcars)\n\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_point(data = mtcars, aes(hp, mpg)) \n</code></pre> <p></p> <p>The slope of this line is calculated using the same technique we all learned in grade school: dividing rise over run.</p> <pre><code>p &lt;- plot_predictions(mod, condition = \"hp\", vcov = TRUE, draw = FALSE)\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[10], y = p$estimate[10], yend = p$estimate[20])) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[20], y = p$estimate[20], yend = p$estimate[20])) +\n  annotate(\"text\", label = \"Rise\", y = 10, x = 140) +\n  annotate(\"text\", label = \"Run\", y = 2, x = 200)\n</code></pre> <p></p> <p>Instead of computing this slope manually, we can just call:</p> <pre><code>avg_slopes(mod, variables = \"hp\")\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp   -0.112     0.0126 -8.92   &lt;0.001 61.0 -0.137 -0.0874\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Now, consider the fact that our model includes an interaction between <code>hp</code> and <code>qsec</code>. This means that the slope will actually differ based on the value of the moderator variable <code>qsec</code>:</p> <pre><code>plot_predictions(mod, condition = list(\"hp\", \"qsec\" = \"quartile\"))\n</code></pre> <p></p> <p>We can estimate the slopes of these three fit lines easily:</p> <pre><code>slopes(\n  mod,\n  variables = \"hp\",\n  newdata = datagrid(qsec = quantile(mtcars$qsec, probs = c(.25, .5, .75))))\n#&gt; \n#&gt;  Term qsec Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp 16.9  -0.0934     0.0111 -8.43   &lt;0.001 54.6 -0.115 -0.0717\n#&gt;    hp 17.7  -0.1093     0.0123 -8.92   &lt;0.001 60.9 -0.133 -0.0853\n#&gt;    hp 18.9  -0.1325     0.0154 -8.61   &lt;0.001 56.9 -0.163 -0.1023\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, qsec, predicted_lo, predicted_hi, predicted, mpg, hp \n#&gt; Type:  response\n</code></pre> <p>As we see in the graph, all three slopes are negative, but the Q3 slope is steepest.</p> <p>We could then push this one step further, and measure the slope of <code>mpg</code> with respect to <code>hp</code>, for all observed values of <code>qsec</code>. This is achieved with the <code>plot_slopes()</code> function:</p> <pre><code>plot_slopes(mod, variables = \"hp\", condition = \"qsec\") +\n  geom_hline(yintercept = 0, linetype = 3)\n</code></pre> <p></p> <p>This plot shows that the marginal effect of <code>hp</code> on <code>mpg</code> is always negative (the slope is always below zero), and that this effect becomes even more negative as <code>qsec</code> increases.</p>"},{"location":"vignettes/slopes/#prediction-types","title":"Prediction types","text":"<p>The <code>marginaleffect</code> function takes the derivative of the fitted (or predicted) values of the model, as is typically generated by the <code>predict(model)</code> function. By default, <code>predict</code> produces predictions on the <code>\"response\"</code> scale, so the marginal effects should be interpreted on that scale. However, users can pass a string or a vector of strings to the <code>type</code> argument, and <code>marginaleffects</code> will consider different outcomes.</p> <p>Typical values include <code>\"response\"</code> and <code>\"link\"</code>, but users should refer to the documentation of the <code>predict</code> of the package they used to fit the model to know what values are allowable. documentation.</p> <pre><code>mod &lt;- glm(am ~ mpg, family = binomial, data = mtcars)\navg_slopes(mod, type = \"response\")\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;   mpg   0.0465    0.00887 5.24   &lt;0.001 22.6 0.0291 0.0639\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\navg_slopes(mod, type = \"link\")\n#&gt; \n#&gt;  Term Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n#&gt;   mpg    0.307      0.115 2.67  0.00751 7.1 0.0819  0.532\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  link\n</code></pre>"},{"location":"vignettes/slopes/#minimum-or-maximum-slope-velocity","title":"Minimum or maximum slope (velocity)","text":"<p>In some fields such as epidemiology, it is common to compute the minimum or maximum slope, as a measure of the \u201cvelocity\u201d of the response function. To achieve this, we can use the <code>comparisons()</code> function and define custom functions. Here, we present a quick example without much detail, but you can refer to the comparisons vignette for more explanations on custom functions in the <code>comparison</code> argument.</p> <p>Consider this simple GAM model:</p> <pre><code>library(marginaleffects)\nlibrary(itsadug)\nlibrary(mgcv)\n\nsimdat$Subject &lt;- as.factor(simdat$Subject)\nmodel &lt;- bam(Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\"),\n             data = simdat)\n\nplot_slopes(model, variables = \"Time\", condition = c(\"Time\", \"Group\"))\n</code></pre> <p></p> <p>Minimum velocity, overall:</p> <pre><code>comparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  vcov = FALSE,\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;  Term Contrast Estimate\n#&gt;  Time   +1e-06  -0.0267\n#&gt; \n#&gt; Columns: term, contrast, estimate, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>Minimum velocity by group:</p> <pre><code>comparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  by = \"Group\",\n  vcov = FALSE,\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;  Term Contrast    Group Estimate\n#&gt;  Time   +1e-06 Children  -0.0153\n#&gt;  Time   +1e-06 Adults    -0.0267\n#&gt; \n#&gt; Columns: term, contrast, Group, estimate \n#&gt; Type:  response\n</code></pre> <p>Difference between the minimum velocity of each group:</p> <pre><code>comparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  vcov = FALSE,\n  by = \"Group\",\n  hypothesis = \"pairwise\",\n  comparison = \\(hi, lo) min((hi - lo) / 1e-6))\n#&gt; \n#&gt;               Term Estimate\n#&gt;  Children - Adults   0.0114\n#&gt; \n#&gt; Columns: term, estimate \n#&gt; Type:  response\n</code></pre> <p>What <code>Time</code> value corresponds to the minimum velocity?</p> <pre><code>low = function(hi, lo, x) {\n    dydx &lt;- (hi - lo) / 1e-6\n    dydx_min &lt;- min(dydx)\n    x[dydx == dydx_min][1]\n}\n\ncomparisons(model,\n  variables = list(\"Time\" = 1e-6),\n  vcov = FALSE,\n  by = \"Group\",\n  comparison = low)\n#&gt; \n#&gt;  Term Contrast    Group Estimate\n#&gt;  Time   +1e-06 Children     1313\n#&gt;  Time   +1e-06 Adults       1556\n#&gt; \n#&gt; Columns: term, contrast, Group, estimate \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/slopes/#manual-computation","title":"Manual computation","text":"<p>Now we illustrate how to reproduce the output of <code>slopes</code> manually:</p> <pre><code>library(marginaleffects)\n\nmod &lt;- glm(am ~ hp, family = binomial, data = mtcars)\n\neps &lt;- 1e-4\nd1 &lt;- transform(mtcars, hp = hp - eps / 2)\nd2 &lt;- transform(mtcars, hp = hp + eps / 2)\np1 &lt;- predict(mod, type = \"response\", newdata = d1)\np2 &lt;- predict(mod, type = \"response\", newdata = d2)\ns &lt;- (p2 - p1) / eps\ntail(s)\n#&gt;  Porsche 914-2   Lotus Europa Ford Pantera L   Ferrari Dino  Maserati Bora     Volvo 142E \n#&gt;  -0.0020285496  -0.0020192814  -0.0013143243  -0.0018326764  -0.0008900012  -0.0020233577\n</code></pre> <p>Which is equivalent to:</p> <pre><code>slopes(mod, eps = eps) |&gt; tail()\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %    97.5 %\n#&gt;    hp -0.00203   0.001347 -1.51  0.13199 2.9 -0.00467  0.000611\n#&gt;    hp -0.00202   0.001556 -1.30  0.19440 2.4 -0.00507  0.001031\n#&gt;    hp -0.00131   0.000441 -2.98  0.00285 8.5 -0.00218 -0.000451\n#&gt;    hp -0.00183   0.001323 -1.39  0.16591 2.6 -0.00443  0.000760\n#&gt;    hp -0.00089   0.000283 -3.14  0.00169 9.2 -0.00145 -0.000334\n#&gt;    hp -0.00202   0.001586 -1.28  0.20206 2.3 -0.00513  0.001085\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, am, hp \n#&gt; Type:  response\n</code></pre> <p>And we can get average marginal effects by subgroup as follows:</p> <pre><code>tapply(s, mtcars$cyl, mean)\n#&gt;            4            6            8 \n#&gt; -0.002010526 -0.001990774 -0.001632681\n\nslopes(mod, eps = eps, by = \"cyl\")\n#&gt; \n#&gt;  Term    Contrast cyl Estimate Std. Error     z Pr(&gt;|z|)   S    2.5 %   97.5 %\n#&gt;    hp mean(dY/dX)   4 -0.00201    0.00141 -1.43   0.1540 2.7 -0.00478 0.000754\n#&gt;    hp mean(dY/dX)   6 -0.00199    0.00150 -1.33   0.1833 2.4 -0.00492 0.000941\n#&gt;    hp mean(dY/dX)   8 -0.00163    0.00095 -1.72   0.0858 3.5 -0.00350 0.000230\n#&gt; \n#&gt; Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \n#&gt; Type:  response\n</code></pre> <p>[1] The term \u201ceffect\u201d is itself tricky. To be clear, this vignette does not use the word \u201ceffect\u201d to imply \u201ccausality\u201d.</p>"},{"location":"vignettes/supported_models/","title":"Supported models","text":""},{"location":"vignettes/supported_models/#supported-models","title":"Supported Models","text":"<p><code>marginaleffects</code> effects supports 90 model types directly, and dozens more via the <code>tidymodels</code> and <code>mlr3</code> frameworks. This table shows the list of directly supported model types. There are three main alternative software packages to compute such slopes (1) <code>Stata</code>\u2019s <code>margins</code> command, (2) <code>R</code>\u2019s <code>margins::margins()</code> function, and (3) <code>R</code>\u2019s <code>emmeans::emtrends()</code> function. The test suite hosted on Github compares the numerical equivalence of results produced by <code>marginaleffects::slopes()</code> to those produced by all 3 alternative software packages:</p> <ul> <li>\u2713: a green check means that the results of at least one model are     equal to a reasonable tolerance.</li> <li>\u2716: a red cross means that the results are not identical; extra     caution is warranted.</li> <li>U: a grey U means that computing slopes for a model type is     unsupported by alternative packages, but supported by     <code>marginaleffects</code>.</li> <li>An empty cell means means that no comparison has been made yet.</li> </ul> <p>I am eager to add support for new models. Feel free to file a request or submit code on Github.</p>  Numerical equivalence   Supported by marginaleffects   Stata   margins   emtrends  Package Function dY/dX SE dY/dX SE dY/dX SE stats lm \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 glm \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 nls loess \u2713 U U AER ivreg \u2713 \u2713 \u2713 \u2713 U U tobit \u2713 \u2713 U U \u2713 \u2713 afex afex_aov U U \u2713 \u2713 aod betabin U U U U betareg betareg \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 bife bife U U U U biglm biglm U U U U bigglm U U U U blme blmer bglmer brglm2 bracl U U U U brglmFit \u2713 \u2713 \u2713 \u2713 brnb \u2713 \u2713 U U brmultinom U U U U brms brm U U \u2713 \u2713 crch crch U U U U hxlr U U U U DCchoice oohbchoice estimatr lm_lin lm_robust \u2713 \u2713 \u2713 U \u2713 \u2713 iv_robust \u2713 \u2713 U U U U fixest feols \u2713 \u2713 U U U U feglm U U U U fenegbin U U U U fepois \u2713 \u2713 U U U U gam gam U U \u2713 \u2713 gamlss gamlss U U \u2713 \u2713 geepack geeglm U U \u2713 \u2713 glmmTMB glmmTMB U U \u2713 \u2713 glmx glmx \u2713 U U U ivreg ivreg \u2713 \u2713 \u2713 \u2713 U U mlr3 Learner lme4 lmer \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 glmer \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 glmer.nb \u2713 \u2713 \u2713 \u2713 lmerTest lmer \u2713 \u2713 \u2713 \u2713 logistf logistf flic flac MASS glmmPQL U U \u2713 \u2713 glm.nb \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 polr \u2713 \u2713 \u2716 \u2716 \u2713 \u2713 rlm \u2713 \u2713 \u2713 \u2713 mclogit mblogit U U U U mclogit U U U U MCMCglmm MCMCglmm U U U U U U mgcv gam U U \u2713 \u2713 bam U U \u2713 \u2716 mhurdle mhurdle \u2713 \u2713 U U mlogit mlogit U U U U mvgam mvgam nlme gls U U \u2713 \u2713 lme nnet multinom \u2713 \u2713 U U U U ordbetareg ordbetareg U U ordinal clm \u2713 \u2713 U U U U plm plm \u2713 \u2713 \u2713 \u2713 U U phylolm phylolm phyloglm pscl hurdle \u2713 U \u2713 \u2716 hurdle \u2713 U \u2713 \u2716 zeroinfl \u2713 \u2713 \u2713 U \u2713 \u2713 quantreg rq \u2713 \u2713 U U \u2713 \u2713 Rchoice hetprob ivpml rms ols lrm orm Gls robust lmRob U U U U robustbase glmrob \u2713 \u2713 U U lmrob \u2713 \u2713 U U robustlmm rlmer U U rstanarm stan_glm \u2716 U \u2713 \u2713 sampleSelection selection U U U U heckit U U U U scam scam U U U U speedglm speedglm \u2713 \u2713 \u2713 \u2713 U U speedlm \u2713 \u2713 \u2713 \u2713 U U survey svyglm \u2713 \u2713 \u2713 \u2713 svyolr survival clogit coxph \u2713 \u2713 U U \u2713 \u2713 survreg tobit1 tobit1 \u2713 \u2713 U U truncreg truncreg \u2713 \u2713 \u2713 \u2713 U U"},{"location":"vignettes/svalues/","title":"S Values","text":"<p>The S value \u2014 \u201cShannon transform\u201d or \u201cbinary surprisal value\u201d \u2014 is a cognitive tool to help analysts make intuitive sense of p values [@RafGre2020]. It allows us to compare a p value to the outcome of a familiar game of chance.</p> <p>Consider this: We toss a coin 4 times to see if we can reject the null hypothesis that the coin toss is fair. If the null is true, the probability of drawing Heads on any single toss is $\\frac{1}{2}$. The probability of observing 4 Heads in a row is $\\left (\\frac{1}{2} \\right )^4=\\frac{1}{16}=0.0625$. This probability characterizes the \u201csurprise\u201d caused by observing 4 straight heads in a world where the null holds, that is, where the coin toss is fair.</p> <p>Now consider a different exercise: We estimate a model and use <code>marginaleffects::hypotheses()</code> to test if two of the estimated coefficients are equal:</p> <pre><code>library(marginaleffects)\ndat &lt;- transform(mtcars, cyl = factor(cyl))\nmod &lt;- lm(mpg ~ cyl, dat)\nhyp &lt;- hypotheses(mod, \"cyl6 = cyl8\")\nhyp\n#&gt; \n#&gt;         Term Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  cyl6 = cyl8     4.64       1.49 3.11  0.00186 9.1  1.72   7.57\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n</code></pre> <p>The difference between <code>cyl6</code> and <code>cyl8</code> is 4.64, and the associated p value is 0.0018593. Again, the p value can be interpreted as a measure of the surprise caused by the data if the null were true (i.e., if the two coefficients were in fact equal).</p> <p>How many consecutive Heads tosses would be as surprising as this test of equality? To answer this question, we solve for s in $p=\\left (\\frac{1}{2} \\right )^s$. The solution is the negative log<sub>2</sub> of p:</p> <pre><code>-log2(hyp$p.value)\n#&gt; [1] 9.070986\n</code></pre> <p>Indeed, the probability of obtaining 9 straight Heads with fair coin tosses is $\\left (\\frac{1}{2} \\right )^9=0.0019531$, which is very close to the p value we observed in the test of coefficient equality (see the S column in the <code>marginaleffects</code> printout above). Comparing our p value to the outcome of such a familiar game of chance gives us a nice intuitive interpretation:</p> <p>If the <code>cyl6</code> and <code>cyl8</code> coefficients were truly equal, finding an absolute difference greater than 4.64 purely by chance would be as surprising as tossing 9 straight Heads with a fair coin toss.</p> <p>The benefits of S values include [@ColEdwGre2021]:</p> <ol> <li>Calibrates the analyst\u2019s intuitions by reference to a well-known     physical process (coin flips).</li> <li>Avoids the problematic dichotomization of findings as \u201csignificant\u201d     and \u201cnot significant\u201d [@Rot2021].</li> <li>Reduces the reliance on arbitrary thresholds of significance like     \u03b1\u2004=\u20040.05.</li> <li>Guards against the common misinterpretation of p values as the     \u201cprobability that the null hypothesis is true\u201d or as the probability     of the alternative hypothesis. This is in part because S is above 1     whenever p\\&lt;0.5.[1]</li> <li>Refers to a more natural scale: \u201cThe difference between a p value of     0.99 and 0.90 in terms of how surprising the observed test statistic     is, is not the same as the difference between 0.10 and 0.01.\u201d[2]</li> </ol> <p></p> <p>[1] Thanks to Sander Greenland for this note.</p> <p>[2] Thanks to Zad Rafi for noting this and for linking to [@RafGre2020].</p>"},{"location":"vignettes/uncertainty/","title":"Standard Errors","text":""},{"location":"vignettes/uncertainty/#delta-method","title":"Delta Method","text":"<p>All the standard errors generated by the <code>slopes()</code>, <code>comparisons()</code>, and <code>hypotheses()</code> functions of this package package are estimated using the delta method. Mathematical treatments of this method can be found in most statistics textbooks and on Wikipedia. Roughly speaking, the delta method allows us to approximate the distribution of a smooth function of an asymptotically normal estimator.</p> <p>Concretely, this allows us to generate standard errors around functions of a model\u2019s coefficient estimates. Predictions, contrasts, marginal effects, and marginal means are functions of the coefficients, so we can use the delta method to estimate standard errors around all of those quantities. Since there are a lot of mathematical treatments available elsewhere, this vignette focuses on the \u201cimplementation\u201d in <code>marginaleffects</code>.</p> <p>Consider the case of the <code>marginal_means()</code> function. When a user calls this function, they obtain a vector of marginal means. To estimate standard errors around this vector:</p> <ol> <li>Take the numerical derivative of the marginal means vector with     respect to the first coefficient in the model:<ul> <li>Compute marginal means with the original model: f(\u03b2)</li> <li>Increment the first (and only the first) coefficient held inside     the model object by a small amount, and compute marginal means     again: f(\u03b2+\u03b5)</li> <li>Calculate: $\\frac{f(\\beta+\\varepsilon) - f(\\beta)}{\\varepsilon}$</li> </ul> </li> <li>Repeat step 1 for every coefficient in the model to construct a J     matrix.</li> <li>Extract the variance-covariance matrix of the coefficient estimates:     V</li> <li>Standard errors are the square root of the diagonal of JVJ\u2032</li> </ol> <p>Scroll down this page to the Numerical Derivatives section to see a detailed explanation, along with code for manual computation.</p>"},{"location":"vignettes/uncertainty/#standard-errors-and-intervals-for-slopes-and-comparisons","title":"Standard errors and intervals for <code>slopes()</code> and <code>comparisons()</code>","text":"<p>All standard errors for the <code>slopes()</code> and <code>comparisons()</code> functions are computed using the delta method, as described above. The confidence intervals are calculated as estimate \u00b1 <code>qnorm((1 - conf_level) / 2)</code> standard errors (e.g., for 95% confidence intervals, estimate \u00b1 1.96 standard errors) and assume that the (transformed) estimates are normally distributed.</p>"},{"location":"vignettes/uncertainty/#standard-errors-and-intervals-for-marginal_means-and-predictions","title":"Standard errors and intervals for <code>marginal_means()</code> and <code>predictions()</code>","text":"<p>The <code>marginal_means()</code> and <code>predictions()</code> function can compute the confidence intervals in two ways. If the following conditions hold:</p> <ul> <li>The user sets: <code>type = \"response\"</code></li> <li>The model class is <code>glm</code></li> <li>The <code>transform</code> argument is <code>NULL</code></li> </ul> <p>then <code>marginal_means()</code> and <code>predictions()</code> will first compute estimates on the link scale, and then back transform them using the inverse link function supplied by <code>insight::link_inverse(model)</code> function.</p> <p>In all other cases, standard errors are computed using the delta method as described above.</p>"},{"location":"vignettes/uncertainty/#robust-standard-errors","title":"Robust standard errors","text":"<p>All the functions in the <code>marginaleffects</code> package can compute robust standard errors on the fly for any model type supported by the <code>sandwich</code> package. The <code>vcov</code> argument supports string shortcuts like <code>\"HC3\"</code>, a one-sided formula to request clustered standard errors, variance-covariance matrices, or functions which return such matrices. Here are a few examples.</p> <p>Adjusted predictions with classical or heteroskedasticity-robust standard errors:</p> <pre><code>library(marginaleffects)\nlibrary(patchwork)\nmod &lt;- lm(mpg ~ hp, data = mtcars)\n\np &lt;- predictions(mod)\nhead(p, 2)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      22.6      0.777 29.1   &lt;0.001 614.7  21.1   24.1\n#&gt;      22.6      0.777 29.1   &lt;0.001 614.7  21.1   24.1\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp \n#&gt; Type:  response\n\np &lt;- predictions(mod, vcov = \"HC3\")\nhead(p, 2)\n#&gt; \n#&gt;  Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n#&gt;      22.6      0.863 26.2   &lt;0.001 499.5  20.9   24.3\n#&gt;      22.6      0.863 26.2   &lt;0.001 499.5  20.9   24.3\n#&gt; \n#&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp \n#&gt; Type:  response\n</code></pre> <p>Marginal effects with cluster-robust standard errors:</p> <pre><code>avg_slopes(mod, vcov = ~cyl)\n#&gt; \n#&gt;  Term Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n#&gt;    hp  -0.0682     0.0187 -3.65   &lt;0.001 11.9 -0.105 -0.0316\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Comparing adjusted predictions with classical and robust standard errors:</p> <pre><code>p1 &lt;- plot_predictions(mod, condition = \"hp\")\np2 &lt;- plot_predictions(mod, condition = \"hp\", vcov = \"HC3\")\np1 + p2\n</code></pre> <p></p>"},{"location":"vignettes/uncertainty/#simulation-based-inference","title":"Simulation-based inference","text":"<p><code>marginaleffects</code> offers an experimental <code>inferences</code> function to conduct simulation-based inference following the strategy proposed by Krinsky &amp; Robb (1986):</p> <ol> <li>Draw <code>iter</code> sets of simulated coefficients from a multivariate     normal distribution with mean equal to the original model\u2019s     estimated coefficients and variance equal to the model\u2019s     variance-covariance matrix (classical, \u201cHC3\u201d, or other).</li> <li>Use the <code>iter</code> sets of coefficients to compute <code>iter</code> sets of     estimands: predictions, comparisons, or slopes.</li> <li>Take quantiles of the resulting distribution of estimands to obtain     a confidence interval and the standard deviation of simulated     estimates to estimate the standard error.</li> </ol> <p>Here are a few examples:</p> <pre><code>library(marginaleffects)\nlibrary(ggplot2)\nlibrary(ggdist)\n\nmod &lt;- glm(vs ~ hp * wt + factor(gear), data = mtcars, family = binomial)\n\nmod |&gt; predictions() |&gt; inferences(method = \"simulation\")\n#&gt; \n#&gt;  Estimate Std. Error    2.5 % 97.5 %\n#&gt;  7.84e-01      0.199 2.43e-01  0.978\n#&gt;  7.84e-01      0.170 3.34e-01  0.963\n#&gt;  8.98e-01      0.144 4.54e-01  0.991\n#&gt;  8.74e-01      0.233 1.67e-01  0.996\n#&gt;  1.31e-02      0.185 5.13e-05  0.770\n#&gt; --- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#&gt;  3.83e-01      0.300 1.57e-02  0.956\n#&gt;  1.21e-06      0.120 2.06e-12  0.226\n#&gt;  6.89e-03      0.150 4.19e-05  0.591\n#&gt;  8.07e-11      0.144 2.22e-16  0.377\n#&gt;  7.95e-01      0.172 3.15e-01  0.970\n#&gt; Columns: rowid, estimate, std.error, conf.low, conf.high, vs, hp, wt, gear \n#&gt; Type:  response\n\nmod |&gt; avg_slopes(vcov = ~gear) |&gt; inferences(method = \"simulation\")\n#&gt; \n#&gt;  Term Contrast  Estimate Std. Error   2.5 %  97.5 %\n#&gt;  gear    4 - 3 -3.92e-02    0.05786 -0.0912 0.13230\n#&gt;  gear    5 - 3 -1.93e-01    0.27180 -0.4878 0.33234\n#&gt;  hp      dY/dX -5.02e-03    0.00454 -0.0114 0.00522\n#&gt;  wt      dY/dX -3.98e-05    0.31966 -0.6546 0.69303\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Since simulation based inference generates <code>iter</code> estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the <code>posterior_draws()</code> function, and plot their distributions using packages like<code>ggplot2</code> and <code>ggdist</code>:</p> <pre><code>mod |&gt;\n  avg_comparisons(variables = \"gear\") |&gt;\n  inferences(method = \"simulation\") |&gt;\n  posterior_draws(\"rvar\") |&gt;\n  ggplot(aes(y = contrast, xdist = rvar)) +\n  stat_slabinterval()\n</code></pre> <p></p>"},{"location":"vignettes/uncertainty/#bootstrap","title":"Bootstrap","text":"<p>It is easy to use the bootstrap as an alternative strategy to compute standard errors and confidence intervals. Several <code>R</code> packages can help us achieve this, including the long-established <code>boot</code> package:</p> <pre><code>library(boot)\nset.seed(123)\n\nbootfun &lt;- function(data, indices, ...) {\n    d &lt;- data[indices, ]\n    mod &lt;- lm(mpg ~ am + hp + factor(cyl), data = d)\n    cmp &lt;- comparisons(mod, newdata = d, vcov = FALSE, variables = \"am\")\n    tidy(cmp)$estimate\n}\n\nb &lt;- boot(data = mtcars, statistic = bootfun, R = 1000)\n\nb\n#&gt; \n#&gt; ORDINARY NONPARAMETRIC BOOTSTRAP\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; boot(data = mtcars, statistic = bootfun, R = 1000)\n#&gt; \n#&gt; \n#&gt; Bootstrap Statistics :\n#&gt;     original     bias    std. error\n#&gt; t1* 4.157856 0.01543426    1.003461\nboot.ci(b, type = \"perc\")\n#&gt; BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#&gt; Based on 1000 bootstrap replicates\n#&gt; \n#&gt; CALL : \n#&gt; boot.ci(boot.out = b, type = \"perc\")\n#&gt; \n#&gt; Intervals : \n#&gt; Level     Percentile     \n#&gt; 95%   ( 2.240,  6.277 )  \n#&gt; Calculations and Intervals on Original Scale\n</code></pre> <p>Note that, in the code above, we set <code>vcov=FALSE</code> to avoid computation of delta method standard errors and speed things up.</p> <p>Compare to the delta method standard errors:</p> <pre><code>mod &lt;- lm(mpg ~ am + hp + factor(cyl), data = mtcars)\navg_comparisons(mod, variables = \"am\")\n#&gt; \n#&gt;  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n#&gt;    am    1 - 0     4.16       1.26 3.31   &lt;0.001 10.1   1.7   6.62\n#&gt; \n#&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre>"},{"location":"vignettes/uncertainty/#mixed-effects-models-satterthwaite-and-kenward-roger-corrections","title":"Mixed effects models: Satterthwaite and Kenward-Roger corrections","text":"<p>For linear mixed effects models we can apply the Satterthwaite and Kenward-Roger corrections in the same way as above:</p> <pre><code>library(marginaleffects)\nlibrary(patchwork)\nlibrary(lme4)\n\ndat &lt;- mtcars\ndat$cyl &lt;- factor(dat$cyl)\ndat$am &lt;- as.logical(dat$am)\nmod &lt;- lmer(mpg ~ hp + am + (1 | cyl), data = dat)\n</code></pre> <p>Marginal effects at the mean with classical standard errors and z-statistic:</p> <pre><code>slopes(mod, newdata = \"mean\")\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n#&gt;    am TRUE - FALSE   4.6661     1.1343  4.11   &lt;0.001 14.6  2.4430  6.8892\n#&gt;    hp dY/dX         -0.0518     0.0115 -4.52   &lt;0.001 17.3 -0.0743 -0.0294\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, am, cyl \n#&gt; Type:  response\n</code></pre> <p>Marginal effects at the mean with Kenward-Roger adjusted variance-covariance and degrees of freedom:</p> <pre><code>slopes(mod,\n                newdata = \"mean\",\n                vcov = \"kenward-roger\")\n#&gt; \n#&gt;  Term     Contrast Estimate Std. Error     t Pr(&gt;|t|)   S  2.5 %  97.5 %   Df\n#&gt;    am TRUE - FALSE   4.6661     1.2824  3.64   0.0874 3.5 -1.980 11.3121 1.68\n#&gt;    hp dY/dX         -0.0518     0.0152 -3.41   0.0964 3.4 -0.131  0.0269 1.68\n#&gt; \n#&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, am, cyl, df \n#&gt; Type:  response\n</code></pre> <p>We can use the same option in any of the package\u2019s core functions, including:</p> <pre><code>plot_predictions(mod, condition = \"hp\", vcov = \"satterthwaite\")\n</code></pre> <p></p>"},{"location":"vignettes/uncertainty/#numerical-derivatives-sensitivity-to-step-size","title":"Numerical derivatives: Sensitivity to step size","text":"<pre><code>dat &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin &lt;- ifelse(dat$body_mass_g &gt; median(dat$body_mass_g, na.rm = TRUE), 1, 0)\nmod &lt;- glm(large_penguin ~ bill_length_mm * flipper_length_mm + species, data = dat, family = binomial)\n</code></pre> <p><code>marginaleffects</code> uses numerical derivatives in two contexts:</p> <ol> <li>Estimate the partial derivatives reported by <code>slopes()</code> function.<ul> <li>Centered finite difference</li> <li>$\\frac{f(x + \\varepsilon_1 / 2) - f(x - \\varepsilon_1 / 2)}{\\varepsilon_1}$,     where we take the derivative with respect to a predictor of     interest, and f is the <code>predict()</code> function.</li> </ul> </li> <li>Estimate standard errors using the delta method.<ul> <li>Forward finite difference</li> <li>$\\frac{g(\\hat{\\beta}) - g(\\hat{\\beta} + \\varepsilon_2)}{\\varepsilon_2}$,     where we take the derivative with respect to a model\u2019s     coefficients, and g is a <code>marginaleffects</code> function which     returns some quantity of interest (e.g., slope, marginal means,     predictions, etc.)</li> </ul> </li> </ol> <p>Note that the step sizes used in those two contexts can differ. If the variables and coefficients have very different scales, it may make sense to use different values for \u03b5<sub>1</sub> and \u03b5<sub>2</sub>.</p> <p>By default, \u03b5<sub>1</sub> is set to <code>1e-4</code> times the range of the variable with respect to which we are taking the derivative. By default, \u03b5<sub>2</sub> is set to the maximum value of <code>1e-8</code>, or <code>1e-4</code> times the smallest absolute coefficient estimate. (These choices are arbitrary, but I have found that in practice, smaller values can produce unstable results.)</p> <p>\u03b5<sub>1</sub> can be controlled by the <code>eps</code> argument of the <code>slopes()</code> function. \u03b5<sub>2</sub> can be controlled by setting a global option which tells <code>marginaleffects</code> to compute the jacobian using the <code>numDeriv</code> package instead of its own internal functions. This allows more control over the step size, and also gives access to other differentiation methods, such as Richardson\u2019s. To use <code>numDeriv</code>, we define a list of arguments which will be pushed forward to <code>numDeriv::jacobian</code>:</p> <pre><code>avg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.69   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"Richardson\"))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.69   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-3)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error     z Pr(&gt;|z|)   S 2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279      0.568 0.049    0.961 0.1 -1.09   1.14\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-5)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00601 4.64   &lt;0.001 18.1 0.0161 0.0396\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-7)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#&gt; \n#&gt;            Term Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n#&gt;  bill_length_mm   0.0279    0.00595 4.68   &lt;0.001 18.4 0.0162 0.0395\n#&gt; \n#&gt; Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n#&gt; Type:  response\n</code></pre> <p>Notice that the standard errors can vary considerably when using different step sizes. It is good practice for analysts to consider the sensitivity of their results to this setting.</p> <p>Now, we illustrate the full process of standard error computation, using raw <code>R</code> code. First, we choose two step sizes:</p> <pre><code>eps1 &lt;- 1e-5 # slope\neps2 &lt;- 1e-7 # delta method\n\ns &lt;- slopes(mod, newdata = head(dat, 3), variables = \"bill_length_mm\", eps = eps1)\nprint(s[, 1:5], digits = 6)\n#&gt; \n#&gt;            Term  Estimate Std. Error       z\n#&gt;  bill_length_mm 0.0179765 0.00872988 2.05919\n#&gt;  bill_length_mm 0.0359630 0.01254767 2.86611\n#&gt;  bill_length_mm 0.0849071 0.02128459 3.98913\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic\n</code></pre> <p>We can get the same estimates manually with these steps:</p> <pre><code>linkinv &lt;- mod$family$linkinv\n\n## increment the variable of interest by h\ndat_hi &lt;- transform(dat, bill_length_mm = bill_length_mm + eps1)\n\n## model matrices: first 3 rows\nmm_lo &lt;- insight::get_modelmatrix(mod, data = dat)[1:3,]\nmm_hi &lt;- insight::get_modelmatrix(mod, data = dat_hi)[1:3,]\n\n## predictions\np_lo &lt;- linkinv(mm_lo %*% coef(mod))\np_hi &lt;- linkinv(mm_hi %*% coef(mod))\n\n## slopes\n(p_hi - p_lo) / eps1\n#&gt;         [,1]\n#&gt; 1 0.01797653\n#&gt; 2 0.03596304\n#&gt; 3 0.08490712\n</code></pre> <p>To get standard errors, we build a jacobian matrix where each column holds derivatives of the vector valued slope function, with respect to each of the coefficients. Using the same example:</p> <pre><code>b_lo &lt;- b_hi &lt;- coef(mod)\nb_hi[1] &lt;- b_hi[1] + eps2\n\ndydx_lo &lt;- (linkinv(mm_hi %*% b_lo) - linkinv(mm_lo %*% b_lo)) / eps1\ndydx_hi &lt;- (linkinv(mm_hi %*% b_hi) - linkinv(mm_lo %*% b_hi)) / eps1\n(dydx_hi - dydx_lo) / eps2\n#&gt;         [,1]\n#&gt; 1 0.01600109\n#&gt; 2 0.02771394\n#&gt; 3 0.02275957\n</code></pre> <p>This gives us the first column of J, which we can recover in full from the <code>marginaleffects</code> object attribute:</p> <pre><code>J &lt;- attr(s, \"jacobian\")\nJ\n#&gt;      (Intercept) bill_length_mm flipper_length_mm speciesChinstrap speciesGentoo bill_length_mm:flipper_length_mm\n#&gt; [1,]  0.01600803      0.6777495          2.897238                0             0                         122.6914\n#&gt; [2,]  0.02770006      1.1957935          5.153072                0             0                         222.4989\n#&gt; [3,]  0.02275957      1.1491919          4.440004                0             0                         224.0833\n</code></pre> <p>To build the full matrix, we would simply iterate through the coefficients, incrementing them one after the other. Finally, we get standard errors via:</p> <pre><code>sqrt(diag(J %*% vcov(mod) %*% t(J)))\n#&gt; [1] 0.008729884 0.012547666 0.021284589\n</code></pre> <p>Which corresponds to our original standard errors:</p> <pre><code>print(s[, 1:5], digits = 7)\n#&gt; \n#&gt;            Term   Estimate  Std. Error        z\n#&gt;  bill_length_mm 0.01797650 0.008729884 2.059192\n#&gt;  bill_length_mm 0.03596299 0.012547666 2.866110\n#&gt;  bill_length_mm 0.08490708 0.021284589 3.989134\n#&gt; \n#&gt; Columns: rowid, term, estimate, std.error, statistic\n</code></pre> <p>Reverting to default settings:</p> <pre><code>options(marginaleffects_numDeriv = NULL)\n</code></pre> <p>Note that our default results for this model are very similar \u2013 but not exactly identical \u2013 to those generated by the <code>margins</code>. As should be expected, the results in <code>margins</code> are also very sensitive to the value of <code>eps</code> for this model:</p> <pre><code>library(margins)\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.008727977 0.012567079 0.021293275\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-4, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.2269512 0.2255849 0.6636208\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-5, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#&gt; [1] 0.02317078 0.02928267 0.05480282\n</code></pre>"},{"location":"vignettes/uncertainty/#bayesian-estimates-and-credible-intervals","title":"Bayesian estimates and credible intervals","text":"<p>See the <code>brms</code> vignette for a discussion of bayesian estimates and credible intervals.</p>"}]}